{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b08a835",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8b0d0992c778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# In[1]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGNN_graphpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGNN_graphpred_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfast_transformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLengthMask\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mLM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\esm-main22222\\model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMessagePassing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madd_self_loops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglobal_add_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_mean_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_max_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalAttention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSet2Set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "import torch\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from splitters import scaffold_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "import esm\n",
    "\n",
    "class SeqTeacher(nn.Module):\n",
    "    def __init__(self,protein_embd_dim,output_embd_dim,dropout):\n",
    "        super(SeqTeacher,self).__init__(protein_embd_dim,output_embd_dim,dropout)\n",
    "        self.protein_embd_dim=protein_embd_dim\n",
    "        self.protein_model, self.protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.alphabet=self.protein_model.alphabet##########.to(device)\n",
    "        self.batch_converter=self.alphabet.get_batch_converter()\n",
    "        #self.protein_sequence_representations = torch.tensor([1,2])\n",
    "        self.protein_linear=torch.nn.Linear(protein_embd_dim,output_embd_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            batch_labels, batch_strs, batch_tokens = self.batch_converter(x)#############\n",
    "            batch_tokens=batch_tokens.to(self.device)######################\n",
    "            #print('batch_tokens:',batch_tokens.device.type)\n",
    "            batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "            #print('protein_batch_lens:',len(batch_lens),batch_lens)\n",
    "        \n",
    "            results = self.protein_model(batch_tokens, repr_layers=[33], return_contacts=True)###############3\n",
    "        \n",
    "            protein_token_representations = results[\"representations\"][33]\n",
    "            m,n,v=protein_token_representations.shape\n",
    "        #print('m,n,v:',m,n,v)\n",
    "        #print('protein_token_representations:',protein_token_representations.shape)\n",
    "        \n",
    "        \n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            if i==0:\n",
    "                u=protein_token_representations[i, 1 : tokens_len - 1].mean(0).reshape(-1,self.protein_embd_dim)\n",
    "                output=u\n",
    "                \n",
    "            else:\n",
    "                u=protein_token_representations[i, 1 : tokens_len - 1].mean(0).reshape(-1,self.protein_embd_dim)\n",
    "                output=torch.concat([output,u],dim=0)\n",
    "        output=self.dropout(self.relu(self.protein_linear( output)))\n",
    "        return output\n",
    "    \n",
    "class MolecularTeacher(nn.Module):\n",
    "    def __init__(self,checkpoint_file,num_layer,mole_embd_dim,output_embed_dim,num_task,JK,dropout,graph_poolingg,gnn_type):\n",
    "        super(MolecularTeacher,self).__init__(num_layer,mole_embd_dim,num_task,JK,dropout,graph_poolingg,gnn_type)\n",
    "        \n",
    "        self.molecular_model = GNN_graphpred_1(num_layer,emb_dim, num_tasks, JK, dropout, graph_pooling, gnn_type)\n",
    "        self.molecular_model.from_pretrained('model_gin/{}.pth'.format(checkpoint_file))\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.mole_model=self.molecular_model.gnn#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_pool=self.molecular_model.pool#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_linear=torch.nn.Linear(mol_embd_dim,output_embd_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,molecular_data):\n",
    "        #model=self.molecular_model.from_pretrained('model_gin/{}.pth'.format(args.input_model_file))\n",
    "        y=self.mole_model.gnn(molecular_data.x,molecular_data.edge_index,molecular_data.edge_attr)\n",
    "        y=self.mole_pool(y,molecular_data.batch)\n",
    "        y=self.relu(self.dropout(self.mole_linear(y)))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    \n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
    "#from finetune.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "#from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
    "from functools import partial\n",
    "from apex import optimizers\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "from tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "class LightningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(LightningModule, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.save_hyperparameters(config)\n",
    "        self.tokenizer=tokenizer\n",
    "        # Word embeddings layer\n",
    "        n_vocab, d_emb = len(tokenizer.vocab), config.n_embd\n",
    "        # input embedding stem\n",
    "        \n",
    "        builder = rotate_builder.from_kwargs(\n",
    "            n_layers=config.n_layer,\n",
    "            n_heads=config.n_head,\n",
    "            query_dimensions=config.n_embd//config.n_head,\n",
    "            value_dimensions=config.n_embd//config.n_head,\n",
    "            feed_forward_dimensions=config.n_embd,\n",
    "            attention_type='linear',\n",
    "            feature_map=partial(GeneralizedRandomFeatures, n_dims=config.num_feats),\n",
    "            activation='gelu',\n",
    "            )\n",
    "        self.pos_emb = None\n",
    "        self.tok_emb = nn.Embedding(n_vocab, config.n_embd)\n",
    "        #print('self.tok_emb:',self.tok_emb)\n",
    "        self.drop = nn.Dropout(config.d_dropout)\n",
    "        \n",
    "        ## transformer\n",
    "        self.blocks = builder.get()\n",
    "        \n",
    "\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        dims = [150, 50, 50, 2]\n",
    "\n",
    "\n",
    "        def __init__(self, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.desc_skip_connection = True \n",
    "            self.fcs = []  # nn.ModuleList()\n",
    "            #print('dropout is {}'.format(dropout))\n",
    "\n",
    "            self.fc1 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.relu1 = nn.GELU()\n",
    "            self.fc2 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.relu2 = nn.GELU()\n",
    "            self.final = nn.Linear(smiles_embed_dim, 1)\n",
    "\n",
    "        def forward(self, smiles_emb):\n",
    "            x_out = self.fc1(smiles_emb)\n",
    "            x_out = self.dropout1(x_out)\n",
    "            x_out = self.relu1(x_out)\n",
    "\n",
    "            if self.desc_skip_connection is True:\n",
    "                x_out = x_out + smiles_emb\n",
    "\n",
    "            z = self.fc2(x_out)\n",
    "            z = self.dropout2(z)\n",
    "            z = self.relu2(z)\n",
    "            if self.desc_skip_connection is True:\n",
    "                z = self.final(z + x_out)\n",
    "            else:\n",
    "                z = self.final(z)\n",
    "\n",
    "            return z\n",
    "\n",
    "    class lm_layer(nn.Module):\n",
    "        def __init__(self, n_embd, n_vocab):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Linear(n_embd, n_embd)\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.head = nn.Linear(n_embd, n_vocab, bias=False)\n",
    "        def forward(self, tensor):\n",
    "            tensor = self.embed(tensor)\n",
    "            tensor = F.gelu(tensor)\n",
    "            tensor = self.ln_f(tensor)\n",
    "            tensor = self.head(tensor)\n",
    "            return tensor\n",
    "\n",
    "    def get_loss(self, smiles_emb, measures):\n",
    "\n",
    "        z_pred = self.net.forward(smiles_emb).squeeze()\n",
    "        measures = measures.float()\n",
    "\n",
    "        return self.loss(z_pred, measures), z_pred, measures\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    print('collate_batch:',batch)\n",
    "    print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')    \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "\n",
    "from fast_transformers.masking import LengthMask as LM    \n",
    "class SmileTeacher(nn.Module):\n",
    "    def __init__(self,smile_embd_dim,output_embd_dim,margs,seed_path,config,tokenizer,vocab,strict):\n",
    "       \n",
    "        self.margs = margs\n",
    "        self.tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        self.smile_model=None\n",
    "        if margs.seed_path == '':\n",
    "            \n",
    "            self.smile_model = LightningModule(self.margs, self.tokenizer)\n",
    "        else:\n",
    "            \n",
    "            self.smile_model = LightningModule(margs, tokenizer).load_from_checkpoint(seed_path,  config, tokenizer, vocab,strict)#########################33\n",
    "        self.smile_linear=nn.Linear(smile_embd_dim,output_embd_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, smile_data):\n",
    "        x,mask=smile_data\n",
    "        length_mask=LM(mask.sum(-1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_embeddings = self.smile_model.tok_emb(x) # each index maps to a (learnable) vector\n",
    "            x6 = self.smile_model.drop(token_embeddings)\n",
    "            x7 = self.smile_model.blocks(x6, length_mask=LM(mask.sum(-1)))\n",
    "            token_embeddings = x7\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            x4 = sum_embeddings / sum_mask####################################\n",
    "        x4=self.relu(self.dropout(self.smile_linear(x4)))\n",
    "        return x4\n",
    "        \n",
    "        \n",
    "\n",
    "class SeqMLPStudent(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SeqMLPDistillation, self).__init__(input_seq_embd_dim,output_seq_embd_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_seq_embd_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_seq_embd_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "    \n",
    "class MolMLPStudent(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MolMLPDistillation, self).__init__(input_mol_embd_dim,output_mol_embd_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_mol_embd_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_mol_embd_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "class SmileMLPStudent(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SmileMLPDistillation, self).__init__(input_smile_embd_dim,output_smile_embd_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_smile_embd_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_smile_embd_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim1, input_dim2)###################妙\n",
    "        self.linear2 = nn.Linear(input_dim2, input_dim1)\n",
    "\n",
    "    def forward(self, x1, x2,d1,d2):#d1=d2=d=output_embd_dim\n",
    "        # 计算注意力权重\n",
    "        #print('x1,x2:',x1.shape,x2.shape)\n",
    "        attn_weights2 = torch.matmul(self.linear1(x1), x2.transpose(0, 1))\n",
    "        #print('attn_wights:',attn_weights.shape)\n",
    "        #attn_weights = attn_weights.squeeze(dim=2)\n",
    "        attn_weights2 = nn.functional.softmax(attn_weights2, dim=1)\n",
    "        \n",
    "        # 使用注意力权重加权融合两个向量\n",
    "        fused_x2 = torch.matmul(attn_weights2, x2)/np.sqrt(d2)\n",
    "        #fused_x1 = torch.matmul(attn_weights2, x2)/np.sqrt(d2)\n",
    "        #print('fused_x1:',fused_x1.shape)\n",
    "        \n",
    "        attn_weights1 = torch.matmul(self.linear2(x2), x1.transpose(0, 1))\n",
    "        #print('attn_wights:',attn_weights.shape)\n",
    "        #attn_weights = attn_weights.squeeze(dim=2)\n",
    "        attn_weights1 = nn.functional.softmax(attn_weights1, dim=1)\n",
    "        fused_x1 = torch.matmul(attn_weights1, x1)/np.sqrt(d1)\n",
    "        #fused_x2 = torch.matmul(attn_weights1.transpose(0, 1), x1)/np.sqrt(d1)\n",
    "        #print('fused_x2:',fused_x2.shape)\n",
    "        return fused_x1, fused_x2\n",
    "\n",
    "    \n",
    "    \n",
    "class InteractionModel_4(torch.nn.Module):\n",
    "    def __init__(self, protein_model,molecular_model,smile_model,protein_embd_dim,mol_embd_dim,num_tasks,output_embd_dim, device,smile_embd_dim, dropout=0.2,aggr = \"mean\"):\n",
    "        super( InteractionModel_4, self ).__init__() \n",
    "        self.protein_embd_dim=protein_embd_dim\n",
    "        self.mol_embd_dim=mol_embd_dim\n",
    "        self.smile_embd_dim=smile_embd_dim\n",
    "        self.output_embd_dim=output_embd_dim\n",
    "        \n",
    "        self.protein_model=protein_model.to(device)\n",
    "        self.mole_model=molecular_model\n",
    "        self.smile_model=smile_model\n",
    "        \n",
    "        self.pred_linear = torch.nn.Linear(2*output_embed_dim, num_tasks)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        self.mol_cross_attention = CrossAttentionLayer(protein_embd_dim, mol_embd_dim)\n",
    "        self.smile_cross_attention = CrossAttentionLayer(protein_embd_dim, smile_embd_dim)\n",
    "        \n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(output_embd_dim, eps=1e-6)  # 默认对最后一个维度初始化\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.relu1 = nn.GELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)  # dropout训练\n",
    "        # 定义可学习参数 t\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        self.t = nn.Parameter(torch.Tensor(1))\n",
    "        self.t_linear1=nn.Linear(1,50)\n",
    "        self.t_linear2=nn.Linear(50,1)\n",
    "        self.t.data.fill_(0.5)  # 初始化 t 为0.5\n",
    "        \n",
    "    #def forward(self, protein_data,*molecular_data):#******不能有，否则出错\n",
    "    def forward(self, protein_data,molecular_data,smile_data):#\n",
    "        x1=self.protein_model(protein_data)\n",
    "        x2=self.mole_model(molecular_data)\n",
    "        x3=self.protein_model(protein_data)\n",
    "        x4=self.smile_model(smile_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x1=self.layer_norm(x1)\n",
    "        x2=self.layer_norm(x2)\n",
    "        x3=self.layer_norm(x3)\n",
    "        x4=self.layer_norm(x4)\n",
    "        fused_x1, fused_x2 =  self.mol_cross_attention(x1, x2,self.output_embd_dim,self.output_embd_dim)  \n",
    "        \n",
    "        \n",
    "        x11_att=fused_x1\n",
    "        x22_att=fused_x2\n",
    "        \n",
    "        x11=torch.add(x11_att,x1)\n",
    "        x22=torch.add(x22_att,x2)\n",
    "        # 合并两个融合后的向量\n",
    "        output12 = torch.cat([x11, x22], dim=1)   \n",
    "        out12 = self.pred_linear(self.dropout(self.relu(out12)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        fused_x3, fused_x4 =  self.smile_cross_attention(x3, x4,self.protein_embd_dim,self.smile_embd_dim)  \n",
    "        x33_att=fused_x3\n",
    "        x44_att=fused_x4\n",
    "        x33=torch.add(x33_att,x3)\n",
    "        x44=torch.add(x44_att,x4)\n",
    "        # 合并两个融合后的向量\n",
    "        output34 = torch.cat([x33, x44], dim=1)   \n",
    "        out34 = self.pred_linear(self.dropout(self.relu(out34)))\n",
    "        \n",
    "        \n",
    "        \n",
    "         # 融合两个分支特征\n",
    "        #t1=self.relu(self.t_linear1(self.t))\n",
    "        t2=torch.sigmoid(self.t_linear2(t1))\n",
    "        \n",
    "        out = t2 * out12 + (1 - t2) * out34\n",
    "        \n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57a826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
