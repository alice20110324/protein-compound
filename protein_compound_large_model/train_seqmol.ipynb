{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not from scratch\n",
      "rese:model_gin\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:217: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['lang_model.embed.weight', 'lang_model.embed.bias', 'lang_model.ln_f.weight', 'lang_model.ln_f.bias', 'lang_model.head.weight']\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [17:45:10] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[17:45:10] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:10] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 12 13 14\n",
      "RDKit ERROR: \n",
      "[17:45:10] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 12 13 14\n",
      "\n",
      "RDKit ERROR: [17:45:10] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:10] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:10] Explicit valence for atom # 30 N, 4, is greater than permitted\n",
      "[17:45:10] Explicit valence for atom # 30 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:10] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[17:45:10] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:10] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:10] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:10] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "[17:45:10] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:10] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[17:45:10] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:11] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:11] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:11] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[17:45:11] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:11] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[17:45:11] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:11] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:11] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:11] Can't kekulize mol.  Unkekulized atoms: 8 10 11 13 15\n",
      "RDKit ERROR: \n",
      "[17:45:11] Can't kekulize mol.  Unkekulized atoms: 8 10 11 13 15\n",
      "\n",
      "RDKit ERROR: [17:45:11] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[17:45:11] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [17:45:12] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[17:45:12] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:12] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[17:45:12] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [17:45:12] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[17:45:12] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:12] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[17:45:12] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:12] Can't kekulize mol.  Unkekulized atoms: 9 13 15 16 21\n",
      "RDKit ERROR: \n",
      "[17:45:12] Can't kekulize mol.  Unkekulized atoms: 9 13 15 16 21\n",
      "\n",
      "RDKit ERROR: [17:45:12] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "RDKit ERROR: \n",
      "[17:45:12] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "\n",
      "RDKit ERROR: [17:45:12] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[17:45:12] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:12] Can't kekulize mol.  Unkekulized atoms: 3 5 7\n",
      "RDKit ERROR: \n",
      "[17:45:12] Can't kekulize mol.  Unkekulized atoms: 3 5 7\n",
      "\n",
      "RDKit ERROR: [17:45:12] Can't kekulize mol.  Unkekulized atoms: 35 37 38 39 40\n",
      "RDKit ERROR: \n",
      "[17:45:12] Can't kekulize mol.  Unkekulized atoms: 35 37 38 39 40\n",
      "\n",
      "RDKit ERROR: [17:45:12] Explicit valence for atom # 36 N, 4, is greater than permitted\n",
      "[17:45:12] Explicit valence for atom # 36 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:13] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[17:45:13] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:13] Explicit valence for atom # 31 N, 4, is greater than permitted\n",
      "[17:45:13] Explicit valence for atom # 31 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:13] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "RDKit ERROR: \n",
      "[17:45:13] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "\n",
      "RDKit ERROR: [17:45:13] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:13] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:13] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:13] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:13] Can't kekulize mol.  Unkekulized atoms: 22 24 25 26 27\n",
      "RDKit ERROR: \n",
      "[17:45:13] Can't kekulize mol.  Unkekulized atoms: 22 24 25 26 27\n",
      "\n",
      "RDKit ERROR: [17:45:13] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "RDKit ERROR: \n",
      "[17:45:13] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "\n",
      "RDKit ERROR: [17:45:14] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[17:45:14] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:14] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "[17:45:14] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:14] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[17:45:14] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [17:45:14] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:14] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:14] Can't kekulize mol.  Unkekulized atoms: 8 11 12\n",
      "RDKit ERROR: \n",
      "[17:45:14] Can't kekulize mol.  Unkekulized atoms: 8 11 12\n",
      "\n",
      "RDKit ERROR: [17:45:14] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:14] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:14] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[17:45:14] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:15] Can't kekulize mol.  Unkekulized atoms: 22 23 25\n",
      "RDKit ERROR: \n",
      "[17:45:15] Can't kekulize mol.  Unkekulized atoms: 22 23 25\n",
      "\n",
      "RDKit ERROR: [17:45:15] Can't kekulize mol.  Unkekulized atoms: 9 10 12\n",
      "RDKit ERROR: \n",
      "[17:45:15] Can't kekulize mol.  Unkekulized atoms: 9 10 12\n",
      "\n",
      "RDKit ERROR: [17:45:15] Can't kekulize mol.  Unkekulized atoms: 11 16 17\n",
      "RDKit ERROR: \n",
      "[17:45:15] Can't kekulize mol.  Unkekulized atoms: 11 16 17\n",
      "\n",
      "RDKit ERROR: [17:45:15] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[17:45:15] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:15] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:15] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:15] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[17:45:15] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:15] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "RDKit ERROR: \n",
      "[17:45:15] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "\n",
      "RDKit ERROR: [17:45:16] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:16] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [17:45:16] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "RDKit ERROR: \n",
      "[17:45:16] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "\n",
      "RDKit ERROR: [17:45:17] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[17:45:17] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:17] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[17:45:17] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:17] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[17:45:17] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[17:45:19] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "[17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "RDKit ERROR: \n",
      "[17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[17:45:19] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "RDKit ERROR: \n",
      "[17:45:19] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[17:45:19] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "\n",
      "[17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "[17:45:19] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[17:45:19] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "\n",
      "[17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "RDKit ERROR: \n",
      "[17:45:19] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[17:45:19] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[17:45:19] Explicit valence for atom # 16 C, 6, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/esm-main/SeqMol_model2.py:398: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fused_x1=F.softmax(fused_x1)\n",
      "/home/wuyou/esm-main/SeqMol_model2.py:399: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fused_x2=F.softmax(fused_x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Evaluation\n",
      "train: 808.674640 val: 913.783149 test: 711374.444945\n",
      "====epoch 2\n",
      "====Evaluation\n",
      "train: 459.692747 val: 507.865730 test: 260197.055976\n",
      "====epoch 3\n",
      "====Evaluation\n",
      "train: 159.994890 val: 183.205300 test: 28693.833324\n",
      "====epoch 4\n",
      "====Evaluation\n",
      "train: 87.569750 val: 102.827805 test: 8806.018340\n",
      "====epoch 5\n",
      "====Evaluation\n",
      "train: 18.668780 val: 25.341564 test: 425.923255\n",
      "====epoch 6\n",
      "====Evaluation\n",
      "train: 27.311829 val: 26.316105 test: 883.822165\n",
      "====epoch 7\n",
      "====Evaluation\n",
      "train: 26.669668 val: 25.424953 test: 852.582890\n",
      "====epoch 8\n",
      "====Evaluation\n",
      "train: 15.612732 val: 13.038908 test: 306.683065\n",
      "====epoch 9\n",
      "training Epoch [10/10], Loss: 11.4146\n",
      "====Evaluation\n",
      "train: 4.022400 val: 0.312114 test: 29.412602\n",
      "====epoch 10\n",
      "====Evaluation\n",
      "train: 2.655725 val: 6.836276 test: 6.763040\n",
      "====epoch 11\n",
      "====Evaluation\n",
      "train: 4.970091 val: 8.979674 test: 25.350654\n",
      "====epoch 12\n",
      "====Evaluation\n",
      "train: 5.099448 val: 8.982271 test: 26.545895\n",
      "====epoch 13\n",
      "====Evaluation\n",
      "train: 5.070838 val: 8.965838 test: 26.376837\n",
      "====epoch 14\n",
      "====Evaluation\n",
      "train: 5.043466 val: 8.938466 test: 26.096423\n",
      "====epoch 15\n",
      "====Evaluation\n",
      "train: 5.006858 val: 8.901858 test: 25.723744\n",
      "====epoch 16\n",
      "====Evaluation\n",
      "train: 4.962391 val: 8.857391 test: 25.274663\n",
      "====epoch 17\n",
      "====Evaluation\n",
      "train: 4.911191 val: 8.806191 test: 24.762473\n",
      "====epoch 18\n",
      "====Evaluation\n",
      "train: 4.854186 val: 8.749186 test: 24.198394\n",
      "====epoch 19\n",
      "training Epoch [20/10], Loss: 4.8542\n",
      "====Evaluation\n",
      "train: 4.792154 val: 8.687154 test: 23.591949\n",
      "====epoch 20\n",
      "====Evaluation\n",
      "train: 4.725748 val: 8.620748 test: 22.951270\n",
      "====epoch 21\n",
      "====Evaluation\n",
      "train: 4.655522 val: 8.550522 test: 22.283329\n",
      "====epoch 22\n",
      "====Evaluation\n",
      "train: 4.581948 val: 8.476948 test: 21.594128\n",
      "====epoch 23\n",
      "====Evaluation\n",
      "train: 4.505432 val: 8.400432 test: 20.888852\n",
      "====epoch 24\n",
      "====Evaluation\n",
      "train: 4.426324 val: 8.321324 test: 20.171996\n",
      "====epoch 25\n",
      "====Evaluation\n",
      "train: 4.344928 val: 8.239928 test: 19.447462\n",
      "====epoch 26\n",
      "====Evaluation\n",
      "train: 4.261506 val: 8.156506 test: 18.718653\n",
      "====epoch 27\n",
      "====Evaluation\n",
      "train: 4.176289 val: 8.071289 test: 17.988532\n",
      "====epoch 28\n",
      "====Evaluation\n",
      "train: 4.089479 val: 7.984479 test: 17.259693\n",
      "====epoch 29\n",
      "training Epoch [30/10], Loss: 4.0895\n",
      "====Evaluation\n",
      "train: 4.001252 val: 7.896252 test: 16.534405\n",
      "====epoch 30\n",
      "====Evaluation\n",
      "train: 3.911765 val: 7.806765 test: 15.814657\n",
      "====epoch 31\n",
      "====Evaluation\n",
      "train: 3.821154 val: 7.716154 test: 15.102195\n",
      "====epoch 32\n",
      "====Evaluation\n",
      "train: 3.729542 val: 7.624542 test: 14.398551\n",
      "====epoch 33\n",
      "====Evaluation\n",
      "train: 3.637036 val: 7.532036 test: 13.705073\n",
      "====epoch 34\n",
      "====Evaluation\n",
      "train: 3.543732 val: 7.438732 test: 13.022947\n",
      "====epoch 35\n",
      "====Evaluation\n",
      "train: 3.449714 val: 7.344714 test: 12.353215\n",
      "====epoch 36\n",
      "====Evaluation\n",
      "train: 3.355058 val: 7.250058 test: 11.696797\n",
      "====epoch 37\n",
      "====Evaluation\n",
      "train: 3.259831 val: 7.154831 test: 11.054500\n",
      "====epoch 38\n",
      "====Evaluation\n",
      "train: 3.164093 val: 7.059093 test: 10.427039\n",
      "====epoch 39\n",
      "training Epoch [40/10], Loss: 3.1641\n",
      "====Evaluation\n",
      "train: 3.067897 val: 6.962897 test: 9.815041\n",
      "====epoch 40\n",
      "====Evaluation\n",
      "train: 2.971291 val: 6.866291 test: 9.219062\n",
      "====epoch 41\n",
      "====Evaluation\n",
      "train: 2.874318 val: 6.769318 test: 8.639589\n",
      "====epoch 42\n",
      "====Evaluation\n",
      "train: 2.777015 val: 6.672015 test: 8.077051\n",
      "====epoch 43\n",
      "====Evaluation\n",
      "train: 2.679418 val: 6.574418 test: 7.531828\n",
      "====epoch 44\n",
      "====Evaluation\n",
      "train: 2.582136 val: 6.476556 test: 7.004257\n",
      "====epoch 45\n",
      "====Evaluation\n",
      "train: 2.486309 val: 6.378566 test: 6.495189\n",
      "====epoch 46\n",
      "====Evaluation\n",
      "train: 2.392096 val: 6.280591 test: 6.005394\n",
      "====epoch 47\n",
      "====Evaluation\n",
      "train: 2.299615 val: 6.182756 test: 5.535460\n",
      "====epoch 48\n",
      "====Evaluation\n",
      "train: 2.208963 val: 6.085175 test: 5.085813\n",
      "====epoch 49\n",
      "training Epoch [50/10], Loss: 2.2090\n",
      "====Evaluation\n",
      "train: 2.120219 val: 5.987948 test: 4.656741\n",
      "====epoch 50\n",
      "====Evaluation\n",
      "train: 2.033445 val: 5.891167 test: 4.248410\n",
      "====epoch 51\n",
      "====Evaluation\n",
      "train: 1.948692 val: 5.794912 test: 3.860881\n",
      "====epoch 52\n",
      "====Evaluation\n",
      "train: 1.865996 val: 5.699256 test: 3.494118\n",
      "====epoch 53\n",
      "====Evaluation\n",
      "train: 1.785382 val: 5.604263 test: 3.148008\n",
      "====epoch 54\n",
      "====Evaluation\n",
      "train: 1.706867 val: 5.509990 test: 2.822367\n",
      "====epoch 55\n",
      "====Evaluation\n",
      "train: 1.630458 val: 5.416490 test: 2.516951\n",
      "====epoch 56\n",
      "====Evaluation\n",
      "train: 1.556156 val: 5.323808 test: 2.231463\n",
      "====epoch 57\n",
      "====Evaluation\n",
      "train: 1.483954 val: 5.231986 test: 1.965564\n",
      "====epoch 58\n",
      "====Evaluation\n",
      "train: 1.413841 val: 5.141059 test: 1.718874\n",
      "====epoch 59\n",
      "training Epoch [60/10], Loss: 1.4138\n",
      "====Evaluation\n",
      "train: 1.345801 val: 5.051059 test: 1.490985\n",
      "====epoch 60\n",
      "====Evaluation\n",
      "train: 1.279812 val: 4.962015 test: 1.281459\n",
      "====epoch 61\n",
      "====Evaluation\n",
      "train: 1.215849 val: 4.873953 test: 1.089838\n",
      "====epoch 62\n",
      "====Evaluation\n",
      "train: 1.153886 val: 4.786894 test: 0.915646\n",
      "====epoch 63\n",
      "====Evaluation\n",
      "train: 1.094289 val: 4.700857 test: 0.758391\n",
      "====epoch 64\n",
      "====Evaluation\n",
      "train: 1.037884 val: 4.615968 test: 0.617746\n",
      "====epoch 65\n",
      "====Evaluation\n",
      "train: 0.984644 val: 4.532368 test: 0.493321\n",
      "====epoch 66\n",
      "====Evaluation\n",
      "train: 0.934465 val: 4.450165 test: 0.384604\n",
      "====epoch 67\n",
      "====Evaluation\n",
      "train: 0.887511 val: 4.369471 test: 0.291029\n",
      "====epoch 68\n",
      "====Evaluation\n",
      "train: 0.845248 val: 4.290471 test: 0.212034\n",
      "====epoch 69\n",
      "training Epoch [70/10], Loss: 0.8452\n",
      "====Evaluation\n",
      "train: 0.808077 val: 4.213512 test: 0.147081\n",
      "====epoch 70\n",
      "====Evaluation\n",
      "train: 0.775810 val: 4.138900 test: 0.095419\n",
      "====epoch 71\n",
      "====Evaluation\n",
      "train: 0.748193 val: 4.066908 test: 0.056125\n",
      "====epoch 72\n",
      "====Evaluation\n",
      "train: 0.724925 val: 3.997773 test: 0.028148\n",
      "====epoch 73\n",
      "====Evaluation\n",
      "train: 0.705665 val: 3.931697 test: 0.010342\n",
      "====epoch 74\n",
      "====Evaluation\n",
      "train: 0.690047 val: 3.868851 test: 0.001509\n",
      "====epoch 75\n",
      "====Evaluation\n",
      "train: 0.677691 val: 3.809377 test: 0.000425\n",
      "====epoch 76\n",
      "====Evaluation\n",
      "train: 0.668214 val: 3.753381 test: 0.005870\n",
      "====epoch 77\n",
      "====Evaluation\n",
      "train: 0.661234 val: 3.700946 test: 0.016655\n",
      "====epoch 78\n",
      "====Evaluation\n",
      "train: 0.656383 val: 3.652124 test: 0.031640\n",
      "====epoch 79\n",
      "training Epoch [80/10], Loss: 0.6564\n",
      "====Evaluation\n",
      "train: 0.653310 val: 3.606942 test: 0.049755\n",
      "====epoch 80\n",
      "====Evaluation\n",
      "train: 0.651685 val: 3.565400 test: 0.070013\n",
      "====epoch 81\n",
      "====Evaluation\n",
      "train: 0.651206 val: 3.527478 test: 0.091520\n",
      "====epoch 82\n",
      "====Evaluation\n",
      "train: 0.651600 val: 3.493129 test: 0.113482\n",
      "====epoch 83\n",
      "====Evaluation\n",
      "train: 0.652624 val: 3.462291 test: 0.135210\n",
      "====epoch 84\n",
      "====Evaluation\n",
      "train: 0.654066 val: 3.434879 test: 0.156121\n",
      "====epoch 85\n",
      "====Evaluation\n",
      "train: 0.655746 val: 3.410793 test: 0.175734\n",
      "====epoch 86\n",
      "====Evaluation\n",
      "train: 0.657516 val: 3.389919 test: 0.193671\n",
      "====epoch 87\n",
      "====Evaluation\n",
      "train: 0.659253 val: 3.372128 test: 0.209647\n",
      "====epoch 88\n",
      "====Evaluation\n",
      "train: 0.660865 val: 3.357280 test: 0.223464\n",
      "====epoch 89\n",
      "training Epoch [90/10], Loss: 0.6609\n",
      "====Evaluation\n",
      "train: 0.662281 val: 3.345226 test: 0.235006\n",
      "====epoch 90\n",
      "====Evaluation\n",
      "train: 0.663455 val: 3.335809 test: 0.244225\n",
      "====epoch 91\n",
      "====Evaluation\n",
      "train: 0.664358 val: 3.328864 test: 0.251137\n",
      "====epoch 92\n",
      "====Evaluation\n",
      "train: 0.664980 val: 3.324225 test: 0.255809\n",
      "====epoch 93\n",
      "====Evaluation\n",
      "train: 0.665321 val: 3.321719 test: 0.258349\n",
      "====epoch 94\n",
      "====Evaluation\n",
      "train: 0.665396 val: 3.321176 test: 0.258902\n",
      "====epoch 95\n",
      "====Evaluation\n",
      "train: 0.665225 val: 3.322421 test: 0.257636\n",
      "====epoch 96\n",
      "====Evaluation\n",
      "train: 0.664837 val: 3.325284 test: 0.254738\n",
      "====epoch 97\n",
      "====Evaluation\n"
     ]
    }
   ],
   "source": [
    "from loader1 import MoleculeDatasetBig, SeqDataset,SeqMolDataset,SmileDataset#########################\n",
    "import torch\n",
    "import torch\n",
    "#import args\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "#from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from splitters import scaffold_split,scaffold_split_1\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "import esm\n",
    "\n",
    "#from SeqMolModel import InteractionModel,InteractionModel_1,SequenceModel,InteractionModel_4\n",
    "#from SeqMolSmile import InteractionModel_4\n",
    "#from SeqMolModel import InteractionModel_4\n",
    "from SeqMol_model2 import InteractionModel_4\n",
    "print(torch.cuda.is_available())\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda._initialized = True\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')#0000\n",
    "parser.add_argument('--batch_size', type=int, default=64,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=150,\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "parser.add_argument('--decay', type=float, default=0,\n",
    "                        help='weight decay (default: 0)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "parser.add_argument('--dataset', type=str, default = 'affinity', help='root directory of dataset. For now, only classification.')\n",
    "#parser.add_argument('--input_model_file', type=str, default = 'None', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--input_model_file', type=str, default = 'Mole-BERT', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--filename', type=str, default = '', help='output filename')\n",
    "parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "parser.add_argument('--runseed', type=int, default=0, help = \"Seed for minibatch selection, random initialization.\")\n",
    "parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "parser.add_argument('--eval_train', type=int, default = 1, help='evaluating training or not')\n",
    "parser.add_argument('--num_workers', type=int, default = 4, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default = 12, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_layer', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--d_dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--n_embd', type=int, default = 768, help='number of workers for dataset loading')\n",
    "parser.add_argument('--dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--lr_start', type=float, default =  3e-5, help='number of workers for dataset loading')\n",
    "parser.add_argument('--max_epochs', type=int, default = 500, help='number of workers for dataset loading')\n",
    "parser.add_argument('--num_feats', type=int, default = 32, help='number of workers for dataset loading')\n",
    "parser.add_argument('--checkpoint_every', type=int, default = 100, help='number of workers for dataset loading')\n",
    "parser.add_argument('--seed_path', type=str, default =  'data/checkpoints/N-Step-Checkpoint_3_30000.ckpt', help='number of workers for dataset loading')\n",
    "parser.add_argument('--dims', type=list, default = [ 768, 768, 768, 1], help='number of workers for dataset loading')\n",
    "\n",
    "args = parser.parse_args(args=[])###############33\n",
    "\n",
    "\n",
    "#load pretained model of Mole-Bert\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.runseed)\n",
    "\n",
    "num_tasks=1\n",
    "# Load ESM-2 model\n",
    "protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "protein_model.to(device)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "#print(protein_alphabet)\n",
    "#alphabet = esm.Alphabet.from_architecture(model_data[\"args\"].arch)\n",
    "#batch_converter = alphabet.get_batch_converter()\n",
    "protein_model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "#self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "molecular_model = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "###################################\n",
    "if not args.input_model_file == \"None\":###############\n",
    "    print('Not from scratch')\n",
    "    molecular_model.from_pretrained('model_gin/{}.pth'.format(args.input_model_file))\n",
    "    print('rese:model_gin')\n",
    "molecular_model.to(device)\n",
    "for i,p in enumerate(molecular_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "\n",
    "'''\n",
    "model_param_group = []\n",
    "model_param_group.append({\"params\": molecular_model.gnn.parameters()})\n",
    "if args.graph_pooling == \"attention\":\n",
    "    model_param_group.append({\"params\": molecular_model.pool.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "'''\n",
    "'''\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「Dreamcatcher风」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/Wind_2028/article/details/120541017   \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "model_param_group.append({\"params\": molecular_model.graph_pred_linear.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "optimizer = optim.Adam(model_param_group, lr=0.01, weight_decay=args.decay)#############\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
    "#from finetune.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "#from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
    "from functools import partial\n",
    "from apex import optimizers\n",
    "import subprocess\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "#from utils import normalize_smiles\n",
    "import sys\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "from tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "#from utils import normalize_smiles\n",
    "# create a function (this my favorite choice)\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(LightningModule, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        #self.hparams = config\n",
    "        #self.mode = config.mode\n",
    "        self.save_hyperparameters(config)\n",
    "        self.tokenizer=tokenizer\n",
    "        '''\n",
    "        self.min_loss = {\n",
    "            self.hparams.measure_name + \"min_valid_loss\": torch.finfo(torch.float32).max,\n",
    "            self.hparams.measure_name + \"min_epoch\": 0,\n",
    "        }\n",
    "        '''\n",
    "        # Word embeddings layer\n",
    "        n_vocab, d_emb = len(tokenizer.vocab), config.n_embd\n",
    "        # input embedding stem\n",
    "        \n",
    "        builder = rotate_builder.from_kwargs(\n",
    "            n_layers=config.n_layer,\n",
    "            n_heads=config.n_head,\n",
    "            query_dimensions=config.n_embd//config.n_head,\n",
    "            value_dimensions=config.n_embd//config.n_head,\n",
    "            feed_forward_dimensions=config.n_embd,\n",
    "            attention_type='linear',\n",
    "            feature_map=partial(GeneralizedRandomFeatures, n_dims=config.num_feats),\n",
    "            activation='gelu',\n",
    "            )\n",
    "        self.pos_emb = None\n",
    "        self.tok_emb = nn.Embedding(n_vocab, config.n_embd)\n",
    "        #print('self.tok_emb:',self.tok_emb)\n",
    "        self.drop = nn.Dropout(config.d_dropout)\n",
    "        \n",
    "        ## transformer\n",
    "        self.blocks = builder.get()\n",
    "        #self.lang_model = self.lm_layer(config.n_embd, n_vocab)\n",
    "        #self.train_config = config\n",
    "        #if we are starting from scratch set seeds\n",
    "        #########################################\n",
    "        # protein_emb_dim, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "        #########################################\n",
    "        '''\n",
    "        self.fcs = []  \n",
    "        self.loss = torch.nn.L1Loss()\n",
    "        self.net = self.Net(\n",
    "            config.n_embd, dims=config.dims, dropout=config.dropout,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        dims = [150, 50, 50, 2]\n",
    "\n",
    "\n",
    "        def __init__(self, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.desc_skip_connection = True \n",
    "            self.fcs = []  # nn.ModuleList()\n",
    "            #print('dropout is {}'.format(dropout))\n",
    "\n",
    "            self.fc1 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.relu1 = nn.GELU()\n",
    "            self.fc2 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.relu2 = nn.GELU()\n",
    "            self.final = nn.Linear(smiles_embed_dim, 1)\n",
    "\n",
    "        def forward(self, smiles_emb):\n",
    "            x_out = self.fc1(smiles_emb)\n",
    "            x_out = self.dropout1(x_out)\n",
    "            x_out = self.relu1(x_out)\n",
    "\n",
    "            if self.desc_skip_connection is True:\n",
    "                x_out = x_out + smiles_emb\n",
    "\n",
    "            z = self.fc2(x_out)\n",
    "            z = self.dropout2(z)\n",
    "            z = self.relu2(z)\n",
    "            if self.desc_skip_connection is True:\n",
    "                z = self.final(z + x_out)\n",
    "            else:\n",
    "                z = self.final(z)\n",
    "\n",
    "            return z\n",
    "\n",
    "    class lm_layer(nn.Module):\n",
    "        def __init__(self, n_embd, n_vocab):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Linear(n_embd, n_embd)\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.head = nn.Linear(n_embd, n_vocab, bias=False)\n",
    "        def forward(self, tensor):\n",
    "            tensor = self.embed(tensor)\n",
    "            tensor = F.gelu(tensor)\n",
    "            tensor = self.ln_f(tensor)\n",
    "            tensor = self.head(tensor)\n",
    "            return tensor\n",
    "\n",
    "    def get_loss(self, smiles_emb, measures):\n",
    "\n",
    "        z_pred = self.net.forward(smiles_emb).squeeze()\n",
    "        measures = measures.float()\n",
    "\n",
    "        return self.loss(z_pred, measures), z_pred, measures\n",
    "    \n",
    "    \n",
    "margs = args\n",
    "tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "seed.seed_everything(margs.seed)\n",
    "if margs.seed_path == '':\n",
    "    #print(\"# training from scratch\")\n",
    "    smile_model = LightningModule(margs, tokenizer)\n",
    "else:\n",
    "    #print(\"# loaded pre-trained model from {args.seed_path}\")\n",
    "    smile_model = LightningModule(margs, tokenizer).load_from_checkpoint(margs.seed_path, strict=False, config=margs, tokenizer=tokenizer, vocab=len(tokenizer.vocab))#########################33\n",
    "\n",
    "#print('model:',smile_model)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(smile_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "    \n",
    "\n",
    "#num_tasks=1\n",
    "model= InteractionModel_4(protein_model=protein_model,molecular_model=molecular_model,protein_embd_dim=1280,num_tasks=1,device=device,mol_embd_dim=300) \n",
    "model.to(device)\n",
    "#print(model)#nice#num_tasks=1\n",
    "\n",
    "\n",
    "###################when doing geometric.data process, if there is some changes, please delete the directory process and let it generate again\n",
    "\n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset, dataset=args.dataset)###########################转换成了分子图的格式\n",
    "#print('args.dataset:',args.dataset)\n",
    "#print(gnn_dataset)\n",
    "#for i,gnn_data in enumerate(gnn_dataset):\n",
    "    #print(gnn_data)\n",
    "    \n",
    "seq_dataset=SeqDataset('dataset/affinity/processed/sequence.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#seq_dataset[2]\n",
    "\n",
    "def collate(batch):\n",
    "    #print('collate_batch:',batch)\n",
    "    #print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    #print('tokens[1]_mask:',tokens[1])\n",
    "    #print('colate###########################')\n",
    "    #for i,m in enumerate(tokens):\n",
    "            \n",
    "    #print('collate_tokens_input_ids:',tokens['input_ids'])\n",
    "    #print('colate_tokens_attention_mask:',tokens['attention_mask'])\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "    \n",
    "smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#####test with chartGPT  DataSet extends two parents' classes\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MultiDatasetMixin:\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.dataset3=dataset3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset1), len(self.dataset2),len(self.dataset3))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dataset1[idx], self.dataset2[idx],self.dataset3[idx]\n",
    "\n",
    "class CustomMultiDataset(MultiDatasetMixin, Dataset):###############3extends two classes\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        MultiDatasetMixin.__init__(self, dataset1, dataset2,dataset3)\n",
    "\n",
    "\n",
    "#chartGPT太厉害了，杀了我吧\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
    "\n",
    "class MultiDataLoader:\n",
    "    def __init__(self, dataloader1, dataloader2,dataloader3):\n",
    "        self.dataloader1 = dataloader1\n",
    "        self.dataloader2 = dataloader2\n",
    "        self.dataloader3=dataloader3\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data1, data2,data3 in zip(self.dataloader1, self.dataloader2, self.dataloader3):\n",
    "            yield data1, data2, data3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader1), len(self.dataloader2), len(self.dataloader3))\n",
    "\n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.dataloader1.shuffle = shuffle\n",
    "        self.dataloader2.shuffle = shuffle\n",
    "        self.dataloader3.shuffle=shuffle\n",
    "\n",
    "        \n",
    "###########split train dataset validate dataset test dataset\n",
    "seq_gnn_smile_dataset=MultiDatasetMixin(seq_dataset,gnn_dataset,smiles_dataset)\n",
    "\n",
    "#print('seq_dataset:',seq_dataset)\n",
    "\n",
    "#seq_dataloader=DataLoader(seq_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.num_workers)\n",
    "'''\n",
    "for i ,seq in enumerate(seq_dataloader):\n",
    "    print(seq)\n",
    "'''\n",
    "if args.split == \"scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        \n",
    "        #print('smiles_list:',smiles_list)\n",
    "        mol_train_dataset, mol_valid_dataset, mol_test_dataset ,seq_train_dataset,seq_valid_dataset,seq_test_dataset,smile_train_dataset,smile_valid_dataset,smile_test_dataset= scaffold_split_1(seq_gnn_smile_dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)##########dataset\n",
    "        print(\"scaffold\")\n",
    "elif args.split == \"random\":\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random\")\n",
    "elif args.split == \"random_scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random scaffold\")\n",
    "else:\n",
    "        raise ValueError(\"Invalid split option.\")\n",
    "\n",
    "#print('++++++++++', mol_train_dataset[0])\n",
    "'''\n",
    "for i, mol in enumerate(mol_train_dataset):\n",
    "    print('mol:',mol)\n",
    "'''\n",
    "#seq_mol_train_dataset=MultiDatasetMixini(seq_train_dataset,mol_train_dataset)\n",
    "#seq_mol_valid_dataset=SeqMolDataset(seq_valid_dataset,mol_valid_dataset)\n",
    "#seq_mol_test_dataset=SeqMolDataset(seq_train_dataset,mol_test_dataset)\n",
    "seq_train_dataloader1 = DataLoader(seq_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_train_dataloader2 = GeometricDataLoader(mol_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_train_dataloader3=DataLoader(smile_train_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "seq_mol_smile_train_multi_loader = MultiDataLoader(seq_train_dataloader1, mol_train_dataloader2,smile_train_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_train_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "print('seq_train_dataset#########:',seq_train_dataset)\n",
    "\n",
    "for i,seq in enumerate(seq_train_dataloader1):\n",
    "    print(seq)\n",
    "'''\n",
    "seq_valid_dataloader1 = DataLoader(seq_valid_dataset, batch_size=args.batch_size,shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_valid_dataloader2 = GeometricDataLoader(mol_valid_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_valid_dataloader3=DataLoader(smile_valid_dataset, batch_size=args.batch_size,collate_fn=collate,shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "'''\n",
    "for i,m in enumerate(smile_train_dataloader3):\n",
    "    #print('m@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:',m)\n",
    "    break\n",
    "'''\n",
    "seq_mol_smile_valid_multi_loader = MultiDataLoader(seq_valid_dataloader1, mol_valid_dataloader2,smile_valid_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_valid_multi_loader.set_shuffle(True)\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_valid_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "for i ,(seq,mol,smile) in enumerate(seq_mol_smile_train_multi_loader):\n",
    "    print(seq)\n",
    "    print(mol)\n",
    "    print(smile)\n",
    "'''\n",
    "'''\n",
    "print('mol_dataloader:')\n",
    "for mol in mol_train_dataloader2:\n",
    "    print(mol)\n",
    "for seq in seq_train_dataloader1:\n",
    "    print(seq)\n",
    " '''   \n",
    "\n",
    "def train(args, epoch, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    save_pt='results/model2/'\n",
    "    #epoch_iter = tqdm(loader, desc=\"Iteration\")\n",
    "    for step, (A,B,C) in enumerate(loader):\n",
    "        \n",
    "        seq_data_list=[]\n",
    "        seq=A\n",
    "        lenth=len(seq)\n",
    "        for m , s in enumerate(seq):\n",
    "            seq_data_list.append((str(m),s))\n",
    "        B=B.to(device)\n",
    "        #D,E=C\n",
    "        #D=D.to(device)\n",
    "        #E=E.to(device)\n",
    "        #C=(D,E)\n",
    "        #print('D!!!!!!!!!!!!!!!!:',D)\n",
    "        #print('E#####################:',E)\n",
    "        #pred=model(seq_data_list,B,C)#model is error\n",
    "        pred=model(seq_data_list,B)#model is error\n",
    "        y_true = B.y.view(pred.shape).to(torch.float64)\n",
    "        #loss = criterion(pred, y_true)\n",
    "        \n",
    "        loss1=criterion(pred,y_true)\n",
    "        #loss2=criterion(u12,u34)\n",
    "        #print('loss1{0},loss2{1}:',loss1,loss2)\n",
    "        loss=loss1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #epoch_iter.set_description(f\"Epoch: {epoch} tloss: {loss:.4f}\")\n",
    "        nn=(epoch+1)//10\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'training Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')\n",
    "            torch.save(model, save_pt+f'full_model_{nn}.pt')\n",
    "def eval(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            #D,E=C\n",
    "            #D=D.to(device)\n",
    "            #E=E.to(device)##################\n",
    "            #C=(D,E)\n",
    "        \n",
    "        \n",
    "            #pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            pred=model(seq_data_list,B)#model is error\n",
    "            y_true=B.y.view(pred.shape).to(torch.float64)\n",
    "            val_loss = criterion(pred, y_true)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "\n",
    "    #y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "    #y_scores = torch.cat(y_scores, dim = 0).detach().cpu().numpy()\n",
    "    '''\n",
    "    roc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        #AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:,i] == 1) > 0 and np.sum(y_true[:,i] == -1) > 0:\n",
    "            is_valid = y_true[:,i]**2 > 0\n",
    "            roc_list.append(roc_auc_score((y_true[is_valid,i] + 1)/2, y_scores[is_valid,i]))\n",
    "    if len(roc_list)==0:#########################\n",
    "        return 0\n",
    "    if len(roc_list) < y_true.shape[1]:\n",
    "        print(\"Some target is missing!\")\n",
    "        miss_ratio=(1 - float(len(roc_list))/y_true.shape[1])\n",
    "        print(\"Missing ratio: %f\" %(1 - float(len(roc_list))/y_true.shape[1]))\n",
    "    '''\n",
    "    \n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "def test(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            #D,E=C\n",
    "            #D=D.to(device)\n",
    "            #E=E.to(device)##################\n",
    "            #C=(D,E)\n",
    "        \n",
    "        \n",
    "            #pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            pred=model(seq_data_list,B)#model is error\n",
    "            y_true=B.y.view(pred.shape).to(torch.float64)\n",
    "            test_loss = mse_criterion(pred, y_true)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "\n",
    "    \n",
    "    \n",
    "    return test_loss.item()\n",
    "\n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset+'/test/', dataset=args.dataset)\n",
    "seq_dataset=SeqDataset('dataset/affinity/test/processed/sequence.csv')\n",
    "smiles_dataset=SmileDataset('dataset/affinity/test/processed/smiles.csv')\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_test_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "\n",
    "\n",
    "results_save_file='results/model2/seqmol/results_save.txt'\n",
    "\n",
    "\n",
    "import torch, gc\n",
    "#criterion = nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "criterion=nn.SmoothL1Loss()\n",
    "\n",
    "mse_criterion=nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "if not args.filename == \"\":\n",
    "    fname = 'runs/seq_mol_finetune_cls_runseed' + str(args.runseed) + '/' + args.filename\n",
    "    #delete the directory if there exists one\n",
    "    if os.path.exists(fname):\n",
    "        shutil.rmtree(fname)\n",
    "        print(\"removed the existing file.\")\n",
    "    writer = SummaryWriter(fname)\n",
    "\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    print(\"====epoch \" + str(epoch))\n",
    "        \n",
    "    train(args, epoch, model, device, seq_mol_smile_train_multi_loader, optimizer)\n",
    "\n",
    "    print(\"====Evaluation\")\n",
    "    if args.eval_train:\n",
    "        train_loss = eval(args, model, device, seq_mol_smile_train_multi_loader)\n",
    "    else:\n",
    "        print(\"omit the training accuracy computation\")\n",
    "        train_loss = 0\n",
    "    val_loss = eval(args, model, device, seq_mol_smile_valid_multi_loader)\n",
    "    test_loss = test(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "    with open(results_save_file, 'w+') as f:\n",
    "        f.write(str(epoch)+'\\t'+str(train_loss)+'\\t'+str(val_loss)+'\\n')\n",
    "        #f.write(epoch)\n",
    "        #f.write('\\t')\n",
    "        #f.write(train_loss)\n",
    "        #f.write('\\t')\n",
    "        #f.write(val_loss)\n",
    "        #f.write('\\n')\n",
    "    print(\"train: %f val: %f test: %f\" %(train_loss, val_loss, test_loss))\n",
    "    \n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset+'/test/', dataset=args.dataset)\n",
    "seq_dataset=SeqDataset('dataset/affinity/test/processed/sequence.csv')\n",
    "smiles_dataset=SmileDataset('dataset/affinity/test/processed/smiles.csv')\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_test_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "\n",
    "\n",
    "test_loss = eval(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "print(\"test: %f \" %(test_loss)) \n",
    "#test: 0.044179 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49882495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
