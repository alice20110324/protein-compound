{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d746610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.multiprocessing as mp\n",
    "from torch.cuda.amp import GradScaler\n",
    "from loader1 import MoleculeDataset,MoleculeDatasetBig, SeqDataset,SeqMolDataset,SmileDataset#########################\n",
    "import torch\n",
    "import torch\n",
    "#import args\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "#from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "#from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "\n",
    "\n",
    "from splitters import scaffold_split,scaffold_split_1\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "#import esm2_t33_650M_UR50D\n",
    "import esm\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
    "#from finetune.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "#from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
    "from functools import partial\n",
    "from apex import optimizers\n",
    "import subprocess\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "#from utils import normalize_smiles\n",
    "import sys\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "from tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "#from SeqMolModel import InteractionModel,InteractionModel_1,SequenceModel,InteractionModel_4\n",
    "#from SeqMolSmile import InteractionModel_4\n",
    "#from SeqMolModel import InteractionModel_4\n",
    "from SeqMolSmile_model2 import InteractionModel_4\n",
    "#print(torch.cuda.is_available())\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda._initialized = True\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int,default=0,\n",
    "                        help='which gpu to use if any (default: 0)')#0000\n",
    "parser.add_argument('--gpu',default='0,1,2')\n",
    "parser.add_argument('--batch_size', type=int, default=4,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "parser.add_argument('--decay', type=float, default=0,\n",
    "                        help='weight decay (default: 0)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "parser.add_argument('--dataset', type=str, default = 'davis', help='root directory of dataset. For now, only classification.')\n",
    "#parser.add_argument('--input_model_file', type=str, default = 'None', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--input_model_file', type=str, default = 'Mole-BERT', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--filename', type=str, default = '', help='output filename')\n",
    "parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "parser.add_argument('--runseed', type=int, default=0, help = \"Seed for minibatch selection, random initialization.\")\n",
    "parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "parser.add_argument('--eval_train', type=int, default = 1, help='evaluating training or not')\n",
    "parser.add_argument('--num_workers', type=int, default = 0, help='number of workers for dataset loading')\n",
    "#parser.add_argument('--gpu', type=int, default=0, help='')\n",
    "parser.add_argument('--rank',type=int,default=0,help='')\n",
    "parser.add_argument('--world_size', type=float,default=0.1,help='')\n",
    "parser.add_argument('--dist_backend ',type=str, default='nccl',help='')\n",
    "parser.add_argument('--model_protein',type=nn.Module)\n",
    "parser.add_argument('--n_head', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--local_rank',type=int,default=0,help='')\n",
    "parser.add_argument('--n_layer', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--d_dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--n_embd', type=int, default = 768, help='number of workers for dataset loading')\n",
    "parser.add_argument('--dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--lr_start', type=float, default =  3e-5, help='number of workers for dataset loading')\n",
    "parser.add_argument('--max_epochs', type=int, default = 500, help='number of workers for dataset loading')\n",
    "parser.add_argument('--num_feats', type=int, default = 32, help='number of workers for dataset loading')\n",
    "parser.add_argument('--checkpoint_every', type=int, default = 100, help='number of workers for dataset loading')\n",
    "parser.add_argument('--seed_path', type=str, default =  'data/checkpoints/N-Step-Checkpoint_3_30000.ckpt', help='number of workers for dataset loading')\n",
    "parser.add_argument('--dims', type=list, default = [ 768, 768, 768, 1], help='number of workers for dataset loading')\n",
    "\n",
    "args = parser.parse_args(args=[])###############33\n",
    "import torch.nn.functional as F\n",
    "num_epochs=200\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def train(teacher_model, student_model,trainloader,teacher_model_optimizer,student_model_optimizer):\n",
    "    \n",
    "        for i, inputs in enumerate(trainloader):\n",
    "            \n",
    "            #teacher_model_optimizer.zero_grad()\n",
    "            # 生成教师和学生模型的输出\n",
    "            student_model_optimizer.zero_grad()\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            student_outputs = student_model(inputs)\n",
    "\n",
    "            # 标准的交叉熵损失\n",
    "            #loss_ce = criterion(student_outputs, labels)\n",
    "\n",
    "            # 教师机与学生机输出的损失（比如使用均方误差）\n",
    "            loss_kd = F.mse_loss(student_outputs, teacher_outputs.detach())\n",
    "\n",
    "            # 组合两种损失\n",
    "            #loss = loss_ce + alpha * loss_kd  # alpha 是一个超参数，用于平衡两种损失\n",
    "            loss = loss_kd \n",
    "            '''\n",
    "            if epoch % save_interval == 0:\n",
    "                # 保存模型参数到文件\n",
    "                torch.save(model.state_dict(), 'model_epoch_{}.pth'.format(epoch))\n",
    "            '''\n",
    "            print(\"train_loss:{0} at {1} epoch.\".format(loss,epoch))\n",
    "                \n",
    "            # 计算损失并进行反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step\n",
    "            \n",
    "from ModelDistillation import *\n",
    "import GPUtil\n",
    "def  getGPU():\n",
    "    # 获取所有GPU的详细信息\n",
    "    gpus = GPUtil.getGPUs()\n",
    "\n",
    "    # 打印每个GPU的信息\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU: {gpu.name}\")\n",
    "        print(f\"  GPU ID: {gpu.id}\")\n",
    "        print(f\"  显存总量: {gpu.memoryTotal}MB\")\n",
    "        print(f\"  显存使用: {gpu.memoryUsed}MB\")\n",
    "        print(f\"  显存空闲: {gpu.memoryFree}MB\")\n",
    "        print(f\"  GPU负载: {gpu.load*100}%\")\n",
    "\n",
    "def trainSeq(epoch,protein_model,trainloader,student_model,student_model_optimizer,protein_embd_dim):\n",
    "        gc.collect()\n",
    "        loss=0\n",
    "        inputs_1500_list=[]\n",
    "        for i, inputs in enumerate(trainloader):\n",
    "            \n",
    "                seq_data_list=[]\n",
    "                seq=inputs\n",
    "                lenth=len(seq)\n",
    "                #print('seq type:',type(seq)) \n",
    "                \n",
    "                for m , s in enumerate(seq):\n",
    "                    seq_data_list.append((str(m),s))\n",
    "                #student_model_optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                del m\n",
    "                del s\n",
    "                #inputs.cuda()\n",
    "                \n",
    "                student_model_optimizer.zero_grad()\n",
    "                teacher_outputs = SeqInferTeacher(protein_model,seq_data_list,protein_embd_dim)\n",
    "                student_outputs = student_model(inputs)\n",
    "                del seq_data_list\n",
    "                del inputs\n",
    "                loss_kd = F.mse_loss(student_outputs.detach(), teacher_outputs.detach())\n",
    "            \n",
    "                loss = loss_kd \n",
    "            \n",
    "                print(\"train_loss:{0} at{1} epoch.\".format(loss,epoch))\n",
    "                getGPU()\n",
    "                gc.collect()\n",
    "                #print(loss.requires_grad)  # 应该为 True\n",
    "                #print(loss.grad_fn)        # 不应该为 None\n",
    "                # 计算损失并进行反向传播\n",
    "                loss.requires_grad_(True)\n",
    "                loss.backward()\n",
    "                student_model_optimizer.step()\n",
    "        print(\"train_loss:{0} at{1} epoch.##############\".format(loss,epoch))\n",
    "\n",
    "g_dataset='two'\n",
    "#gnn_train_dataset = TestbedDataset(root='dataset/pretrain_dataset/', dataset=g_dataset+'_train', xd=train_drugs, xt=train_prots, y=train_Y,smile_graph=smile_graph)\n",
    "seq_train_dataset=SeqDataset('/media/ext_disk/zhenfang/dataset/pretrain_dataset/processed/'+g_dataset+'_train_sequence.csv')\n",
    "#smiles_train_dataset=SmileDataset('dataset/pretrain_dataset/processed/'+g_dataset+'_train_smiles.csv')\n",
    "\n",
    "\n",
    "\n",
    "def trainSeqStudent(epoch,trainloader,student_model,student_model_optimizer,protein_embd_dim):\n",
    "        gc.collect()\n",
    "        loss=0\n",
    "        inputs_1500_list=[]\n",
    "        for i, inputs in enumerate(trainloader):\n",
    "            \n",
    "                \n",
    "                student_model_optimizer.zero_grad()\n",
    "                \n",
    "                student_outputs = student_model(inputs)\n",
    "                \n",
    "                \n",
    "            \n",
    "                print(\"train_output:{0} at {1} epoch.\".format(student_outputs.shape,epoch))\n",
    "                getGPU()\n",
    "                gc.collect()\n",
    "                \n",
    "        print(\"train_loss:{0} at {1} epoch.##############\".format(student_outputs.shape,epoch))\n",
    "g_dataset='two'\n",
    "#gnn_train_dataset = TestbedDataset(root='dataset/pretrain_dataset/', dataset=g_dataset+'_train', xd=train_drugs, xt=train_prots, y=train_Y,smile_graph=smile_graph)\n",
    "seq_train_dataset=SeqDataset('/media/ext_disk/zhenfang/dataset/pretrain_dataset/processed/'+g_dataset+'_train_sequence.csv')\n",
    "#smiles_train_dataset=SmileDataset('dataset/pretrain_dataset/processed/'+g_dataset+'_train_smiles.csv')\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "import torch_geometric\n",
    "\n",
    "#gnn_train_dataloader=torch_geometric.data.DataLoader(gnn_train_dataset,batch_size=args.batch_size,num_workers=args.num_workers,shuffle=True)\n",
    "#seq_train_dataloader=torch.utils.data.DataLoader(seq_train_dataset,batch_size=args.batch_size,num_workers=args.num_workers,shuffle=True)\n",
    "#smiles_train_dataloader=torch.utils.data.DataLoader(smiles_train_dataset,batch_size=args.batch_size,num_workers=args.num_workers,shuffle=True)\n",
    "\n",
    "#del seq_train_dataset\n",
    "# In[18]:\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from ModelDistillation import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def prepare():\n",
    "    \n",
    "\n",
    "    # 下面几行是新加的，用于启动多进程 DDP\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'  # 0号机器的IP\n",
    "    os.environ['MASTER_PORT'] = '19198'  # 0号机器的可用端口\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu  # 使用哪些GPU\n",
    "    world_size = torch.cuda.device_count()\n",
    "    os.environ['WORLD_SIZE'] = str(world_size)\n",
    "    os.environ['WORLD_SIZE'] = str(world_size)\n",
    "    os.environ['WORLD_SIZE'] = str(world_size)\n",
    "    return args\n",
    "\n",
    "\n",
    "def init_ddp(local_rank):\n",
    "    # 有了这一句之后，在转换device的时候直接使用 a=a.cuda()即可，否则要用a=a.cuda(local+rank)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    os.environ['RANK'] = str(local_rank)\n",
    "    dist.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "\n",
    "def get_ddp_generator(seed=3407):\n",
    "    local_rank = dist.get_rank()\n",
    "    #print('local_rank:',local_rank)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed + local_rank)\n",
    "    return g\n",
    "\n",
    "\n",
    "def main(local_rank, args):\n",
    "    init_ddp(local_rank)  ### 进程初始化\n",
    "    #model = ConvNet().cuda()  ### 模型的 forward 方法变了\n",
    "    protein_embd_dim=320\n",
    "    #seq_embd_dim=1280\n",
    "    output_embd_dim=256\n",
    "    \n",
    "   \n",
    "            \n",
    "    \n",
    "    student_model=SeqMLPStudent(1500,10,320,0.5)\n",
    "    \n",
    "    \n",
    "    student_model.cuda()\n",
    "    #student_model = nn.SyncBatchNorm.convert_sync_batchnorm(student_model)  ### 转换模型的 BN 层\n",
    "    student_model = nn.parallel.DistributedDataParallel(student_model,\n",
    "                                                device_ids=[local_rank])\n",
    "    \n",
    "    \n",
    "    student_model_optimizer = optim.SGD(student_model.parameters(), lr=0.1, momentum=0.9)                                                                                                       \n",
    "    \n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        seq_train_dataset)  ### 用于在 DDP 环境下采样\n",
    "    g = get_ddp_generator()  ###\n",
    "    train_dloader = torch.utils.data.DataLoader(\n",
    "        dataset=seq_train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,  ### shuffle 通过 sampler 完成\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "        generator=g)  ### 添加额外的 generator\n",
    "    \n",
    "    dropout=0.5\n",
    "    for epoch in range(args.epochs):\n",
    "        if local_rank == 0:  ### 防止每个进程都输出一次\n",
    "            print(f'begin training of epoch {epoch + 1}/{args.epochs}')\n",
    "        train_dloader.sampler.set_epoch(epoch)  ### 防止采样出 bug\n",
    "        trainSeqStudent(epoch,train_dloader,student_model,student_model_optimizer,protein_embd_dim)\n",
    "        if epoch % 50 == 0:\n",
    "            # 保存模型参数到文件\n",
    "            #torch.save(model.state_dict(), 'model_epoch_{}.pth'.format(epoch))\n",
    "            #print(f'begin testing')\n",
    "            \n",
    "            print('epoch333333333333333:',epoch)\n",
    "    dist.destroy_process_group\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "import time\n",
    "import gc\n",
    "if __name__ == '__main__':\n",
    "    # 设置 PYTORCH_CUDA_ALLOC_CONF 环境变量\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'  # 示例值，您可以根据需要调整\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    prepare()\n",
    "    time_start = time.time()\n",
    "    mp.spawn(main, args=(args, ), nprocs=torch.cuda.device_count())\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print(f'\\ntime elapsed: {time_elapsed:.2f} seconds')\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
