{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d7e94e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: torch.Size([32, 10, 1280])\n",
      "x2: torch.Size([32, 20, 300])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c247f9e98208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x1:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x2:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_head_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"输出形状:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c247f9e98208>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Concatenate the multi-head outputs and linearly transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, num_heads, head_dim):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.total_dim = num_heads * head_dim\n",
    "        \n",
    "        # Linear transformations for queries and keys\n",
    "        self.linear_q1 = nn.Linear(input_dim1, self.total_dim)\n",
    "        self.linear_k2 = nn.Linear(input_dim2, self.total_dim)\n",
    "        \n",
    "        # Linear transformation for values\n",
    "        self.linear_v1 = nn.Linear(input_dim1, self.total_dim)\n",
    "        \n",
    "        # Linear transformation for the output of multi-head attention\n",
    "        self.linear_out = nn.Linear(self.total_dim, self.total_dim)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input tensor to separate heads\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Linearly transform queries, keys, and values\n",
    "        q1 = self.linear_q1(x1)\n",
    "        k2 = self.linear_k2(x2)\n",
    "        v1 = self.linear_v1(x1)\n",
    "        \n",
    "        # Split heads for queries, keys, and values\n",
    "        q1 = self.split_heads(q1)\n",
    "        k2 = self.split_heads(k2)\n",
    "        v1 = self.split_heads(v1)\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attn_weights = torch.matmul(q1, k2.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)\n",
    "        output = torch.matmul(attn_weights, v1)\n",
    "        \n",
    "        # Concatenate the multi-head outputs and linearly transform\n",
    "        output = output.permute(0, 2, 1, 3).contiguous().view(x1.size(0), -1, self.total_dim)\n",
    "        output = self.linear_out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 示例用法\n",
    "input_dim1 = 1280\n",
    "input_dim2 = 300\n",
    "num_heads = 4\n",
    "head_dim = 64\n",
    "\n",
    "multi_head_attention = MultiHeadCrossAttention(input_dim1, input_dim2, num_heads, head_dim)\n",
    "\n",
    "# 输入示例\n",
    "x1 = torch.randn(32, 10, input_dim1)  # 输入1，形状为(batch_size, sequence_length, input_dim1)\n",
    "x2 = torch.randn(32, 20, input_dim2)  # 输入2，形状为(batch_size, sequence_length, input_dim2)\n",
    "print('x1:',x1.shape)\n",
    "print('x2:',x2.shape)\n",
    "output = multi_head_attention(x1, x2)\n",
    "print(\"输出形状:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1720d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MLPWithMultiHeadCrossAttention(\n",
      "  (multi_head_attention): MultiHeadCrossAttention(\n",
      "    (linear_q1): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (linear_k2): Linear(in_features=300, out_features=256, bias=True)\n",
      "    (linear_v1): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, num_heads, head_dim):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.total_dim = num_heads * head_dim\n",
    "        \n",
    "        # Linear transformations for queries and keys\n",
    "        self.linear_q1 = nn.Linear(input_dim1, self.total_dim)\n",
    "        self.linear_k2 = nn.Linear(input_dim2, self.total_dim)\n",
    "        \n",
    "        # Linear transformation for values\n",
    "        self.linear_v1 = nn.Linear(input_dim1, self.total_dim)\n",
    "        \n",
    "        # Linear transformation for the output of multi-head attention\n",
    "        self.linear_out = nn.Linear(self.total_dim, self.total_dim)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input tensor to separate heads\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Linearly transform queries, keys, and values\n",
    "        q1 = self.linear_q1(x1)\n",
    "        k2 = self.linear_k2(x2)\n",
    "        v1 = self.linear_v1(x1)\n",
    "        print('q1:',q1.shape)\n",
    "        print('k2:',k2.shape)\n",
    "        print('v1:',v1.shape)\n",
    "        # Split heads for queries, keys, and values\n",
    "        q1 = self.split_heads(q1)\n",
    "        k2 = self.split_heads(k2)\n",
    "        v1 = self.split_heads(v1)\n",
    "        print('v1_split:',v1.shape)\n",
    "        print('q1_split:',q1.shape)\n",
    "        print('k2_split:',k2.shape)\n",
    "        # Compute scaled dot-product attention\n",
    "        attn_weights = torch.matmul(q1, k2.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)\n",
    "        print('attn_weight:',attn_weights.shape)\n",
    "        print('v1:',v1.shape)\n",
    "        output = torch.matmul(attn_weights, v1)\n",
    "        \n",
    "        # Concatenate the multi-head outputs and linearly transform\n",
    "        output = output.permute(0, 2, 1, 3).contiguous().view(x1.size(0), -1, self.total_dim)\n",
    "        print('output:',output.shape)\n",
    "        output = self.linear_out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MLPWithMultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, num_heads, head_dim, hidden_dim, num_classes):\n",
    "        super(MLPWithMultiHeadCrossAttention, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadCrossAttention(input_dim1, input_dim2, num_heads, head_dim)\n",
    "        self.fc1 = nn.Linear(num_heads * head_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        print('x1:',x1.shape)\n",
    "        print('x2:',x2.shape)\n",
    "        \n",
    "        # Multi-Head Cross Attention\n",
    "        attention_output = self.multi_head_attention(x1, x2)\n",
    "        print('attention_output:',attention_output.shape)\n",
    "        # Flatten and pass through MLP layers\n",
    "        flattened_output = attention_output.view(attention_output.size(0), -1)\n",
    "        print('flattened_output:',flattened_output.shape)\n",
    "        out = self.fc1(flattened_output)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 示例用法\n",
    "input_dim1 = 1280\n",
    "input_dim2 = 300\n",
    "num_heads = 4\n",
    "head_dim = 64\n",
    "hidden_dim = 256\n",
    "num_classes = 10  # 替换为你的分类类别数\n",
    "num_epoches=200\n",
    "\n",
    "model = MLPWithMultiHeadCrossAttention(input_dim1, input_dim2, num_heads, head_dim, hidden_dim, num_classes)\n",
    "print('model:',model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# 输入示例\n",
    "#x1 = torch.randn(32, 10, input_dim1)  # 输入1，形状为(batch_size, sequence_length, input_dim1)\n",
    "#x2 = torch.randn(32, 20, input_dim2)  # 输入2，形状为(batch_size, sequence_length, input_dim2)\n",
    "\n",
    "#output = model(x1, x2)\n",
    "#print(\"输出形状:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87b3eab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 生成 2x3 的随机矩阵\n",
    "#a=np.random.randint(100,1028)\n",
    "#a = np.random.rand(100, 1028)\n",
    "#print(a)\n",
    "a=np.random.randint(low=0,high=1000,size=(100,27,1280),dtype='int')\n",
    "a=torch.from_numpy(a)\n",
    "a=torch.tensor(a,dtype=torch.float)\n",
    "\n",
    "#b=np.random.rand(100,300)\n",
    "#print(b)\n",
    "b=np.random.randint(low=0,high=1000,size=(100,35,300),dtype='int')\n",
    "b=torch.from_numpy(b)\n",
    "b=torch.tensor(b,dtype=torch.float)\n",
    "\n",
    "label=np.random.randint(low=6,high=10,size=(100),dtype='int')\n",
    "#print(label,label.shape)\n",
    "label=torch.from_numpy(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ce505a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,a,b,label):\n",
    "        #super.__init__()\n",
    "        self.a=a\n",
    "        self.b=b\n",
    "        self.label=label\n",
    "    def __getitem__(self,idx):\n",
    "        x1=self.a[idx]\n",
    "        x2=self.b[idx]\n",
    "        label=self.label[idx]\n",
    "        return (x1,x2,label)\n",
    "    def __len__(self,):\n",
    "        return len(self.a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48685322",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=myDataset(a,b,label)\n",
    "train_loader=DataLoader(train_dataset,batch_size=32,num_workers=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77437f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: torch.Size([32, 27, 1280])\n",
      "x2: torch.Size([32, 35, 300])\n",
      "q1: torch.Size([32, 27, 256])\n",
      "k2: torch.Size([32, 35, 256])\n",
      "v1: torch.Size([32, 27, 256])\n",
      "v1_split: torch.Size([32, 4, 27, 64])\n",
      "q1_split: torch.Size([32, 4, 27, 64])\n",
      "k2_split: torch.Size([32, 4, 35, 64])\n",
      "attn_weight: torch.Size([32, 4, 27, 35])\n",
      "v1: torch.Size([32, 4, 27, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-147eab5a8ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# dataloader是你的数据加载器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-fd12f53538c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Multi-Head Cross Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_head_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attention_output:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Flatten and pass through MLP layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-fd12f53538c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attn_weight:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'v1:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Concatenate the multi-head outputs and linearly transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    for x1_batch, x2_batch, labels in train_loader:  # dataloader是你的数据加载器\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x1_batch, x2_batch)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31004c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 10, 10])\n",
      "torch.Size([10, 4, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "q1=torch.randn(10,4,10,64)\n",
    "k2=torch.randn(10,4,10,64)\n",
    "v1=torch.randn(10,4,10,64)\n",
    "#print(q1)\n",
    "#print(k2)\n",
    "head_dim=4\n",
    "attn_weights = torch.matmul(q1, k2.permute(0, 1, 3, 2)) / (head_dim ** 0.5)\n",
    "print(attn_weights.shape)\n",
    "output = torch.matmul(attn_weights, v1)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48a637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建两个四维张量，形状分别为 (batch_size, channels, height, width)\n",
    "input1 = torch.randn(32, 3, 64, 64)\n",
    "input2 = torch.randn(32, 3, 64, 64)\n",
    "\n",
    "# 执行矩阵乘法操作\n",
    "result = torch.matmul(input1, input2)\n",
    "\n",
    "# 输出结果的形状\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "02ac8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # n_heads：多头注意力的数量\n",
    "    # hid_dim：每个词输出的向量维度\n",
    "    def __init__(self, input_dim1, input_dim2, n_heads,head_dim, dropout=0.4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 强制 hid_dim 必须整除 h\n",
    "        assert head_dim % n_heads == 0\n",
    "        # 定义 W_q 矩阵\n",
    "        self.w_q = nn.Linear(input_dim1, head_dim)\n",
    "        # 定义 W_k 矩阵\n",
    "        self.w_k = nn.Linear(input_dim2, head_dim)\n",
    "        # 定义 W_v 矩阵\n",
    "        self.w_v = nn.Linear(input_dim1, head_dim)\n",
    "        self.fc = nn.Linear(head_dim, head_dim)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        # 缩放\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([head_dim // n_heads]))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # K: [64,10,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维\n",
    "        # V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维\n",
    "        # Q: [64,12,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维\n",
    "        bsz = query.shape[0]\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        print('Q,K,V:',Q.shape,K.shape,V.shape)\n",
    "        # 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵\n",
    "        # 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50\n",
    "        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度\n",
    "        # K: [64,10,300] 拆分多组注意力 -> [64,10,6,50] 转置得到 -> [64,6,10,50]\n",
    "        # V: [64,10,300] 拆分多组注意力 -> [64,10,6,50] 转置得到 -> [64,6,10,50]\n",
    "        # Q: [64,12,300] 拆分多组注意力 -> [64,12,6,50] 转置得到 -> [64,6,12,50]\n",
    "        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算\n",
    "        Q = Q.view(bsz, -1, self.n_heads, self.head_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        print('Q:',Q.shape)\n",
    "        K = K.view(bsz, -1, self.n_heads, self.head_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        V = V.view(bsz, -1, self.n_heads, self.head_dim //\n",
    "                   self.n_heads).permute(0, 2, 1, 3)\n",
    "        print('Q,K,V:',Q.shape,K.shape,V.shape)\n",
    "        # 第 1 步：Q 乘以 K的转置，除以scale\n",
    "        # [64,6,12,50] * [64,6,50,10] = [64,6,12,10]\n",
    "        # attention：[64,6,12,10]\n",
    "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        print('attention:',attention.shape)\n",
    "        # 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。\n",
    "        # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax\n",
    "        # attention: [64,6,12,10]\n",
    "        attention = self.do(torch.softmax(attention, dim=-1))\n",
    "\n",
    "        # 第三步，attention结果与V相乘，得到多头注意力的结果\n",
    "        # [64,6,12,10] * [64,6,10,50] = [64,6,12,50]\n",
    "        # x: [64,6,12,50]\n",
    "        #x = torch.matmul(attention, V)\n",
    "        x = torch.matmul(V,attention)\n",
    "        # 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果\n",
    "        # x: [64,6,12,50] 转置-> [64,12,6,50]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # 这里的矩阵转换就是：把多组注意力的结果拼接起来\n",
    "        # 最终结果就是 [64,12,300]\n",
    "        # x: [64,12,6,50] -> [64,12,300]\n",
    "        x = x.view(bsz, -1, self.n_heads * (self.head_dim // self.n_heads))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维\n",
    "#query = torch.rand(64, 12, 300)\n",
    "# batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维\n",
    "#key = torch.rand(64, 10, 1280)\n",
    "# batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维\n",
    "#value = torch.rand(64, 12, 300)\n",
    "#attention = MultiheadAttention(hid_dim=300, n_heads=6, dropout=0.1)\n",
    "#output = attention(query, key, value)\n",
    "## output: torch.Size([64, 12, 300])\n",
    "#print(output.shape)\n",
    "#————————————————\n",
    "#版权声明：本文为CSDN博主「小郭小郭学富五车」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "#原文链接：https://blog.csdn.net/qq_42750193/article/details/122715902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9f0df0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: MLPWithMultiHeadCrossAttention(\n",
      "  (multi_head_attention): MultiHeadAttention(\n",
      "    (w_q): Linear(in_features=1280, out_features=64, bias=True)\n",
      "    (w_k): Linear(in_features=300, out_features=64, bias=True)\n",
      "    (w_v): Linear(in_features=1280, out_features=64, bias=True)\n",
      "    (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (do): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLPWithMultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, num_heads, head_dim, hidden_dim, num_classes):\n",
    "        super(MLPWithMultiHeadCrossAttention, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(input_dim1, input_dim2, num_heads, head_dim)\n",
    "        self.fc1 = nn.Linear(num_heads * head_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        print('x1:',x1.shape)\n",
    "        print('x2:',x2.shape)\n",
    "        \n",
    "        # Multi-Head Cross Attention\n",
    "        attention_output = self.multi_head_attention(x1, x2,x1)\n",
    "        print('attention_output:',attention_output.shape)\n",
    "        # Flatten and pass through MLP layers\n",
    "        flattened_output = attention_output.view(attention_output.size(0), -1)\n",
    "        print('flattened_output:',flattened_output.shape)\n",
    "        out = self.fc1(flattened_output)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 示例用法\n",
    "input_dim1 = 1280\n",
    "input_dim2 = 300\n",
    "num_heads = 4\n",
    "head_dim = 64\n",
    "hidden_dim = 256\n",
    "num_classes = 10  # 替换为你的分类类别数\n",
    "num_epoches=200\n",
    "\n",
    "model = MLPWithMultiHeadCrossAttention(input_dim1, input_dim2, num_heads, head_dim, hidden_dim, num_classes)\n",
    "print('model:',model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# 输入示例\n",
    "#x1 = torch.randn(32, 10, input_dim1)  # 输入1，形状为(batch_size, sequence_length, input_dim1)\n",
    "#x2 = torch.randn(32, 20, input_dim2)  # 输入2，形状为(batch_size, sequence_length, input_dim2)\n",
    "\n",
    "#output = model(x1, x2)\n",
    "#print(\"输出形状:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2a44fcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 生成 2x3 的随机矩阵\n",
    "#a=np.random.randint(100,1028)\n",
    "#a = np.random.rand(100, 1028)\n",
    "#print(a)\n",
    "a=np.random.randint(low=0,high=1000,size=(100,27,1280),dtype='int')\n",
    "a=torch.from_numpy(a)\n",
    "a=torch.tensor(a,dtype=torch.float)\n",
    "\n",
    "#b=np.random.rand(100,300)\n",
    "#print(b)\n",
    "b=np.random.randint(low=0,high=1000,size=(100,35,300),dtype='int')\n",
    "b=torch.from_numpy(b)\n",
    "b=torch.tensor(b,dtype=torch.float)\n",
    "\n",
    "label=np.random.randint(low=6,high=10,size=(100),dtype='int')\n",
    "#print(label,label.shape)\n",
    "label=torch.from_numpy(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50c86d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,a,b,label):\n",
    "        #super.__init__()\n",
    "        self.a=a\n",
    "        self.b=b\n",
    "        self.label=label\n",
    "    def __getitem__(self,idx):\n",
    "        x1=self.a[idx]\n",
    "        x2=self.b[idx]\n",
    "        label=self.label[idx]\n",
    "        return (x1,x2,label)\n",
    "    def __len__(self,):\n",
    "        return len(self.a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f517b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=myDataset(a,b,label)\n",
    "train_loader=DataLoader(train_dataset,batch_size=32,num_workers=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "95dc4074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: torch.Size([32, 27, 1280])\n",
      "x2: torch.Size([32, 35, 300])\n",
      "Q,K,V: torch.Size([32, 27, 64]) torch.Size([32, 35, 64]) torch.Size([32, 27, 64])\n",
      "Q: torch.Size([32, 4, 27, 16])\n",
      "Q,K,V: torch.Size([32, 4, 27, 16]) torch.Size([32, 4, 35, 16]) torch.Size([32, 4, 27, 16])\n",
      "attention: torch.Size([32, 4, 27, 35])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-147eab5a8ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# dataloader是你的数据加载器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-1a4d017a3114>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Multi-Head Cross Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_head_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attention_output:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Flatten and pass through MLP layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-1cd81c8d7453>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# x: [64,6,12,50]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m#x = torch.matmul(attention, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m# 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# x: [64,6,12,50] 转置-> [64,12,6,50]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    for x1_batch, x2_batch, labels in train_loader:  # dataloader是你的数据加载器\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x1_batch, x2_batch)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe97cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
