{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dbf669",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.18 GiB total capacity; 77.73 GiB already allocated; 12.31 MiB free; 77.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-46f3bcf3feff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m##protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mprotein_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;31m#freezing parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.18 GiB total capacity; 77.73 GiB already allocated; 12.31 MiB free; 77.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from loader1 import MoleculeDataset,MoleculeDatasetBig, SeqDataset,SeqMolDataset,SmileDataset#########################\n",
    "import torch\n",
    "import torch\n",
    "#import args\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "#from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "#from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from splitters import scaffold_split,scaffold_split_1\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "#import esm2_t33_650M_UR50D\n",
    "import esm\n",
    "\n",
    "#from SeqMolModel import InteractionModel,InteractionModel_1,SequenceModel,InteractionModel_4\n",
    "#from SeqMolSmile import InteractionModel_4\n",
    "#from SeqMolModel import InteractionModel_4\n",
    "from SeqMolSmile_model2 import InteractionModel_4\n",
    "print(torch.cuda.is_available())\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda._initialized = True\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')#0000\n",
    "parser.add_argument('--batch_size', type=int, default=64,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=400,\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "parser.add_argument('--decay', type=float, default=0,\n",
    "                        help='weight decay (default: 0)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "parser.add_argument('--dataset', type=str, default = 'davis', help='root directory of dataset. For now, only classification.')\n",
    "#parser.add_argument('--input_model_file', type=str, default = 'None', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--input_model_file', type=str, default = 'Mole-BERT', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--filename', type=str, default = '', help='output filename')\n",
    "parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "parser.add_argument('--runseed', type=int, default=0, help = \"Seed for minibatch selection, random initialization.\")\n",
    "parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "parser.add_argument('--eval_train', type=int, default = 1, help='evaluating training or not')\n",
    "parser.add_argument('--num_workers', type=int, default = 8, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default = 12, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_layer', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--d_dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--n_embd', type=int, default = 768, help='number of workers for dataset loading')\n",
    "parser.add_argument('--dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--lr_start', type=float, default =  3e-5, help='number of workers for dataset loading')\n",
    "parser.add_argument('--max_epochs', type=int, default = 500, help='number of workers for dataset loading')\n",
    "parser.add_argument('--num_feats', type=int, default = 32, help='number of workers for dataset loading')\n",
    "parser.add_argument('--checkpoint_every', type=int, default = 100, help='number of workers for dataset loading')\n",
    "parser.add_argument('--seed_path', type=str, default =  'data/checkpoints/N-Step-Checkpoint_3_30000.ckpt', help='number of workers for dataset loading')\n",
    "parser.add_argument('--dims', type=list, default = [ 768, 768, 768, 1], help='number of workers for dataset loading')\n",
    "\n",
    "args = parser.parse_args(args=[])###############33\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import torchvision.models as models\n",
    "#load pretained model of Mole-Bert\n",
    "\n",
    "model_path='esm2_t33_650M_UR50D.pt'\n",
    "##esm.load_state_dict(torch.load('esm2_t33_650M_UR50D.pt'))\n",
    "##protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D(model_path)\n",
    "##model_path = r\"esm2_t33_650M_UR50D.pt\"\n",
    "#model,alphabet=esm.pretrained(model_location=model_path)\n",
    "\n",
    "##model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")\n",
    "\n",
    "protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.runseed)\n",
    "\n",
    "num_tasks=1\n",
    "# Load ESM-2 model\n",
    "\n",
    "##protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "protein_model.to(device)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "#print(protein_alphabet)\n",
    "#alphabet = esm.Alphabet.from_architecture(model_data[\"args\"].arch)\n",
    "#batch_converter = alphabet.get_batch_converter()\n",
    "protein_model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "#self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "molecular_model = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "###################################\n",
    "if not args.input_model_file == \"None\":###############\n",
    "    print('Not from scratch')\n",
    "    molecular_model.from_pretrained('model_gin/{}.pth'.format(args.input_model_file))\n",
    "    print('rese:model_gin')\n",
    "molecular_model.to(device)\n",
    "\n",
    "\n",
    "print('molecular load############')\n",
    "for i,p in enumerate(molecular_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "print('parameter frozen###########')\n",
    "'''\n",
    "model_param_group = []\n",
    "model_param_group.append({\"params\": molecular_model.gnn.parameters()})\n",
    "if args.graph_pooling == \"attention\":\n",
    "    model_param_group.append({\"params\": molecular_model.pool.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "'''\n",
    "'''\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「Dreamcatcher风」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/Wind_2028/article/details/120541017   \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "model_param_group.append({\"params\": molecular_model.graph_pred_linear.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "optimizer = optim.Adam(model_param_group, lr=0.01, weight_decay=args.decay)#############\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
    "#from finetune.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "#from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
    "from functools import partial\n",
    "from apex import optimizers\n",
    "import subprocess\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "#from utils import normalize_smiles\n",
    "import sys\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "from tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "#from utils import normalize_smiles\n",
    "# create a function (this my favorite choice)\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(LightningModule, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        #self.hparams = config\n",
    "        #self.mode = config.mode\n",
    "        self.save_hyperparameters(config)\n",
    "        self.tokenizer=tokenizer\n",
    "        '''\n",
    "        self.min_loss = {\n",
    "            self.hparams.measure_name + \"min_valid_loss\": torch.finfo(torch.float32).max,\n",
    "            self.hparams.measure_name + \"min_epoch\": 0,\n",
    "        }\n",
    "        '''\n",
    "        # Word embeddings layer\n",
    "        n_vocab, d_emb = len(tokenizer.vocab), config.n_embd\n",
    "        # input embedding stem\n",
    "        \n",
    "        builder = rotate_builder.from_kwargs(\n",
    "            n_layers=config.n_layer,\n",
    "            n_heads=config.n_head,\n",
    "            query_dimensions=config.n_embd//config.n_head,\n",
    "            value_dimensions=config.n_embd//config.n_head,\n",
    "            feed_forward_dimensions=config.n_embd,\n",
    "            attention_type='linear',\n",
    "            feature_map=partial(GeneralizedRandomFeatures, n_dims=config.num_feats),\n",
    "            activation='gelu',\n",
    "            )\n",
    "        self.pos_emb = None\n",
    "        self.tok_emb = nn.Embedding(n_vocab, config.n_embd)\n",
    "        #print('self.tok_emb:',self.tok_emb)\n",
    "        self.drop = nn.Dropout(config.d_dropout)\n",
    "        \n",
    "        ## transformer\n",
    "        self.blocks = builder.get()\n",
    "        #self.lang_model = self.lm_layer(config.n_embd, n_vocab)\n",
    "        #self.train_config = config\n",
    "        #if we are starting from scratch set seeds\n",
    "        #########################################\n",
    "        # protein_emb_dim, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "        #########################################\n",
    "        '''\n",
    "        self.fcs = []  \n",
    "        self.loss = torch.nn.L1Loss()\n",
    "        self.net = self.Net(\n",
    "            config.n_embd, dims=config.dims, dropout=config.dropout,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        dims = [150, 50, 50, 2]\n",
    "\n",
    "\n",
    "        def __init__(self, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.desc_skip_connection = True \n",
    "            self.fcs = []  # nn.ModuleList()\n",
    "            #print('dropout is {}'.format(dropout))\n",
    "\n",
    "            self.fc1 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.relu1 = nn.GELU()\n",
    "            self.fc2 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.relu2 = nn.GELU()\n",
    "            self.final = nn.Linear(smiles_embed_dim, 1)\n",
    "\n",
    "        def forward(self, smiles_emb):\n",
    "            x_out = self.fc1(smiles_emb)\n",
    "            x_out = self.dropout1(x_out)\n",
    "            x_out = self.relu1(x_out)\n",
    "\n",
    "            if self.desc_skip_connection is True:\n",
    "                x_out = x_out + smiles_emb\n",
    "\n",
    "            z = self.fc2(x_out)\n",
    "            z = self.dropout2(z)\n",
    "            z = self.relu2(z)\n",
    "            if self.desc_skip_connection is True:\n",
    "                z = self.final(z + x_out)\n",
    "            else:\n",
    "                z = self.final(z)\n",
    "\n",
    "            return z\n",
    "\n",
    "    class lm_layer(nn.Module):\n",
    "        def __init__(self, n_embd, n_vocab):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Linear(n_embd, n_embd)\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.head = nn.Linear(n_embd, n_vocab, bias=False)\n",
    "        def forward(self, tensor):\n",
    "            tensor = self.embed(tensor)\n",
    "            tensor = F.gelu(tensor)\n",
    "            tensor = self.ln_f(tensor)\n",
    "            tensor = self.head(tensor)\n",
    "            return tensor\n",
    "\n",
    "    def get_loss(self, smiles_emb, measures):\n",
    "\n",
    "        z_pred = self.net.forward(smiles_emb).squeeze()\n",
    "        measures = measures.float()\n",
    "\n",
    "        return self.loss(z_pred, measures), z_pred, measures\n",
    "    \n",
    "    \n",
    "margs = args\n",
    "tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "seed.seed_everything(margs.seed)\n",
    "\n",
    "print('smile_model_start$$$$$')\n",
    "if margs.seed_path == '':\n",
    "    #print(\"# training from scratch\")\n",
    "    smile_model = LightningModule(margs, tokenizer)\n",
    "else:\n",
    "    #print(\"# loaded pre-trained model from {args.seed_path}\")\n",
    "    smile_model = LightningModule(margs, tokenizer).load_from_checkpoint(margs.seed_path, strict=False, config=margs, tokenizer=tokenizer, vocab=len(tokenizer.vocab))#########################33\n",
    "\n",
    "#print('model:',smile_model)\n",
    "#freezing parameters\n",
    "\n",
    "print('smile_model_load************')\n",
    "for i,p in enumerate(smile_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "    \n",
    "print('smile_model_parameters***********')\n",
    "#num_tasks=1\n",
    "model= InteractionModel_4(protein_model=protein_model,molecular_model=molecular_model,smile_model=smile_model,protein_embd_dim=1280,num_tasks=1,device=device,mol_embd_dim=300,smile_embd_dim=768) \n",
    "model.to(device)\n",
    "#print(model)#nice#num_tasks=1\n",
    "\n",
    "\n",
    "###################when doing geometric.data process, if there is some changes, please delete the directory process and let it generate again\n",
    "'''\n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset, dataset=args.dataset)###########################转换成了分子图的格式\n",
    "print('args.dataset:',args.dataset)\n",
    "print(gnn_dataset)\n",
    "'''\n",
    "\n",
    "def collate(batch):\n",
    "    print('collate_batch:',batch)\n",
    "    print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    #print('tokens[1]_mask:',tokens[1])\n",
    "    #print('colate###########################')\n",
    "    #for i,m in enumerate(tokens):\n",
    "            \n",
    "    #print('collate_tokens_input_ids:',tokens['input_ids'])\n",
    "    #print('colate_tokens_attention_mask:',tokens['attention_mask'])\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "    \n",
    "#smiles_dataset=SmileDataset('dataset/davis/processed/smiles.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#####test with chartGPT  DataSet extends two parents' classes\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MultiDatasetMixin:\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.dataset3=dataset3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset1), len(self.dataset2),len(self.dataset3))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dataset1[idx], self.dataset2[idx],self.dataset3[idx]\n",
    "\n",
    "class CustomMultiDataset(MultiDatasetMixin, Dataset):###############3extends two classes\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        MultiDatasetMixin.__init__(self, dataset1, dataset2,dataset3)\n",
    "\n",
    "\n",
    "#chartGPT太厉害了，杀了我吧\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
    "\n",
    "class MultiDataLoader:\n",
    "    def __init__(self, dataloader1, dataloader2,dataloader3):\n",
    "        self.dataloader1 = dataloader1\n",
    "        self.dataloader2 = dataloader2\n",
    "        self.dataloader3=dataloader3\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data1, data2,data3 in zip(self.dataloader1, self.dataloader2, self.dataloader3):\n",
    "            yield data1, data2, data3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader1), len(self.dataloader2), len(self.dataloader3))\n",
    "\n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.dataloader1.shuffle = shuffle\n",
    "        self.dataloader2.shuffle = shuffle\n",
    "        self.dataloader3.shuffle=shuffle\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json,pickle\n",
    "from collections import OrderedDict\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "import networkx as nx\n",
    "from utilssssss import *\n",
    "\n",
    "def atom_features(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    [atom.GetIsAromatic()])\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    \n",
    "    c_size = mol.GetNumAtoms()\n",
    "    \n",
    "    features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        feature = atom_features(atom)\n",
    "        features.append( feature / sum(feature) )\n",
    "\n",
    "    edges = []\n",
    "    for bond in mol.GetBonds():\n",
    "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "    g = nx.Graph(edges).to_directed()\n",
    "    edge_index = []\n",
    "    for e1, e2 in g.edges:\n",
    "        edge_index.append([e1, e2])\n",
    "        \n",
    "    return c_size, features, edge_index\n",
    "\n",
    "def seq_cat(prot):\n",
    "    x = np.zeros(max_seq_len)\n",
    "    for i, ch in enumerate(prot[:max_seq_len]): \n",
    "        x[i] = seq_dict[ch]\n",
    "    return x  \n",
    "\n",
    "\n",
    "# from DeepDTA data\n",
    "all_prots = []\n",
    "datasets = ['kiba','davis']\n",
    "for dataset in datasets:\n",
    "    print('convert data from DeepDTA for ', dataset)\n",
    "    fpath = 'dataset/' + dataset + '/'\n",
    "    train_fold = json.load(open(fpath + \"folds/train_fold_setting1.txt\"))\n",
    "    train_fold = [ee for e in train_fold for ee in e ]\n",
    "    valid_fold = json.load(open(fpath + \"folds/test_fold_setting1.txt\"))\n",
    "    ligands = json.load(open(fpath + \"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n",
    "    proteins = json.load(open(fpath + \"proteins.txt\"), object_pairs_hook=OrderedDict)\n",
    "    affinity = pickle.load(open(fpath + \"Y\",\"rb\"), encoding='latin1')\n",
    "    drugs = []\n",
    "    prots = []\n",
    "    for d in ligands.keys():\n",
    "        lg = Chem.MolToSmiles(Chem.MolFromSmiles(ligands[d]),isomericSmiles=True)\n",
    "        drugs.append(lg)\n",
    "    for t in proteins.keys():\n",
    "        prots.append(proteins[t])\n",
    "    if dataset == 'davis':\n",
    "        affinity = [-np.log10(y/1e9) for y in affinity]\n",
    "    affinity = np.asarray(affinity)\n",
    "    opts = ['train','test']\n",
    "    for opt in opts:\n",
    "        rows, cols = np.where(np.isnan(affinity)==False)  \n",
    "        if opt=='train':\n",
    "            rows,cols = rows[train_fold], cols[train_fold]\n",
    "        elif opt=='test':\n",
    "            rows,cols = rows[valid_fold], cols[valid_fold]\n",
    "        with open('dataset/' + dataset + '_' + opt + '.csv', 'w') as f:\n",
    "            f.write('compound_iso_smiles,target_sequence,affinity\\n')\n",
    "            for pair_ind in range(len(rows)):\n",
    "                ls = []\n",
    "                ls += [ drugs[rows[pair_ind]]  ]\n",
    "                ls += [ prots[cols[pair_ind]]  ]\n",
    "                ls += [ affinity[rows[pair_ind],cols[pair_ind]]  ]\n",
    "                f.write(','.join(map(str,ls)) + '\\n')       \n",
    "    print('\\ndataset:', dataset)\n",
    "    print('train_fold:', len(train_fold))\n",
    "    print('test_fold:', len(valid_fold))\n",
    "    print('len(set(drugs)),len(set(prots)):', len(set(drugs)),len(set(prots)))\n",
    "    all_prots += list(set(prots))\n",
    "    \n",
    "    \n",
    "seq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\n",
    "seq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\n",
    "seq_dict_len = len(seq_dict)\n",
    "max_seq_len = 1000\n",
    "\n",
    "compound_iso_smiles = []\n",
    "for dt_name in ['kiba','davis']:\n",
    "    opts = ['train','test']\n",
    "    for opt in opts:\n",
    "        df = pd.read_csv('data/' + dt_name + '_' + opt + '.csv')\n",
    "        compound_iso_smiles += list( df['compound_iso_smiles'] )\n",
    "compound_iso_smiles = set(compound_iso_smiles)###########去重\n",
    "smile_graph = {}\n",
    "for smile in compound_iso_smiles:\n",
    "    g = smile_to_graph(smile)\n",
    "    smile_graph[smile] = g\n",
    "\n",
    "datasets = ['davis','kiba']\n",
    "# convert to PyTorch data format\n",
    "g_dataset=args.dataset\n",
    "\"\"\"\n",
    "if g_dataset in datasets:\n",
    "    processed_data_file_train = 'dataset/processed/' + g_dataset + '_train.pt'\n",
    "    processed_data_file_test = 'dataset/processed/' + g_dataset + '_test.pt'\n",
    "    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n",
    "        df = pd.read_csv('dataset/' + g_dataset + '_train.csv')\n",
    "        train_drugs, train_prots,  train_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n",
    "        XT = [seq_cat(t) for t in train_prots]\n",
    "        train_drugs, train_prots,  train_Y = np.asarray(train_drugs), np.asarray(XT), np.asarray(train_Y)\n",
    "        df = pd.read_csv('dataset/' + g_dataset + '_test.csv')\n",
    "        test_drugs, test_prots,  test_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n",
    "        XT = [seq_cat(t) for t in test_prots]\n",
    "        test_drugs, test_prots,  test_Y = np.asarray(test_drugs), np.asarray(XT), np.asarray(test_Y)\n",
    "\n",
    "        # make data PyTorch Geometric ready\n",
    "        print('preparing ', dataset + '_train.pt in pytorch format!')\n",
    "        gnn_train_dataset = TestbedDataset(root='dataset', dataset=g_dataset+'_train', xd=train_drugs, xt=train_prots, y=train_Y,smile_graph=smile_graph)\n",
    "        seq_train_dataset=SeqDataset('dataset/processed/'+g_dataset+'_train_sequence.csv')\n",
    "        smiles_train_dataset=SmileDataset('dataset/processed/'+g_dataset+'_train_smiles.csv')\n",
    "        print('preparing ', dataset + '_test.pt in pytorch format!')\n",
    "        gnn_test_dataset = TestbedDataset(root='dataset', dataset=g_dataset+'_test', xd=test_drugs, xt=test_prots, y=test_Y,smile_graph=smile_graph)\n",
    "        seq_test_dataset=SeqDataset('dataset/processed/'+g_dataset+'_test_sequence.csv')\n",
    "        smiles_test_dataset=SmileDataset('dataset/processed/'+g_dataset+'_test_smiles.csv')\n",
    "        print(processed_data_file_train, ' and ', processed_data_file_test, ' have been created')  \n",
    "        \n",
    "        seq_train_dataloader1 = DataLoader(seq_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "        gnn_train_dataloader2 = GeometricDataLoader(gnn_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "        smile_train_dataloader3=DataLoader(smile_train_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "        seq_mol_smile_train_multi_loader = MultiDataLoader(seq_train_dataloader1, mol_train_dataloader2,smile_train_dataloader3)\n",
    "        # Set the shuffle parameter simultaneously for both dataloaders\n",
    "        seq_mol_smile_train_multi_loader.set_shuffle(True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "        mol_test_dataloader2 = GeometricDataLoader(gnn_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "        smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "        seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_valid_dataloader2,smile_test_dataloader3)\n",
    "        # Set the shuffle parameter simultaneously for both dataloaders\n",
    "        seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(processed_data_file_train, ' and ', processed_data_file_test, ' are already created')        \n",
    "\n",
    "        \n",
    "\"\"\"        \n",
    "def collate_fn(batch):\n",
    "    #print('collate_batch:',batch)\n",
    "    #print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    #print('tokens[1]_mask:',tokens[1])\n",
    "    #print('colate###########################')\n",
    "    #for i,m in enumerate(tokens):\n",
    "            \n",
    "    #print('collate_tokens_input_ids:',tokens['input_ids'])\n",
    "    #print('colate_tokens_attention_mask:',tokens['attention_mask'])\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "processed_data_file_train = 'dataset/processed/' + g_dataset + '_train.pt'\n",
    "processed_data_file_test = 'dataset/processed/' + g_dataset + '_test.pt'\n",
    "    \n",
    "df = pd.read_csv('dataset/' + g_dataset + '_train.csv')\n",
    "train_drugs, train_prots,  train_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n",
    "#XT = [seq_cat(t) for t in train_prots]#####################\n",
    "train_drugs, train_prots,  train_Y = np.asarray(train_drugs), np.asarray(train_prots), np.asarray(train_Y)\n",
    "df = pd.read_csv('dataset/' + g_dataset + '_test.csv')\n",
    "test_drugs, test_prots,  test_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n",
    "#XT = [seq_cat(t) for t in test_prots]\n",
    "test_drugs, test_prots,  test_Y = np.asarray(test_drugs), np.asarray(test_prots), np.asarray(test_Y)\n",
    "\n",
    "# make data PyTorch Geometric ready\n",
    "print('preparing ', dataset + '_train.pt in pytorch format!')\n",
    "gnn_train_dataset = TestbedDataset(root='dataset', dataset=g_dataset+'_train', xd=train_drugs, xt=train_prots, y=train_Y,smile_graph=smile_graph)\n",
    "seq_train_dataset=SeqDataset('dataset/processed/'+g_dataset+'_train_sequence.csv')\n",
    "smiles_train_dataset=SmileDataset('dataset/processed/'+g_dataset+'_train_smiles.csv')\n",
    "print('preparing ', dataset + '_test.pt in pytorch format!')\n",
    "gnn_test_dataset = TestbedDataset(root='dataset', dataset=g_dataset+'_test', xd=test_drugs, xt=test_prots, y=test_Y,smile_graph=smile_graph)\n",
    "seq_test_dataset=SeqDataset('dataset/processed/'+g_dataset+'_test_sequence.csv')\n",
    "smiles_test_dataset=SmileDataset('dataset/processed/'+g_dataset+'_test_smiles.csv')\n",
    "print(processed_data_file_train, ' and ', processed_data_file_test, ' have been created')  \n",
    "        \n",
    "seq_train_dataloader1 = torch.utils.data.DataLoader(seq_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_train_dataloader2 = GeometricDataLoader(gnn_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_train_dataloader3=torch.utils.data.DataLoader(smiles_train_dataset,batch_size=args.batch_size,collate_fn=collate_fn, shuffle=False,num_workers=args.num_workers)\n",
    "seq_mol_smile_train_multi_loader = MultiDataLoader(seq_train_dataloader1, mol_train_dataloader2,smile_train_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_train_multi_loader.set_shuffle(True)\n",
    "        \n",
    "        \n",
    "        \n",
    "seq_test_dataloader1 = torch.utils.data.DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(gnn_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=torch.utils.data.DataLoader(smiles_test_dataset,batch_size=args.batch_size,collate_fn=collate_fn, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_test_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "'''       \n",
    "for i,smile in enumerate(smile_train_dataloader3):\n",
    "    print(smile)\n",
    "    break\n",
    "        \n",
    "'''        \n",
    "\"\"\"   \n",
    "         \n",
    "'''\n",
    "for i,gnn_data in enumerate(gnn_dataset):\n",
    "    print(i,end=' ')\n",
    "'''\n",
    "\n",
    "seq_dataset=SeqDataset('dataset/davis/processed/sequence.csv')\n",
    "'''\n",
    "for i,seq_data in enumerate(seq_dataset):\n",
    "    print(i,seq_data)\n",
    "'''\n",
    "smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#seq_dataset[2]\n",
    "\n",
    "def collate(batch):\n",
    "    #print('collate_batch:',batch)\n",
    "    #print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    #print('tokens[1]_mask:',tokens[1])\n",
    "    #print('colate###########################')\n",
    "    #for i,m in enumerate(tokens):\n",
    "            \n",
    "    #print('collate_tokens_input_ids:',tokens['input_ids'])\n",
    "    #print('colate_tokens_attention_mask:',tokens['attention_mask'])\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "    \n",
    "smiles_dataset=SmileDataset('dataset/davis/processed/smiles.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#####test with chartGPT  DataSet extends two parents' classes\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MultiDatasetMixin:\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.dataset3=dataset3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset1), len(self.dataset2),len(self.dataset3))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dataset1[idx], self.dataset2[idx],self.dataset3[idx]\n",
    "\n",
    "class CustomMultiDataset(MultiDatasetMixin, Dataset):###############3extends two classes\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        MultiDatasetMixin.__init__(self, dataset1, dataset2,dataset3)\n",
    "\n",
    "\n",
    "#chartGPT太厉害了，杀了我吧\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
    "\n",
    "class MultiDataLoader:\n",
    "    def __init__(self, dataloader1, dataloader2,dataloader3):\n",
    "        self.dataloader1 = dataloader1\n",
    "        self.dataloader2 = dataloader2\n",
    "        self.dataloader3=dataloader3\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data1, data2,data3 in zip(self.dataloader1, self.dataloader2, self.dataloader3):\n",
    "            yield data1, data2, data3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader1), len(self.dataloader2), len(self.dataloader3))\n",
    "\n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.dataloader1.shuffle = shuffle\n",
    "        self.dataloader2.shuffle = shuffle\n",
    "        self.dataloader3.shuffle=shuffle\n",
    "\n",
    "        \n",
    "###########split train dataset validate dataset test dataset\n",
    "seq_gnn_smile_dataset=MultiDatasetMixin(seq_dataset,gnn_dataset,smiles_dataset)\n",
    "'''\n",
    "for i , seq_gnn_smile in enumerate(seq_gnn_smile_dataset):\n",
    "    print(i)\n",
    "'''\n",
    "#print('seq_dataset:',seq_dataset)\n",
    "\n",
    "#seq_dataloader=DataLoader(seq_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.num_workers)\n",
    "'''\n",
    "for i ,seq in enumerate(seq_dataloader):\n",
    "    print(seq)\n",
    "'''\n",
    "if args.split == \"scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        \n",
    "        #print('smiles_list:',smiles_list)\n",
    "        mol_train_dataset, mol_valid_dataset, mol_test_dataset ,seq_train_dataset,seq_valid_dataset,seq_test_dataset,smile_train_dataset,smile_valid_dataset,smile_test_dataset= scaffold_split_1(seq_gnn_smile_dataset, smiles_list, null_value=0, frac_train=0.6,frac_valid=0.2, frac_test=0.2)##########dataset\n",
    "        print(\"scaffold\")\n",
    "elif args.split == \"random\":\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random\")\n",
    "elif args.split == \"random_scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random scaffold\")\n",
    "else:\n",
    "        raise ValueError(\"Invalid split option.\")\n",
    "\n",
    "#print('++++++++++', mol_train_dataset[0])\n",
    "'''\n",
    "for i, mol in enumerate(mol_train_dataset):\n",
    "    print('mol:',mol)\n",
    "'''\n",
    "#seq_mol_train_dataset=MultiDatasetMixini(seq_train_dataset,mol_train_dataset)\n",
    "#seq_mol_valid_dataset=SeqMolDataset(seq_valid_dataset,mol_valid_dataset)\n",
    "#seq_mol_test_dataset=SeqMolDataset(seq_train_dataset,mol_test_dataset)\n",
    "seq_train_dataloader1 = DataLoader(seq_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_train_dataloader2 = GeometricDataLoader(mol_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_train_dataloader3=DataLoader(smile_train_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "seq_mol_smile_train_multi_loader = MultiDataLoader(seq_train_dataloader1, mol_train_dataloader2,smile_train_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_train_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "print('seq_train_dataset#########:',seq_train_dataset)\n",
    "\n",
    "for i,seq in enumerate(seq_train_dataloader1):\n",
    "    print(seq)\n",
    "'''\n",
    "seq_valid_dataloader1 = DataLoader(seq_valid_dataset, batch_size=args.batch_size,shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_valid_dataloader2 = GeometricDataLoader(mol_valid_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_valid_dataloader3=DataLoader(smile_valid_dataset, batch_size=args.batch_size,collate_fn=collate,shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "'''\n",
    "for i,m in enumerate(smile_train_dataloader3):\n",
    "    #print('m@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:',m)\n",
    "    break\n",
    "'''\n",
    "seq_mol_smile_valid_multi_loader = MultiDataLoader(seq_valid_dataloader1, mol_valid_dataloader2,smile_valid_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_valid_multi_loader.set_shuffle(True)\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_valid_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "for i ,(seq,mol,smile) in enumerate(seq_mol_smile_train_multi_loader):\n",
    "    print(i)\n",
    "    #print(mol)\n",
    "    #print(smile)\n",
    "'''\n",
    "'''\n",
    "print('mol_dataloader:')\n",
    "for mol in mol_train_dataloader2:\n",
    "    print(mol)\n",
    "for seq in seq_train_dataloader1:\n",
    "    print(seq)\n",
    " '''   \n",
    "\"\"\"\n",
    "def train(args, epoch, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    save_pt='results/davis/model1/'\n",
    "    #epoch_iter = tqdm(loader, desc=\"Iteration\")\n",
    "    for step, (A,B,C) in enumerate(loader):\n",
    "        print('eposh,step:',epoch,step)\n",
    "        seq_data_list=[]\n",
    "        seq=A\n",
    "        lenth=len(seq)\n",
    "        \n",
    "        for m , s in enumerate(seq):\n",
    "            seq_data_list.append((str(m),s))\n",
    "        B=B.to(device)\n",
    "        #print(C)\n",
    "        D,E=C\n",
    "        D=D.to(device)\n",
    "        E=E.to(device)\n",
    "        C=(D,E)\n",
    "        #print('D!!!!!!!!!!!!!!!!:',D)\n",
    "        #print('E#####################:',E)\n",
    "        #pred=model(seq_data_list,B,C)#model is error\n",
    "        pred=model(seq_data_list,B,C)#model is error\n",
    "        #pred=pred.to(torch.float32)\n",
    "        y_true = B.y.view(pred.shape).to(torch.float32)\n",
    "        #loss = criterion(pred, y_true)\n",
    "        \n",
    "        loss1=criterion(pred,y_true)\n",
    "        #loss2=criterion(u12,u34)\n",
    "        #print('loss1{0},loss2{1}:',loss1,loss2)\n",
    "        #loss=loss1+loss2\n",
    "        loss=loss1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #epoch_iter.set_description(f\"Epoch: {epoch} tloss: {loss:.4f}\")\n",
    "        nn=(epoch+1)//10\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'training Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "            torch.save(model, save_pt+f'full_model_{nn}.pt')\n",
    "    #return loss.item\n",
    "def eval(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            D,E=C\n",
    "            D=D.to(device)\n",
    "            E=E.to(device)##################\n",
    "            C=(D,E)\n",
    "        \n",
    "        \n",
    "            #pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            pred=model(seq_data_list,B,C)#model is error\n",
    "            y_true=B.y.view(pred.shape).to(torch.float32)\n",
    "            val_loss = mse_criterion(pred, y_true)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "\n",
    "    #y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "    #y_scores = torch.cat(y_scores, dim = 0).detach().cpu().numpy()\n",
    "    '''\n",
    "    roc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        #AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:,i] == 1) > 0 and np.sum(y_true[:,i] == -1) > 0:\n",
    "            is_valid = y_true[:,i]**2 > 0\n",
    "            roc_list.append(roc_auc_score((y_true[is_valid,i] + 1)/2, y_scores[is_valid,i]))\n",
    "    if len(roc_list)==0:#########################\n",
    "        return 0\n",
    "    if len(roc_list) < y_true.shape[1]:\n",
    "        print(\"Some target is missing!\")\n",
    "        miss_ratio=(1 - float(len(roc_list))/y_true.shape[1])\n",
    "        print(\"Missing ratio: %f\" %(1 - float(len(roc_list))/y_true.shape[1]))\n",
    "    '''\n",
    "    \n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "results_save_file='results/davis/model1/results_save_model1.txt'\n",
    "\n",
    "\n",
    "import torch, gc\n",
    "#criterion = nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "criterion=nn.SmoothL1Loss()\n",
    "mse_criterion=nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "if not args.filename == \"\":\n",
    "    fname = 'runs/seq_mol_finetune_cls_runseed' + str(args.runseed) + '/' + args.filename\n",
    "    #delete the directory if there exists one\n",
    "    if os.path.exists(fname):\n",
    "        shutil.rmtree(fname)\n",
    "        print(\"removed the existing file.\")\n",
    "    writer = SummaryWriter(fname)\n",
    "\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    print(\"====epoch \" + str(epoch))\n",
    "        \n",
    "    train(args, epoch, model, device, seq_mol_smile_train_multi_loader, optimizer)\n",
    "    \n",
    "\n",
    "    print(\"====Evaluation\")\n",
    "    if args.eval_train:\n",
    "        train_loss = eval(args, model, device, seq_mol_smile_train_multi_loader)\n",
    "    else:\n",
    "        print(\"omit the training accuracy computation\")\n",
    "        train_loss = 0\n",
    "    #val_loss = eval(args, model, device, seq_mol_smile_valid_multi_loader)\n",
    "    test_loss = eval(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "    with open(results_save_file, 'w+') as f:\n",
    "        f.write(str(epoch)+'\\t'+str(train_loss)+'\\t'+str(val_loss)+'\\n')\n",
    "        #f.write(epoch)\n",
    "        #f.write('\\t')\n",
    "        #f.write(train_loss)\n",
    "        #f.write('\\t')\n",
    "        #f.write(val_loss)\n",
    "        #f.write('\\n')\n",
    "        \n",
    "    print(\"train: %f  test: %f\" %(train_loss, test_loss))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e54c298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O=C(NC1CCNCC1)c1[nH]ncc1NC(=O)c1c(Cl)cccc1Cl', 'CSc1cccc(Nc2ncc3cc(-c4c(Cl)cccc4Cl)c(=O)n(C)c3n2)c1', 'COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OCCCN1CCOCC1', 'COC1C(N(C)C(=O)c2ccccc2)CC2OC1(C)n1c3ccccc3c3c4c(c5c6ccccc6n2c5c31)C(=O)NC4', 'CN(C)CC=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OC1CCOC1', 'CN(C)CC=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OC1CCOC1', 'CCN1CCN(Cc2ccc(NC(=O)Nc3ccc(Oc4cc(NC)ncn4)cc3)cc2C(F)(F)F)CC1', 'Oc1cccc(-c2nc(N3CCOCC3)c3oc4ncccc4c3n2)c1', 'CCn1c(-c2nonc2N)nc2c(C#CC(C)(C)O)ncc(OCC3CCCNC3)c21', 'CN(C)CC=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OC1CCOC1', 'CN1CCN(C(=O)c2cc3cc(Cl)ccc3[nH]2)CC1', 'CS(=O)(=O)N1CCN(Cc2cc3nc(-c4cccc5[nH]ncc45)nc(N4CCOCC4)c3s2)CC1', 'Cc1ccc(Nc2nccc(N(C)c3ccc4c(C)n(C)nc4c3)n2)cc1S(N)(=O)=O', 'Cc1nc(Nc2ncc(C(=O)Nc3c(C)cccc3Cl)s2)cc(N2CCN(CCO)CC2)n1', 'CS(=O)(=O)N1CCN(Cc2cc3nc(-c4cccc5[nH]ncc45)nc(N4CCOCC4)c3s2)CC1', 'N#CCC(C1CCCC1)n1cc(-c2ncnc3[nH]ccc23)cn1.O=P(O)(O)O', 'COc1cc(Nc2c(C#N)cnc3cc(OCCCN4CCN(C)CC4)c(OC)cc23)c(Cl)cc1Cl', 'CCn1c(-c2nonc2N)nc2c(C#CC(C)(C)O)ncc(OCC3CCCNC3)c21', 'Cc1cc2c(F)c(Oc3ncnn4cc(OCC(C)O)c(C)c34)ccc2[nH]1', 'C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1', 'COc1cc(Nc2ncc(F)c(Nc3ccc4c(n3)NC(=O)C(C)(C)O4)n2)cc(OC)c1OC.O=S(=O)(O)c1ccccc1', 'CC(Oc1cc(-c2cnn(C3CCNCC3)c2)cnc1N)c1c(Cl)ccc(F)c1Cl', 'Cc1ccc2nc(NCCN)c3ncc(C)n3c2c1.Cl', 'CC1(C)CNc2cc(NC(=O)c3cccnc3NCc3ccncc3)ccc21', 'Clc1ccc(Nc2nnc(Cc3ccncc3)c3ccccc23)cc1', 'O=C(O)c1ccc(Nc2ncc3c(n2)-c2ccc(Cl)cc2C(c2c(F)cccc2F)=NC3)cc1', 'CNC(=O)c1cc(Oc2ccc(NC(=O)Nc3ccc(Cl)c(C(F)(F)F)c3)cc2)ccn1', 'COc1cc2c(Nc3ccc(Br)cc3F)ncnc2cc1OCC1CCN(C)CC1', 'COc1cc2c(N3CCN(C(=O)Nc4ccc(OC(C)C)cc4)CC3)ncnc2cc1OCCCN1CCCCC1', 'COC1C(N(C)C(=O)c2ccccc2)CC2OC1(C)n1c3ccccc3c3c4c(c5c6ccccc6n2c5c31)C(=O)NC4', 'CC(Oc1cc(-c2cnn(C3CCNCC3)c2)cnc1N)c1c(Cl)ccc(F)c1Cl', 'C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1', 'COc1cc2c(Oc3ccc4[nH]c(C)cc4c3F)ncnc2cc1OCCCN1CCCC1', 'Cc1nc(Nc2ncc(C(=O)Nc3c(C)cccc3Cl)s2)cc(N2CCN(CCO)CC2)n1', 'COc1c(Cl)cc2c([nH]c3cnccc32)c1NC(=O)c1cccnc1C', 'Cc1[nH]c(C=C2C(=O)Nc3ccc(F)cc32)c(C)c1C(=O)NCC(O)CN1CCOCC1', 'Cc1ccc(NC(=O)c2ccc(CN3CCN(C)CC3)cc2)cc1Nc1nc(-c2cccnc2)cs1', 'O=c1ncn2nc(Sc3ccc(F)cc3F)ccc2c1-c1c(Cl)cccc1Cl', 'COc1cc(Nc2ncc(F)c(Nc3ccc4c(n3)NC(=O)C(C)(C)O4)n2)cc(OC)c1OC.O=S(=O)(O)c1ccccc1', 'Cn1cnc2c(F)c(Nc3ccc(Br)cc3Cl)c(C(=O)NOCCO)cc21', 'Nc1nc(N)c2nc(-c3cccc(O)c3)c(-c3cccc(O)c3)nc2n1', 'Cc1nc(Nc2ncc(C(=O)Nc3c(C)cccc3Cl)s2)cc(N2CCN(CCO)CC2)n1', 'Cn1c(Nc2ccc(C(F)(F)F)cc2)nc2cc(Oc3ccnc(-c4ncc(C(F)(F)F)[nH]4)c3)ccc21', 'O=C(NOCC1CC1)c1ccc(F)c(F)c1Nc1ccc(I)cc1Cl', 'O=C(O)c1ccc(Nc2ncc3c(n2)-c2ccc(Cl)cc2C(c2c(F)cccc2F)=NC3)cc1', 'CN(C)CC=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OC1CCOC1', 'COC(=O)c1ccc2c(c1)NC(=O)C2=C(Nc1ccc(N(C)C(=O)CN2CCN(C)CC2)cc1)c1ccccc1', 'COc1cc2c(N3CCN(C(=O)Nc4ccc(OC(C)C)cc4)CC3)ncnc2cc1OCCCN1CCCCC1', 'CNC(=O)c1cc(Oc2ccc(NC(=O)Nc3ccc(Cl)c(C(F)(F)F)c3)cc2)ccn1', 'CC(C)N1NC(=C2C=c3cc(O)ccc3=N2)c2c(N)ncnc21', 'C=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1OCCCN1CCOCC1', 'COc1cc(Nc2ncc(F)c(Nc3ccc4c(n3)NC(=O)C(C)(C)O4)n2)cc(OC)c1OC.O=S(=O)(O)c1ccccc1', 'CC12OC(CC1(O)CO)n1c3ccccc3c3c4c(c5c6ccccc6n2c5c31)CNC4=O', 'CC(C)(C)c1cnc(CSc2cnc(NC(=O)C3CCNCC3)s2)o1', 'CS(=O)c1ccc(-c2nc(-c3ccc(F)cc3)c(-c3ccncc3)[nH]2)cc1', 'Cc1ccc2nc(NCCN)c3ncc(C)n3c2c1.Cl', 'O=c1ncn2nc(Sc3ccc(F)cc3F)ccc2c1-c1c(Cl)cccc1Cl', 'Cc1[nH]nc2ccc(-c3cncc(OCC(N)Cc4ccccc4)c3)cc12', 'CCC1C(=O)N(C)c2cnc(Nc3ccc(C(=O)NC4CCN(C)CC4)cc3OC)nc2N1C1CCCC1', 'O=c1ncn2nc(Sc3ccc(F)cc3F)ccc2c1-c1c(Cl)cccc1Cl', 'CC12OC(CC1(O)CO)n1c3ccccc3c3c4c(c5c6ccccc6n2c5c31)CNC4=O', 'COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OCCCN1CCOCC1', 'Cn1cnc2c(F)c(Nc3ccc(Br)cc3Cl)c(C(=O)NOCCO)cc21', 'CCOc1cc2ncc(C#N)c(Nc3ccc(OCc4ccccn4)c(Cl)c3)c2cc1NC(=O)C=CCN(C)C']\n"
     ]
    }
   ],
   "source": [
    "smile_train_dataloader3=DataLoader(smiles_train_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "for i, smile in enumerate(smile_train_dataloader3):\n",
    "    print(smile)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdecf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm\n",
    "model_path=f'esm2_t33_650M_UR50D.pt'\n",
    "protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D(model_path)\n",
    "protein_model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49ebf264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_esm_main\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36mload_hub_workaround\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_zip_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a9e4b08b6adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facebookresearch/esm:main\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"esm2_t33_650M_UR50D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m                                            verbose=verbose, skip_validation=skip_validation)\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36mesm2_t33_650M_UR50D\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \"\"\"\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_and_alphabet_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"esm2_t33_650M_UR50D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36mload_model_and_alphabet_hub\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_and_alphabet_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_download_model_and_regression_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_and_alphabet_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36m_download_model_and_regression_data\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_download_model_and_regression_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://dl.fbaipublicfiles.com/fair-esm/models/{model_name}.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_hub_workaround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_has_regression_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mregression_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_regression_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36mload_hub_workaround\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Pytorch version issue - see https://github.com/pytorch/pytorch/issues/43106\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         data = torch.load(\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;34mf\"{torch.hub.get_dir()}/checkpoints/{fn}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    775\u001b[0m             \u001b[0;31m# reset back to the original position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                     warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t33_650M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e741fced",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36mload_hub_workaround\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_zip_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-583c0d28f16a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load ESM-2 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesm2_t33_650M_UR50D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36mesm2_t33_650M_UR50D\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \"\"\"\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_and_alphabet_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"esm2_t33_650M_UR50D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36mload_model_and_alphabet_hub\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_and_alphabet_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_download_model_and_regression_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_and_alphabet_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36m_download_model_and_regression_data\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_download_model_and_regression_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://dl.fbaipublicfiles.com/fair-esm/models/{model_name}.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_hub_workaround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_has_regression_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mregression_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_regression_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/esm-main1/esm/pretrained.py\u001b[0m in \u001b[0;36mload_hub_workaround\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Pytorch version issue - see https://github.com/pytorch/pytorch/issues/43106\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         data = torch.load(\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;34mf\"{torch.hub.get_dir()}/checkpoints/{fn}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    775\u001b[0m             \u001b[0;31m# reset back to the original position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                     warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38892756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
