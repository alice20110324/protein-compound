{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c3adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not from scratch\n",
      "rese:model_gin\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:217: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['lang_model.embed.weight', 'lang_model.embed.bias', 'lang_model.ln_f.weight', 'lang_model.ln_f.bias', 'lang_model.head.weight']\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [13:05:02] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[13:05:02] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:02] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 12 13 14\n",
      "RDKit ERROR: \n",
      "[13:05:02] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 12 13 14\n",
      "\n",
      "RDKit ERROR: [13:05:02] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:02] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:02] Explicit valence for atom # 30 N, 4, is greater than permitted\n",
      "[13:05:02] Explicit valence for atom # 30 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:02] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[13:05:02] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:03] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:03] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:03] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "[13:05:03] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:03] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[13:05:03] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:03] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:03] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:03] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[13:05:03] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:03] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[13:05:03] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:03] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:03] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:04] Can't kekulize mol.  Unkekulized atoms: 8 10 11 13 15\n",
      "[13:05:04] Can't kekulize mol.  Unkekulized atoms: 8 10 11 13 15\n",
      "\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [13:05:04] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[13:05:04] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [13:05:04] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[13:05:04] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:04] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[13:05:04] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [13:05:04] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[13:05:04] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:04] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[13:05:04] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:04] Can't kekulize mol.  Unkekulized atoms: 9 13 15 16 21\n",
      "RDKit ERROR: \n",
      "[13:05:04] Can't kekulize mol.  Unkekulized atoms: 9 13 15 16 21\n",
      "\n",
      "RDKit ERROR: [13:05:04] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "RDKit ERROR: \n",
      "[13:05:04] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "\n",
      "RDKit ERROR: [13:05:05] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[13:05:05] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:05] Can't kekulize mol.  Unkekulized atoms: 3 5 7\n",
      "RDKit ERROR: \n",
      "[13:05:05] Can't kekulize mol.  Unkekulized atoms: 3 5 7\n",
      "\n",
      "RDKit ERROR: [13:05:05] Can't kekulize mol.  Unkekulized atoms: 35 37 38 39 40\n",
      "RDKit ERROR: \n",
      "[13:05:05] Can't kekulize mol.  Unkekulized atoms: 35 37 38 39 40\n",
      "\n",
      "RDKit ERROR: [13:05:05] Explicit valence for atom # 36 N, 4, is greater than permitted\n",
      "[13:05:05] Explicit valence for atom # 36 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:05] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[13:05:05] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:05] Explicit valence for atom # 31 N, 4, is greater than permitted\n",
      "[13:05:05] Explicit valence for atom # 31 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:05] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "RDKit ERROR: \n",
      "[13:05:05] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "\n",
      "RDKit ERROR: [13:05:05] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:05] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:06] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:06] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:06] Can't kekulize mol.  Unkekulized atoms: 22 24 25 26 27\n",
      "RDKit ERROR: \n",
      "[13:05:06] Can't kekulize mol.  Unkekulized atoms: 22 24 25 26 27\n",
      "\n",
      "RDKit ERROR: [13:05:06] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "RDKit ERROR: \n",
      "[13:05:06] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "\n",
      "RDKit ERROR: [13:05:06] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[13:05:06] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:06] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "[13:05:06] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:07] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[13:05:07] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [13:05:07] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:07] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:07] Can't kekulize mol.  Unkekulized atoms: 8 11 12\n",
      "RDKit ERROR: \n",
      "[13:05:07] Can't kekulize mol.  Unkekulized atoms: 8 11 12\n",
      "\n",
      "RDKit ERROR: [13:05:07] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:07] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:07] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[13:05:07] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:07] Can't kekulize mol.  Unkekulized atoms: 22 23 25\n",
      "RDKit ERROR: \n",
      "[13:05:07] Can't kekulize mol.  Unkekulized atoms: 22 23 25\n",
      "\n",
      "RDKit ERROR: [13:05:07] Can't kekulize mol.  Unkekulized atoms: 9 10 12\n",
      "RDKit ERROR: \n",
      "[13:05:07] Can't kekulize mol.  Unkekulized atoms: 9 10 12\n",
      "\n",
      "RDKit ERROR: [13:05:07] Can't kekulize mol.  Unkekulized atoms: 11 16 17\n",
      "RDKit ERROR: \n",
      "[13:05:07] Can't kekulize mol.  Unkekulized atoms: 11 16 17\n",
      "\n",
      "RDKit ERROR: [13:05:08] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[13:05:08] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:08] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:08] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:08] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[13:05:08] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:08] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "RDKit ERROR: \n",
      "[13:05:08] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "\n",
      "RDKit ERROR: [13:05:08] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:08] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [13:05:09] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "[13:05:09] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [13:05:09] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[13:05:09] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:09] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[13:05:09] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:05:10] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[13:05:10] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold\n",
      "====epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Evaluation\n",
      "train: 603.689392 val: 631.403259\n",
      "====epoch 2\n",
      "====Evaluation\n",
      "train: 791.278015 val: 782.941528\n",
      "====epoch 3\n",
      "====Evaluation\n",
      "train: 537.219482 val: 568.432678\n",
      "====epoch 4\n",
      "====Evaluation\n",
      "train: 340.572296 val: 360.903717\n",
      "====epoch 5\n",
      "====Evaluation\n",
      "train: 95.864349 val: 103.706299\n",
      "====epoch 6\n",
      "====Evaluation\n",
      "train: 80.913132 val: 81.769745\n",
      "====epoch 7\n",
      "====Evaluation\n",
      "train: 104.063133 val: 105.705734\n",
      "====epoch 8\n",
      "====Evaluation\n",
      "train: 71.855286 val: 71.667015\n",
      "====epoch 9\n",
      "====Evaluation\n",
      "train: 21.840590 val: 19.082817\n",
      "====epoch 10\n",
      "====Evaluation\n",
      "train: 21.829464 val: 26.960165\n",
      "====epoch 11\n",
      "====Evaluation\n",
      "train: 32.465267 val: 37.690613\n",
      "====epoch 12\n",
      "====Evaluation\n",
      "train: 27.063324 val: 31.891563\n",
      "====epoch 13\n",
      "====Evaluation\n",
      "train: 15.964361 val: 20.103291\n",
      "====epoch 14\n",
      "====Evaluation\n",
      "train: 7.649639 val: 11.487409\n",
      "====epoch 15\n",
      "====Evaluation\n",
      "train: 5.050975 val: 8.974978\n",
      "====epoch 16\n",
      "====Evaluation\n",
      "train: 5.037646 val: 8.932646\n",
      "====epoch 17\n",
      "====Evaluation\n",
      "train: 3.223722 val: 7.036849\n",
      "====epoch 18\n",
      "====Evaluation\n",
      "train: 2.292585 val: 0.339575\n",
      "====epoch 19\n",
      "====Evaluation\n",
      "train: 0.797559 val: 3.373447\n",
      "====epoch 20\n",
      "====Evaluation\n",
      "train: 2.563138 val: 6.320536\n",
      "====epoch 21\n",
      "====Evaluation\n",
      "train: 2.735876 val: 6.530490\n",
      "====epoch 22\n",
      "====Evaluation\n",
      "train: 1.840147 val: 5.546642\n",
      "====epoch 23\n",
      "====Evaluation\n",
      "train: 0.920649 val: 2.209098\n",
      "====epoch 24\n",
      "====Evaluation\n",
      "train: 1.042233 val: 1.820388\n",
      "====epoch 25\n",
      "====Evaluation\n",
      "train: 0.734403 val: 4.031353\n",
      "====epoch 26\n",
      "====Evaluation\n",
      "train: 1.458712 val: 5.090738\n",
      "====epoch 27\n",
      "====Evaluation\n",
      "train: 0.949470 val: 4.426497\n",
      "====epoch 28\n",
      "====Evaluation\n",
      "train: 0.327212 val: 3.020342\n",
      "====epoch 29\n",
      "====Evaluation\n",
      "train: 0.409365 val: 3.781582\n",
      "====epoch 30\n",
      "====Evaluation\n",
      "train: 0.295280 val: 3.710477\n",
      "====epoch 31\n",
      "====Evaluation\n",
      "train: 0.103278 val: 3.063078\n",
      "====epoch 32\n",
      "====Evaluation\n",
      "train: 1.414419 val: 5.268653\n",
      "====epoch 33\n",
      "====Evaluation\n",
      "train: 1.571842 val: 5.503439\n",
      "====epoch 34\n",
      "====Evaluation\n",
      "train: 0.807637 val: 4.696247\n",
      "====epoch 35\n",
      "====Evaluation\n",
      "train: 0.305254 val: 2.597923\n",
      "====epoch 36\n",
      "====Evaluation\n",
      "train: 0.088849 val: 3.018291\n",
      "====epoch 37\n",
      "====Evaluation\n",
      "train: 0.423498 val: 4.274548\n",
      "====epoch 38\n",
      "====Evaluation\n",
      "train: 0.932088 val: 4.858446\n",
      "====epoch 39\n",
      "====Evaluation\n",
      "train: 0.939490 val: 4.750864\n",
      "====epoch 40\n",
      "====Evaluation\n",
      "train: 0.070705 val: 3.144143\n",
      "====epoch 41\n",
      "====Evaluation\n",
      "train: 0.078991 val: 3.134546\n",
      "====epoch 42\n",
      "====Evaluation\n",
      "train: 0.322955 val: 4.195117\n",
      "====epoch 43\n",
      "====Evaluation\n",
      "train: 0.882733 val: 4.806142\n",
      "====epoch 44\n",
      "====Evaluation\n",
      "train: 0.101672 val: 3.831847\n",
      "====epoch 45\n",
      "====Evaluation\n",
      "train: 0.119919 val: 3.939408\n",
      "====epoch 46\n",
      "====Evaluation\n",
      "train: 0.259238 val: 4.115124\n",
      "====epoch 47\n",
      "====Evaluation\n",
      "train: 0.903289 val: 4.911175\n",
      "====epoch 48\n",
      "====Evaluation\n",
      "train: 0.201357 val: 4.067305\n",
      "====epoch 49\n",
      "====Evaluation\n",
      "train: 0.275592 val: 2.637701\n",
      "====epoch 50\n",
      "====Evaluation\n",
      "train: 0.041859 val: 3.239675\n",
      "====epoch 51\n",
      "====Evaluation\n",
      "train: 1.124457 val: 5.037543\n",
      "====epoch 52\n",
      "====Evaluation\n",
      "train: 0.545160 val: 4.334663\n",
      "====epoch 53\n",
      "====Evaluation\n",
      "train: 0.687966 val: 2.092948\n",
      "====epoch 54\n",
      "====Evaluation\n",
      "train: 0.042450 val: 3.164918\n",
      "====epoch 55\n",
      "====Evaluation\n",
      "train: 1.166277 val: 5.043800\n",
      "====epoch 56\n",
      "====Evaluation\n",
      "train: 1.238755 val: 5.109442\n",
      "====epoch 57\n",
      "====Evaluation\n",
      "train: 0.385059 val: 4.237850\n",
      "====epoch 58\n",
      "====Evaluation\n",
      "train: 0.889758 val: 1.981021\n",
      "====epoch 59\n",
      "====Evaluation\n",
      "train: 0.810237 val: 2.196504\n",
      "====epoch 60\n",
      "====Evaluation\n",
      "train: 0.538565 val: 4.111341\n",
      "====epoch 61\n",
      "====Evaluation\n",
      "train: 1.468818 val: 5.591414\n",
      "====epoch 62\n",
      "====Evaluation\n",
      "train: 2.139345 val: 6.403975\n",
      "====epoch 63\n",
      "====Evaluation\n",
      "train: 2.065054 val: 6.159160\n",
      "====epoch 64\n",
      "====Evaluation\n",
      "train: 1.673024 val: 5.639969\n",
      "====epoch 65\n",
      "====Evaluation\n",
      "train: 0.966738 val: 4.778012\n",
      "====epoch 66\n",
      "====Evaluation\n",
      "train: 0.268547 val: 3.705224\n",
      "====epoch 67\n",
      "====Evaluation\n",
      "train: 0.093102 val: 3.373075\n",
      "====epoch 68\n",
      "====Evaluation\n",
      "train: 0.088661 val: 2.906922\n",
      "====epoch 69\n",
      "====Evaluation\n",
      "train: 0.067518 val: 3.079219\n",
      "====epoch 70\n",
      "====Evaluation\n",
      "train: 0.649144 val: 4.339856\n",
      "====epoch 71\n",
      "====Evaluation\n",
      "train: 0.699378 val: 4.354482\n",
      "====epoch 72\n",
      "====Evaluation\n",
      "train: 0.160735 val: 3.443823\n",
      "====epoch 73\n",
      "====Evaluation\n",
      "train: 0.130505 val: 3.343760\n",
      "====epoch 74\n",
      "====Evaluation\n",
      "train: 0.653395 val: 4.335176\n",
      "====epoch 75\n",
      "====Evaluation\n",
      "train: 0.695824 val: 4.397807\n",
      "====epoch 76\n",
      "====Evaluation\n",
      "train: 0.286633 val: 2.580009\n",
      "====epoch 77\n",
      "====Evaluation\n",
      "train: 0.218408 val: 3.727142\n",
      "====epoch 78\n",
      "====Evaluation\n",
      "train: 1.056685 val: 4.850062\n",
      "====epoch 79\n",
      "====Evaluation\n",
      "train: 0.850737 val: 4.655402\n",
      "====epoch 80\n",
      "====Evaluation\n",
      "train: 0.238957 val: 3.913051\n",
      "====epoch 81\n",
      "====Evaluation\n",
      "train: 0.127815 val: 3.015532\n",
      "====epoch 82\n",
      "====Evaluation\n",
      "train: 0.054665 val: 3.517998\n",
      "====epoch 83\n",
      "====Evaluation\n",
      "train: 0.559395 val: 4.406779\n",
      "====epoch 84\n",
      "====Evaluation\n",
      "train: 0.627592 val: 4.483940\n",
      "====epoch 85\n",
      "====Evaluation\n",
      "train: 0.584043 val: 4.518801\n",
      "====epoch 86\n",
      "====Evaluation\n",
      "train: 0.402397 val: 4.404595\n",
      "====epoch 87\n",
      "====Evaluation\n",
      "train: 0.058717 val: 3.502336\n",
      "====epoch 88\n",
      "====Evaluation\n",
      "train: 0.406398 val: 2.773891\n",
      "====epoch 89\n",
      "====Evaluation\n",
      "train: 1.103498 val: 1.976373\n",
      "====epoch 90\n",
      "====Evaluation\n",
      "train: 0.542592 val: 2.671824\n",
      "====epoch 91\n",
      "====Evaluation\n",
      "train: 0.337525 val: 4.365854\n",
      "====epoch 92\n",
      "====Evaluation\n",
      "train: 0.960704 val: 5.129066\n",
      "====epoch 93\n",
      "====Evaluation\n",
      "train: 1.160050 val: 5.101110\n",
      "====epoch 94\n",
      "====Evaluation\n",
      "train: 1.227815 val: 5.047570\n",
      "====epoch 95\n",
      "====Evaluation\n",
      "train: 1.233263 val: 4.988850\n",
      "====epoch 96\n",
      "====Evaluation\n",
      "train: 1.217139 val: 4.925603\n",
      "====epoch 97\n",
      "====Evaluation\n",
      "train: 1.185210 val: 4.858427\n",
      "====epoch 98\n",
      "====Evaluation\n",
      "train: 1.140299 val: 4.787863\n",
      "====epoch 99\n",
      "training Epoch [100/100], Loss: 1.1522\n",
      "====Evaluation\n",
      "train: 1.095636 val: 4.714406\n",
      "====epoch 100\n",
      "====Evaluation\n",
      "train: 1.048087 val: 4.638624\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[13:12:13] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "[13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "RDKit ERROR: \n",
      "[13:12:13] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "RDKit ERROR: \n",
      "[13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[13:12:13] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "\n",
      "[13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "[13:12:13] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[13:12:13] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "\n",
      "[13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "RDKit ERROR: \n",
      "[13:12:13] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[13:12:13] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[13:12:13] Explicit valence for atom # 16 C, 6, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 0.326936 \n"
     ]
    }
   ],
   "source": [
    "from loader1 import MoleculeDatasetBig, SeqDataset,SeqMolDataset,SmileDataset#########################\n",
    "import torch\n",
    "import torch\n",
    "#import args\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "#from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from splitters import scaffold_split,scaffold_split_1\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "import esm\n",
    "\n",
    "#from SeqMolModel import InteractionModel,InteractionModel_1,SequenceModel,InteractionModel_4\n",
    "#from SeqMolSmile import InteractionModel_4\n",
    "#from SeqMolModel import InteractionModel_4\n",
    "from SeqSmile_model1 import InteractionModel_4\n",
    "print(torch.cuda.is_available())\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda._initialized = True\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')#0000\n",
    "parser.add_argument('--batch_size', type=int, default=64,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='number of epochs to train (default: 150)')\n",
    "parser.add_argument('--lr', type=float, default=0.1,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "parser.add_argument('--decay', type=float, default=0,\n",
    "                        help='weight decay (default: 0)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "parser.add_argument('--dataset', type=str, default = 'affinity', help='root directory of dataset. For now, only classification.')\n",
    "#parser.add_argument('--input_model_file', type=str, default = 'None', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--input_model_file', type=str, default = 'Mole-BERT', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--filename', type=str, default = '', help='output filename')\n",
    "parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "parser.add_argument('--runseed', type=int, default=0, help = \"Seed for minibatch selection, random initialization.\")\n",
    "parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "parser.add_argument('--eval_train', type=int, default = 1, help='evaluating training or not')\n",
    "parser.add_argument('--num_workers', type=int, default = 4, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default = 12, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_layer', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--d_dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--n_embd', type=int, default = 768, help='number of workers for dataset loading')\n",
    "parser.add_argument('--dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--lr_start', type=float, default =  3e-5, help='number of workers for dataset loading')\n",
    "parser.add_argument('--max_epochs', type=int, default = 500, help='number of workers for dataset loading')\n",
    "parser.add_argument('--num_feats', type=int, default = 32, help='number of workers for dataset loading')\n",
    "parser.add_argument('--checkpoint_every', type=int, default = 100, help='number of workers for dataset loading')\n",
    "parser.add_argument('--seed_path', type=str, default =  'data/checkpoints/N-Step-Checkpoint_3_30000.ckpt', help='number of workers for dataset loading')\n",
    "parser.add_argument('--dims', type=list, default = [ 768, 768, 768, 1], help='number of workers for dataset loading')\n",
    "\n",
    "args = parser.parse_args(args=[])###############33\n",
    "\n",
    "\n",
    "#load pretained model of Mole-Bert\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.runseed)\n",
    "\n",
    "num_tasks=1\n",
    "# Load ESM-2 model\n",
    "protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "protein_model.to(device)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "#print(protein_alphabet)\n",
    "#alphabet = esm.Alphabet.from_architecture(model_data[\"args\"].arch)\n",
    "#batch_converter = alphabet.get_batch_converter()\n",
    "protein_model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "#self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "molecular_model = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "###################################\n",
    "if not args.input_model_file == \"None\":###############\n",
    "    print('Not from scratch')\n",
    "    molecular_model.from_pretrained('model_gin/{}.pth'.format(args.input_model_file))\n",
    "    print('rese:model_gin')\n",
    "molecular_model.to(device)\n",
    "for i,p in enumerate(molecular_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "\n",
    "'''\n",
    "model_param_group = []\n",
    "model_param_group.append({\"params\": molecular_model.gnn.parameters()})\n",
    "if args.graph_pooling == \"attention\":\n",
    "    model_param_group.append({\"params\": molecular_model.pool.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "'''\n",
    "'''\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「Dreamcatcher风」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/Wind_2028/article/details/120541017   \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "model_param_group.append({\"params\": molecular_model.graph_pred_linear.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "optimizer = optim.Adam(model_param_group, lr=0.01, weight_decay=args.decay)#############\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
    "#from finetune.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "#from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
    "from functools import partial\n",
    "from apex import optimizers\n",
    "import subprocess\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "#from utils import normalize_smiles\n",
    "import sys\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "from tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "#from utils import normalize_smiles\n",
    "# create a function (this my favorite choice)\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(LightningModule, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        #self.hparams = config\n",
    "        #self.mode = config.mode\n",
    "        self.save_hyperparameters(config)\n",
    "        self.tokenizer=tokenizer\n",
    "        '''\n",
    "        self.min_loss = {\n",
    "            self.hparams.measure_name + \"min_valid_loss\": torch.finfo(torch.float32).max,\n",
    "            self.hparams.measure_name + \"min_epoch\": 0,\n",
    "        }\n",
    "        '''\n",
    "        # Word embeddings layer\n",
    "        n_vocab, d_emb = len(tokenizer.vocab), config.n_embd\n",
    "        # input embedding stem\n",
    "        \n",
    "        builder = rotate_builder.from_kwargs(\n",
    "            n_layers=config.n_layer,\n",
    "            n_heads=config.n_head,\n",
    "            query_dimensions=config.n_embd//config.n_head,\n",
    "            value_dimensions=config.n_embd//config.n_head,\n",
    "            feed_forward_dimensions=config.n_embd,\n",
    "            attention_type='linear',\n",
    "            feature_map=partial(GeneralizedRandomFeatures, n_dims=config.num_feats),\n",
    "            activation='gelu',\n",
    "            )\n",
    "        self.pos_emb = None\n",
    "        self.tok_emb = nn.Embedding(n_vocab, config.n_embd)\n",
    "        #print('self.tok_emb:',self.tok_emb)\n",
    "        self.drop = nn.Dropout(config.d_dropout)\n",
    "        \n",
    "        ## transformer\n",
    "        self.blocks = builder.get()\n",
    "        #self.lang_model = self.lm_layer(config.n_embd, n_vocab)\n",
    "        #self.train_config = config\n",
    "        #if we are starting from scratch set seeds\n",
    "        #########################################\n",
    "        # protein_emb_dim, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "        #########################################\n",
    "        '''\n",
    "        self.fcs = []  \n",
    "        self.loss = torch.nn.L1Loss()\n",
    "        self.net = self.Net(\n",
    "            config.n_embd, dims=config.dims, dropout=config.dropout,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        dims = [150, 50, 50, 2]\n",
    "\n",
    "\n",
    "        def __init__(self, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.desc_skip_connection = True \n",
    "            self.fcs = []  # nn.ModuleList()\n",
    "            #print('dropout is {}'.format(dropout))\n",
    "\n",
    "            self.fc1 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.relu1 = nn.GELU()\n",
    "            self.fc2 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.relu2 = nn.GELU()\n",
    "            self.final = nn.Linear(smiles_embed_dim, 1)\n",
    "\n",
    "        def forward(self, smiles_emb):\n",
    "            x_out = self.fc1(smiles_emb)\n",
    "            x_out = self.dropout1(x_out)\n",
    "            x_out = self.relu1(x_out)\n",
    "\n",
    "            if self.desc_skip_connection is True:\n",
    "                x_out = x_out + smiles_emb\n",
    "\n",
    "            z = self.fc2(x_out)\n",
    "            z = self.dropout2(z)\n",
    "            z = self.relu2(z)\n",
    "            if self.desc_skip_connection is True:\n",
    "                z = self.final(z + x_out)\n",
    "            else:\n",
    "                z = self.final(z)\n",
    "\n",
    "            return z\n",
    "\n",
    "    class lm_layer(nn.Module):\n",
    "        def __init__(self, n_embd, n_vocab):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Linear(n_embd, n_embd)\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.head = nn.Linear(n_embd, n_vocab, bias=False)\n",
    "        def forward(self, tensor):\n",
    "            tensor = self.embed(tensor)\n",
    "            tensor = F.gelu(tensor)\n",
    "            tensor = self.ln_f(tensor)\n",
    "            tensor = self.head(tensor)\n",
    "            return tensor\n",
    "\n",
    "    def get_loss(self, smiles_emb, measures):\n",
    "\n",
    "        z_pred = self.net.forward(smiles_emb).squeeze()\n",
    "        measures = measures.float()\n",
    "\n",
    "        return self.loss(z_pred, measures), z_pred, measures\n",
    "    \n",
    "    \n",
    "margs = args\n",
    "tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "seed.seed_everything(margs.seed)\n",
    "if margs.seed_path == '':\n",
    "    #print(\"# training from scratch\")\n",
    "    smile_model = LightningModule(margs, tokenizer)\n",
    "else:\n",
    "    #print(\"# loaded pre-trained model from {args.seed_path}\")\n",
    "    smile_model = LightningModule(margs, tokenizer).load_from_checkpoint(margs.seed_path, strict=False, config=margs, tokenizer=tokenizer, vocab=len(tokenizer.vocab))#########################33\n",
    "\n",
    "#print('model:',smile_model)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(smile_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "    \n",
    "\n",
    "#num_tasks=1\n",
    "model= InteractionModel_4(protein_model=protein_model,smile_model=smile_model,protein_embd_dim=1280,num_tasks=1,device=device,smile_embd_dim=768) \n",
    "model.to(device)\n",
    "#print(model)#nice#num_tasks=1\n",
    "\n",
    "\n",
    "###################when doing geometric.data process, if there is some changes, please delete the directory process and let it generate again\n",
    "\n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset, dataset=args.dataset)###########################转换成了分子图的格式\n",
    "#print('args.dataset:',args.dataset)\n",
    "#print(gnn_dataset)\n",
    "#for i,gnn_data in enumerate(gnn_dataset):\n",
    "    #print(gnn_data)\n",
    "    \n",
    "seq_dataset=SeqDataset('dataset/affinity/processed/sequence.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#seq_dataset[2]\n",
    "\n",
    "def collate(batch):\n",
    "    #print('collate_batch:',batch)\n",
    "    #print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    #print('tokens[1]_mask:',tokens[1])\n",
    "    #print('colate###########################')\n",
    "    #for i,m in enumerate(tokens):\n",
    "            \n",
    "    #print('collate_tokens_input_ids:',tokens['input_ids'])\n",
    "    #print('colate_tokens_attention_mask:',tokens['attention_mask'])\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "    \n",
    "smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#####test with chartGPT  DataSet extends two parents' classes\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MultiDatasetMixin:\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.dataset3=dataset3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset1), len(self.dataset2),len(self.dataset3))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dataset1[idx], self.dataset2[idx],self.dataset3[idx]\n",
    "\n",
    "class CustomMultiDataset(MultiDatasetMixin, Dataset):###############3extends two classes\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        MultiDatasetMixin.__init__(self, dataset1, dataset2,dataset3)\n",
    "\n",
    "\n",
    "#chartGPT太厉害了，杀了我吧\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
    "\n",
    "class MultiDataLoader:\n",
    "    def __init__(self, dataloader1, dataloader2,dataloader3):\n",
    "        self.dataloader1 = dataloader1\n",
    "        self.dataloader2 = dataloader2\n",
    "        self.dataloader3=dataloader3\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data1, data2,data3 in zip(self.dataloader1, self.dataloader2, self.dataloader3):\n",
    "            yield data1, data2, data3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader1), len(self.dataloader2), len(self.dataloader3))\n",
    "\n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.dataloader1.shuffle = shuffle\n",
    "        self.dataloader2.shuffle = shuffle\n",
    "        self.dataloader3.shuffle=shuffle\n",
    "\n",
    "        \n",
    "###########split train dataset validate dataset test dataset\n",
    "seq_gnn_smile_dataset=MultiDatasetMixin(seq_dataset,gnn_dataset,smiles_dataset)\n",
    "\n",
    "#print('seq_dataset:',seq_dataset)\n",
    "\n",
    "#seq_dataloader=DataLoader(seq_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.num_workers)\n",
    "'''\n",
    "for i ,seq in enumerate(seq_dataloader):\n",
    "    print(seq)\n",
    "'''\n",
    "if args.split == \"scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        \n",
    "        #print('smiles_list:',smiles_list)\n",
    "        mol_train_dataset, mol_valid_dataset, mol_test_dataset ,seq_train_dataset,seq_valid_dataset,seq_test_dataset,smile_train_dataset,smile_valid_dataset,smile_test_dataset= scaffold_split_1(seq_gnn_smile_dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)##########dataset\n",
    "        print(\"scaffold\")\n",
    "elif args.split == \"random\":\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random\")\n",
    "elif args.split == \"random_scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random scaffold\")\n",
    "else:\n",
    "        raise ValueError(\"Invalid split option.\")\n",
    "\n",
    "#print('++++++++++', mol_train_dataset[0])\n",
    "'''\n",
    "for i, mol in enumerate(mol_train_dataset):\n",
    "    print('mol:',mol)\n",
    "'''\n",
    "#seq_mol_train_dataset=MultiDatasetMixini(seq_train_dataset,mol_train_dataset)\n",
    "#seq_mol_valid_dataset=SeqMolDataset(seq_valid_dataset,mol_valid_dataset)\n",
    "#seq_mol_test_dataset=SeqMolDataset(seq_train_dataset,mol_test_dataset)\n",
    "seq_train_dataloader1 = DataLoader(seq_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_train_dataloader2 = GeometricDataLoader(mol_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_train_dataloader3=DataLoader(smile_train_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "seq_mol_smile_train_multi_loader = MultiDataLoader(seq_train_dataloader1, mol_train_dataloader2,smile_train_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_train_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "print('seq_train_dataset#########:',seq_train_dataset)\n",
    "\n",
    "for i,seq in enumerate(seq_train_dataloader1):\n",
    "    print(seq)\n",
    "'''\n",
    "seq_valid_dataloader1 = DataLoader(seq_valid_dataset, batch_size=args.batch_size,shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_valid_dataloader2 = GeometricDataLoader(mol_valid_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_valid_dataloader3=DataLoader(smile_valid_dataset, batch_size=args.batch_size,collate_fn=collate,shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "'''\n",
    "for i,m in enumerate(smile_train_dataloader3):\n",
    "    #print('m@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:',m)\n",
    "    break\n",
    "'''\n",
    "seq_mol_smile_valid_multi_loader = MultiDataLoader(seq_valid_dataloader1, mol_valid_dataloader2,smile_valid_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_valid_multi_loader.set_shuffle(True)\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_valid_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "for i ,(seq,mol,smile) in enumerate(seq_mol_smile_train_multi_loader):\n",
    "    print(seq)\n",
    "    print(mol)\n",
    "    print(smile)\n",
    "'''\n",
    "'''\n",
    "print('mol_dataloader:')\n",
    "for mol in mol_train_dataloader2:\n",
    "    print(mol)\n",
    "for seq in seq_train_dataloader1:\n",
    "    print(seq)\n",
    " '''   \n",
    "\n",
    "def train(args, epoch, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    save_pt='results/model2/'\n",
    "    #epoch_iter = tqdm(loader, desc=\"Iteration\")\n",
    "    for step, (A,B,C) in enumerate(loader):\n",
    "        \n",
    "        seq_data_list=[]\n",
    "        seq=A\n",
    "        lenth=len(seq)\n",
    "        for m , s in enumerate(seq):\n",
    "            seq_data_list.append((str(m),s))\n",
    "        B=B.to(device)\n",
    "        D,E=C\n",
    "        D=D.to(device)\n",
    "        E=E.to(device)\n",
    "        C=(D,E)\n",
    "        #print('D!!!!!!!!!!!!!!!!:',D)\n",
    "        #print('E#####################:',E)\n",
    "        #pred=model(seq_data_list,B,C)#model is error\n",
    "        pred=model(seq_data_list,C)#model is error\n",
    "        #pred=pred.to(torch.float32)\n",
    "        y_true = B.y.view(pred.shape).to(torch.float32)\n",
    "        #loss = criterion(pred, y_true)\n",
    "        \n",
    "        loss1=criterion(pred,y_true)\n",
    "        #loss2=criterion(u12,u34)\n",
    "        #print('loss1{0},loss2{1}:',loss1,loss2)\n",
    "        #loss=loss1+loss2\n",
    "        loss=loss1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #epoch_iter.set_description(f\"Epoch: {epoch} tloss: {loss:.4f}\")\n",
    "        nn=(epoch+1)//10\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'training Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "            torch.save(model, save_pt+f'full_model_{nn}.pt')\n",
    "    #return loss.item\n",
    "def eval(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            D,E=C\n",
    "            D=D.to(device)\n",
    "            E=E.to(device)##################\n",
    "            C=(D,E)\n",
    "        \n",
    "        \n",
    "            #pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            pred=model(seq_data_list,C)#model is error\n",
    "            y_true=B.y.view(pred.shape).to(torch.float32)\n",
    "            val_loss = criterion(pred, y_true)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "\n",
    "    #y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "    #y_scores = torch.cat(y_scores, dim = 0).detach().cpu().numpy()\n",
    "    '''\n",
    "    roc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        #AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:,i] == 1) > 0 and np.sum(y_true[:,i] == -1) > 0:\n",
    "            is_valid = y_true[:,i]**2 > 0\n",
    "            roc_list.append(roc_auc_score((y_true[is_valid,i] + 1)/2, y_scores[is_valid,i]))\n",
    "    if len(roc_list)==0:#########################\n",
    "        return 0\n",
    "    if len(roc_list) < y_true.shape[1]:\n",
    "        print(\"Some target is missing!\")\n",
    "        miss_ratio=(1 - float(len(roc_list))/y_true.shape[1])\n",
    "        print(\"Missing ratio: %f\" %(1 - float(len(roc_list))/y_true.shape[1]))\n",
    "    '''\n",
    "    \n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "results_save_file='results/model2/results_save_model1.txt'\n",
    "\n",
    "\n",
    "import torch, gc\n",
    "#criterion = nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "criterion=nn.SmoothL1Loss()\n",
    "#criterion=nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "if not args.filename == \"\":\n",
    "    fname = 'runs/seq_mol_finetune_cls_runseed' + str(args.runseed) + '/' + args.filename\n",
    "    #delete the directory if there exists one\n",
    "    if os.path.exists(fname):\n",
    "        shutil.rmtree(fname)\n",
    "        print(\"removed the existing file.\")\n",
    "    writer = SummaryWriter(fname)\n",
    "\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    print(\"====epoch \" + str(epoch))\n",
    "        \n",
    "    train(args, epoch, model, device, seq_mol_smile_train_multi_loader, optimizer)\n",
    "\n",
    "    print(\"====Evaluation\")\n",
    "    if args.eval_train:\n",
    "        train_loss = eval(args, model, device, seq_mol_smile_train_multi_loader)\n",
    "    else:\n",
    "        print(\"omit the training accuracy computation\")\n",
    "        train_loss = 0\n",
    "    val_loss = eval(args, model, device, seq_mol_smile_valid_multi_loader)\n",
    "    ##test_loss = eval(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "    with open(results_save_file, 'w+') as f:\n",
    "        f.write(str(epoch)+'\\t'+str(train_loss)+'\\t'+str(val_loss)+'\\n')\n",
    "        #f.write(epoch)\n",
    "        #f.write('\\t')\n",
    "        #f.write(train_loss)\n",
    "        #f.write('\\t')\n",
    "        #f.write(val_loss)\n",
    "        #f.write('\\n')\n",
    "        \n",
    "    print(\"train: %f val: %f\" %(train_loss, val_loss))\n",
    "    \n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset+'/test/', dataset=args.dataset)\n",
    "seq_dataset=SeqDataset('dataset/affinity/test/processed/sequence.csv')\n",
    "smiles_dataset=SmileDataset('dataset/affinity/test/processed/smiles.csv')\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_test_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "\n",
    "\n",
    "test_loss = eval(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "print(\"test: %f \" %(test_loss))    \n",
    "\n",
    "#test: 0.0926936\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee8973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
