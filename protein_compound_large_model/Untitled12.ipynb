{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "import torch\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import gc\n",
    "from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from splitters import scaffold_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "import esm\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "class InteractionModel(torch.nn.Module):\n",
    "    def __init__(self, protein_model,molecular_model,mol_emb_dim,pro_emb_dim,num_tasks, aggr = \"add\"):\n",
    "        super( InteractionModel, self ).__init__() \n",
    "        \n",
    "        self.batch_converter=protein_model.alphabet.get_batch_converter()\n",
    "        \n",
    "        #self.molecular.node_representation=molecular.model.gnn()\n",
    "        #self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "        self.protein_sequence_representations = []\n",
    "        self.molecular_model=molecular_model############微调，必须写成类成员  self，否则就不能微调\n",
    "        self.pred_linear = torch.nn.Linear(pro_emb_dim+mol_emb_dim, num_tasks)\n",
    "        \n",
    "        def forward(self, protein_data,*molecular_data):\n",
    "            ############protein\n",
    "            batch_labels, batch_strs, batch_tokens = self.batch_converter(protein_data)\n",
    "            batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "            with torch.no_grad():\n",
    "                results = protein_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        \n",
    "            protein_token_representations = results[\"representations\"][33]\n",
    "            \n",
    "            for i, tokens_len in enumerate(batch_lens):\n",
    "                self.protein_sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "            \n",
    "            \n",
    "            ################molecular\n",
    "            \n",
    "            #set up optimizer\n",
    "            #different learning rate for different part of GNN\n",
    "            molecular_node_representation=self.molecular_model.gnn(molecular_data[0],molecular_data[1],molecular_data[2])\n",
    "            molecular_representation=self.molecular_model.pool(molecular.node_representation,molecular_data[3])\n",
    "            #molecular_representation=self.molecular.features(*molecular_data)\n",
    "            \n",
    "            \n",
    "            \n",
    "            all_features=torch.concat([self.protein_sequence_representations,molecular_representation],axis=1)\n",
    "            self.pred_linear(all_features)\n",
    "\n",
    "class InteractionModel_2(torch.nn.Module):\n",
    "    def __init__(self, protein_model,molecular_model,mol_emb_dim,pro_emb_dim,num_tasks, device,aggr = \"add\"):\n",
    "        super( InteractionModel_2, self ).__init__() \n",
    "        self.protein_model=protein_model.to(device)\n",
    "        self.alphabet=protein_model.alphabet##########.to(device)\n",
    "        self.batch_converter=self.alphabet.get_batch_converter()\n",
    "        self.device=device\n",
    "        #self.molecular.node_representation=molecular.model.gnn()\n",
    "        #self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "        self.protein_sequence_representations = []\n",
    "        #self.molecular_model############微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_model=molecular_model.gnn#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_pool=molecular_model.pool#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.pred_linear = torch.nn.Linear(pro_emb_dim+mol_emb_dim, num_tasks)\n",
    "        \n",
    "    #def forward(self, protein_data,*molecular_data):#******不能有，否则出错\n",
    "    def forward(self, protein_data,molecular_data):#\n",
    "        ############protein\n",
    "        print('self.device:',self.device)\n",
    "        print('protein_data:',protein_data)\n",
    "            \n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(protein_data)\n",
    "        batch_tokens=batch_tokens.to(self.device)######################\n",
    "        print('batch_tokens:',batch_tokens.device.type)\n",
    "        batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "        print('protein_batch_lens:',len(batch_lens))\n",
    "        with torch.no_grad():\n",
    "            results = self.protein_model(batch_tokens, repr_layers=[33], return_contacts=True)###############3\n",
    "        \n",
    "        protein_token_representations = results[\"representations\"][33]\n",
    "        print('protein_token_representations:',protein_token_representations.shape)\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            self.protein_sequence_representations.append(protein_token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "            \n",
    "        print('self.protein_sequence_representations:',self.protein_sequence_representations)\n",
    "        ################molecular\n",
    "            \n",
    "        #set up optimizer\n",
    "        #different learning rate for different part of GNN\n",
    "        molecular_node_representation=self.mole_model(molecular_data.x,molecular_data.edge_index,molecular_data.edge_attr)\n",
    "        print('molecular_node_representation:',molecular_node_representation)\n",
    "        molecular_representation=self.mole_pool(molecular_node_representation,molecular_data.batch)\n",
    "        print('molecular_representation:',molecular_representation.shape)\n",
    "        print('molecular_batch:',len(molecular_data.batch))\n",
    "        #molecular_representation=self.molecular.features(*molecular_data)\n",
    "            \n",
    "            \n",
    "            \n",
    "        all_features=torch.concat([self.protein_sequence_representations,molecular_representation],axis=1)\n",
    "        print('all_features:',all_features)\n",
    "        out=self.pred_linear(all_features)\n",
    "        return out\n",
    "\n",
    "\n",
    "'''    \n",
    "#CrossAttention\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim1, input_dim2)###################妙\n",
    "        self.linear2 = nn.Linear(input_dim2, input_dim1)\n",
    "\n",
    "    def forward(self, x1, x2,d):\n",
    "        # 计算注意力权重\n",
    "        #print('x1,x2:',x1.shape,x2.shape)\n",
    "        attn_weights = torch.matmul(self.linear1(x1), x2.transpose(0, 1))\n",
    "        #print('attn_wights:',attn_weights.shape)\n",
    "        #attn_weights = attn_weights.squeeze(dim=2)\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        # 使用注意力权重加权融合两个向量\n",
    "        fused_x1 = torch.matmul(attn_weights, x2)\n",
    "        #print('fused_x1:',fused_x1.shape)\n",
    "        \n",
    "        fused_x2 = torch.matmul(attn_weights.transpose(0, 1), x1)\n",
    "        #print('fused_x2:',fused_x2.shape)\n",
    "        return fused_x1, fused_x2\n",
    "'''\n",
    "#CrossAttention\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim1, input_dim2)###################妙\n",
    "        self.linear2 = nn.Linear(input_dim2, input_dim1)\n",
    "\n",
    "    def forward(self, x1, x2,d1,d2):\n",
    "        # 计算注意力权重\n",
    "        #print('x1,x2:',x1.shape,x2.shape)\n",
    "        attn_weights2 = torch.matmul(self.linear1(x1), x2.transpose(0, 1))\n",
    "        #print('attn_wights:',attn_weights.shape)\n",
    "        #attn_weights = attn_weights.squeeze(dim=2)\n",
    "        attn_weights2 = nn.functional.softmax(attn_weights2, dim=1)\n",
    "        \n",
    "        # 使用注意力权重加权融合两个向量\n",
    "        fused_x2 = torch.matmul(attn_weights2, x2)/np.sqrt(d2*d1)\n",
    "        #fused_x1 = torch.matmul(attn_weights2, x2)/np.sqrt(d2)\n",
    "        #print('fused_x1:',fused_x1.shape)\n",
    "        \n",
    "        attn_weights1 = torch.matmul(self.linear2(x2), x1.transpose(0, 1))\n",
    "        #print('attn_wights:',attn_weights.shape)\n",
    "        #attn_weights = attn_weights.squeeze(dim=2)\n",
    "        attn_weights1 = nn.functional.softmax(attn_weights1, dim=1)\n",
    "        fused_x1 = torch.matmul(attn_weights1, x1)/np.sqrt(d1*d2)\n",
    "        #fused_x2 = torch.matmul(attn_weights1.transpose(0, 1), x1)/np.sqrt(d1)\n",
    "        #print('fused_x2:',fused_x2.shape)\n",
    "        return fused_x1, fused_x2\n",
    "    \n",
    "    \n",
    "class InteractionModel_1(torch.nn.Module):\n",
    "    def __init__(self, protein_model,molecular_model,pro_emb_dim,num_tasks, device,aggr = \"add\"):\n",
    "        super( InteractionModel_1, self ).__init__() \n",
    "        self.protein_model=protein_model.to(device)\n",
    "        self.alphabet=protein_model.alphabet##########.to(device)\n",
    "        self.batch_converter=self.alphabet.get_batch_converter()\n",
    "        self.device=device\n",
    "        #self.molecular.node_representation=molecular.model.gnn()\n",
    "        #self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "        #\n",
    "        self.protein_sequence_representations = torch.tensor([1,2])\n",
    "        #self.molecular_model############微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_model=molecular_model.gnn#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_pool=molecular_model.pool#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.pred_linear = torch.nn.Linear(pro_emb_dim+molecular_model.emb_dim, num_tasks)\n",
    "        self.protein_linear=torch.nn.Linear(1280,1280)\n",
    "        self.protein_relu=nn.ReLU()\n",
    "        \n",
    "        #self.cross_attention = CrossAttentionLayer(input_dim1, input_dim2)\n",
    "        self.cross_attention = CrossAttentionLayer(1280, 300)\n",
    "        self.fc1 = nn.Linear(1580, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #def forward(self, protein_data,*molecular_data):#******不能有，否则出错\n",
    "    def forward(self, protein_data,molecular_data):#\n",
    "        #print('protein_data:',protein_data.shape)\n",
    "        print('molecular_data:',molecular_data.shape)\n",
    "        #print('\n",
    "        ############protein\n",
    "        print('self.device:',self.device)\n",
    "        print('protein_data:',protein_data)\n",
    "            \n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(protein_data)\n",
    "        batch_tokens=batch_tokens.to(self.device)######################\n",
    "        print('batch_tokens:',batch_tokens.device.type)\n",
    "        batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "        print('protein_batch_lens:',len(batch_lens))\n",
    "        with torch.no_grad():\n",
    "            results = self.protein_model(batch_tokens, repr_layers=[33], return_contacts=True)###############3\n",
    "        \n",
    "        protein_token_representations = results[\"representations\"][33]\n",
    "        m,n,v=protein_token_representations.shape\n",
    "        print('protein_token_representations:',protein_token_representations.shape)\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            if i==0:\n",
    "                self.protein_sequence_representations=protein_token_representations[i, 1 : tokens_len - 1].mean(0)\n",
    "            else:\n",
    "                 self.protein_sequence_representations=torch.stack([self.protein_sequence_representations,protein_token_representations[i,1:tokens_len-1].mean(0)],dim=0)\n",
    "            #self.protein_sequence_representations.append(protein_token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "        self.protein_sequence_representations=self.protein_sequence_representations.reshape(-1,1280)    \n",
    "        print('self.protein_sequence_representations:',self.protein_sequence_representations.shape)\n",
    "        ################molecular\n",
    "        self.protein_sequence_representations=self.protein_linear( self.protein_sequence_representations)############fine_tunning protein large model \n",
    "        self.protein_sequence_representations=self.protein_relu(self.protein_sequence_representations)############\n",
    "        #set up optimizer\n",
    "        #different learning rate for different part of GNN\n",
    "        molecular_node_representation=self.mole_model(molecular_data.x,molecular_data.edge_index,molecular_data.edge_attr)\n",
    "        print('molecular_node_representation:',molecular_node_representation.shape)\n",
    "        molecular_representation=self.mole_pool(molecular_node_representation,molecular_data.batch)\n",
    "        print('molecular_representation:',molecular_representation.shape)\n",
    "        print('molecular_batch:',len(molecular_data.batch))\n",
    "        #molecular_representation=self.molecular.features(*molecular_data)\n",
    "        x1= self.protein_sequence_representations\n",
    "        x2=molecular_representation\n",
    "        fused_x1, fused_x2 =  self.cross_attention(x1, x2)  \n",
    "        \n",
    "        fused_x1=F.softmax(fused_x1)\n",
    "        fused_x2=F.softmax(fused_x2)\n",
    "        print('x1:',x1.shape)\n",
    "        print('x2:',x2.shape)\n",
    "        print('fused_x1:',fused_x1.shape)\n",
    "        print('fused_x2:',fused_x2.shape)\n",
    "        \n",
    "        x11_att=x1*fused_x2\n",
    "        x22_att=x2*fused_x1\n",
    "        \n",
    "        print('x11_att:',x11_att.shape)\n",
    "        print('x22_att:',x22_att.shape)\n",
    "        x11=torch.add(x11_att,x1)\n",
    "        x22=torch.add(x22_att,x2)\n",
    "        # 合并两个融合后的向量\n",
    "        combined = torch.cat([x11, x22], dim=1)   \n",
    "        #all_features=torch.concat([self.protein_sequence_representations,molecular_representation],axis=1)\n",
    "        #print('all_features:',all_features.shape)\n",
    "        out = self.fc1(combined)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)                           \n",
    "        #out=self.pred_linear(all_features)\n",
    "        print('out:',out)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n",
    "class PMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1=nn.Linear(1,100)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.fc2=nn.Linear(100,1)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "    def forward(x):\n",
    "        out=self.dropout(self.relu1(self.fc1(x)))\n",
    "        out=self.sig(self.fc2(out))\n",
    "        return out\n",
    "pmodel=PMLP()\n",
    "\n",
    "class InteractionModel_4(torch.nn.Module):\n",
    "    def __init__(self, protein_model,molecular_model,smile_model,protein_embd_dim,mol_embd_dim,num_tasks, device,smile_embd_dim, dropout=0.2,aggr = \"mean\"):\n",
    "        super( InteractionModel_4, self ).__init__() \n",
    "        self.protein_embd_dim=protein_embd_dim\n",
    "        self.mol_embd_dim=mol_embd_dim\n",
    "        self.smile_embd_dim=smile_embd_dim\n",
    "        self.protein_model=protein_model.to(device)\n",
    "        self.alphabet=protein_model.alphabet##########.to(device)\n",
    "        self.batch_converter=self.alphabet.get_batch_converter()\n",
    "        self.device=device\n",
    "        #self.molecular.node_representation=molecular.model.gnn()\n",
    "        #self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "        #\n",
    "        self.protein_sequence_representations = torch.tensor([1,2])\n",
    "        #self.molecular_model############微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_model=molecular_model.gnn#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_pool=molecular_model.pool#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_linear=torch.nn.Linear(mol_embd_dim,mol_embd_dim)\n",
    "        self.mol_pred_linear = torch.nn.Linear(protein_embd_dim+mol_embd_dim, num_tasks)\n",
    "        self.protein_linear=torch.nn.Linear(protein_embd_dim,protein_embd_dim)\n",
    "        self.protein_relu=nn.ReLU()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.protein_embd_dim=protein_embd_dim\n",
    "        #self.cross_attention = CrossAttentionLayer(input_dim1, input_dim2)\n",
    "        self.mol_cross_attention = CrossAttentionLayer(protein_embd_dim, mol_embd_dim)\n",
    "        self.fc1 = nn.Linear(protein_embd_dim+ mol_embd_dim, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.desc_skip_connection = True \n",
    "        self.fcs = []  # nn.ModuleList()\n",
    "        #print('dropout is {}'.format(dropout))\n",
    "        \n",
    "        self.smile_model=smile_model\n",
    "        self.smile_linear=nn.Linear(smile_embd_dim,smile_embd_dim)\n",
    "        \n",
    "        self.smile_pred_linear = torch.nn.Linear(protein_embd_dim+smile_embd_dim, num_tasks)\n",
    "        self.smile_cross_attention = CrossAttentionLayer(protein_embd_dim, smile_embd_dim)\n",
    "        self.fc3 = nn.Linear(protein_embd_dim+ smile_embd_dim, 256)\n",
    "        \n",
    "        self.layer_norm_seq = nn.LayerNorm(protein_embd_dim, eps=1e-6)  # 默认对最后一个维度初始化\n",
    "        self.layer_norm_mol = nn.LayerNorm(mol_embd_dim, eps=1e-6)  # 默认对最后一个维度初始化\n",
    "        self.layer_norm_smile = nn.LayerNorm(smile_embd_dim, eps=1e-6)  # 默认对最后一个维度初始化\n",
    "        \n",
    "        \n",
    "        self.fc11 = nn.Linear(smile_embd_dim, smile_embd_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.relu1 = nn.GELU()\n",
    "        #self.fc22 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "        #self.dropout2 = nn.Dropout(dropout)\n",
    "        #self.relu2 = nn.GELU()\n",
    "        #self.final = nn.Linear(smile_embd_dim, 1)\n",
    "        #self.pmodel=PMLP().to(device)\n",
    "        #self.dropout = nn.Dropout(p=0.5)  # dropout训练\n",
    "        self.dropout = nn.Dropout(p=0.3)  # dropout训练\n",
    "        # 定义可学习参数 t\n",
    "        \n",
    "        '''\n",
    "        self.conv1=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1,out_channels=64,kernel_size=5,stride=1),\n",
    "            nn.Conv1d(in_channels=64,out_channels=128,kernel_size=5,stride=1),\n",
    "            nn.AvgPool1d(5)\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128,out_channels=64,kernel_size=5,stride=1),\n",
    "            nn.Conv1d(in_channels=64,out_channels=1,kernel_size=5,stride=1),\n",
    "            nn.AvgPool1d(5)\n",
    "        )\n",
    "        '''\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        self.t = nn.Parameter(torch.Tensor(1))\n",
    "        self.t_linear1=nn.Linear(1,50)\n",
    "        self.t_linear2=nn.Linear(50,1)\n",
    "        self.t.data.fill_(0.5)  # 初始化 t 为0.5\n",
    "        \n",
    "    #def forward(self, protein_data,*molecular_data):#******不能有，否则出错\n",
    "    def forward(self, protein_data,molecular_data,smile_data):#\n",
    "        \n",
    "        \n",
    "            \n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(protein_data)\n",
    "        batch_tokens=batch_tokens.to(self.device)######################\n",
    "        #print('batch_tokens:',batch_tokens.device.type)\n",
    "        batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "        #print('protein_batch_lens:',len(batch_lens),batch_lens)\n",
    "        results=None\n",
    "        protein_token_repressentations=None\n",
    "        u=None\n",
    "        with torch.no_grad():\n",
    "            #results = self.protein_model(batch_tokens, repr_layers=[6], return_contacts=True)###############3\n",
    "            results = self.protein_model(batch_tokens, repr_layers=[6], return_contacts=False)###############3\n",
    "        protein_token_representations = results[\"representations\"][6]\n",
    "        m,n,v=protein_token_representations.shape\n",
    "        #print('m,n,v:',m,n,v)\n",
    "        #print('protein_token_representations:',protein_token_representations.shape)\n",
    "        del results, batch_tokens\n",
    "        \n",
    "        \n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            if i==0:\n",
    "                u=protein_token_representations[i, 1 : tokens_len - 1].mean(0).reshape(-1,self.protein_embd_dim)\n",
    "                self.protein_sequence_representations=u\n",
    "                \n",
    "            else:\n",
    "                u=protein_token_representations[i, 1 : tokens_len - 1].mean(0).reshape(-1,self.protein_embd_dim)\n",
    "                self.protein_sequence_representations=torch.concat([self.protein_sequence_representations,u],dim=0)\n",
    "                \n",
    "        del protein_token_representations\n",
    "        del u\n",
    "        \n",
    "        self.protein_sequence_representations=self.dropout(self.protein_linear( self.protein_sequence_representations))############fine_tunning protein large model \n",
    "        self.protein_sequence_representations=self.protein_relu(self.protein_sequence_representations)############\n",
    "        \n",
    "        \n",
    "        molecular_node_reprresentation=None\n",
    "        with torch.no_grad():\n",
    "            molecular_node_representation=self.mole_model(molecular_data.x,molecular_data.edge_index,molecular_data.edge_attr)\n",
    "            \n",
    "        molecular_representation=self.mole_pool(molecular_node_representation,molecular_data.batch)\n",
    "        molecular_representation=self.relu(self.dropout(self.mole_linear(molecular_representation)))\n",
    "        del molecular_node_representation\n",
    "        \n",
    "        x1= self.protein_sequence_representations############for mol_bert\n",
    "        x3=self.protein_sequence_representations######################for molformer\n",
    "        x2=molecular_representation\n",
    "        \n",
    "        x1=self.layer_norm_seq(x1)\n",
    "        x2=self.layer_norm_mol(x2)\n",
    "        x3=self.layer_norm_seq(x3)\n",
    "        fused_x1, fused_x2 =  self.mol_cross_attention(x1, x2,self.protein_embd_dim,self.mol_embd_dim)  \n",
    "        \n",
    "        \n",
    "        x11_att=fused_x1\n",
    "        x22_att=fused_x2\n",
    "        \n",
    "        x11=torch.add(x11_att,x1)\n",
    "        x22=torch.add(x22_att,x2)\n",
    "        del x11_att, x22_att\n",
    "        \n",
    "        del x1,x2,fused_x1,fused_x2\n",
    "        \n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "        # 合并两个融合后的向量\n",
    "        combined12 = torch.cat([x11, x22], dim=1)   \n",
    "        \n",
    "        out12 = self.fc1(combined12)\n",
    "        out12 = self.dropout(self.relu(out12))\n",
    "        \n",
    "        out12=self.fc2(out12)\n",
    "        \n",
    "        \n",
    "        x5,mask=smile_data\n",
    "        \n",
    "        length_mask=LM(mask.sum(-1))\n",
    "        #print('length_mask:',length_mask.shape,length_mask)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_embeddings = self.smile_model.tok_emb(x5) # each index maps to a (learnable) vector\n",
    "            #print('token_embbeddings:',token_embeddings.shape)\n",
    "            x6 = self.smile_model.drop(token_embeddings)\n",
    "            x7 = self.smile_model.blocks(x6, length_mask=LM(mask.sum(-1)))\n",
    "            #x7 = self.smile_model.blocks(x6)\n",
    "            #print('smile_x7:',x7.shape)\n",
    "            token_embeddings = x7\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            #print('input_mask_expanded:',input_mask_expanded.shape)\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            #print('sum_embeddings:',sum_embeddings.shape)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            #print('sum_mask:',sum_mask.shape)\n",
    "            x4 = sum_embeddings / sum_mask####################################\n",
    "            #print('x4:',x4.shape)\n",
    "            #x4=self.smile_model(smile_data)\n",
    "            x4=self.relu1(self.dropout(self.fc11(x4)))\n",
    "            x4=self.layer_norm_smile(x4)\n",
    "        \n",
    "        fused_x3, fused_x4 =  self.smile_cross_attention(x3, x4,self.protein_embd_dim,self.smile_embd_dim)  \n",
    "        \n",
    "        #fused_x3=F.softmax(fused_x3)\n",
    "        #fused_x4=F.softmax(fused_x4)\n",
    "        #print('x3:',x3.shape)\n",
    "        #print('x4:',x4.shape)\n",
    "        #print('fused_x3:',fused_x3.shape)\n",
    "        #print('fused_x3:',fused_x4.shape)\n",
    "        \n",
    "        #x33_att=x3*fused_x4\n",
    "        #x44_att=x4*fused_x3\n",
    "        \n",
    "        x33_att=fused_x3\n",
    "        x44_att=fused_x4\n",
    "        #print('x33_att:',x33_att.shape)\n",
    "        #print('x44_att:',x44_att.shape)\n",
    "        x33=torch.add(x33_att,x3)\n",
    "        x44=torch.add(x44_att,x4)\n",
    "        # 合并两个融合后的向量\n",
    "        combined34 = torch.cat([x33, x44], dim=1)  \n",
    "        del x33_att,x44_att,fused_x3,fused_x4, x33,x44,x3,x4\n",
    "        #all_features=torch.concat([self.protein_sequence_representations,molecular_representation],axis=1)\n",
    "        #print('all_features:',all_features.shape)\n",
    "        out34 = self.fc3(combined34)\n",
    "        out34 = self.dropout(self.relu(out34))\n",
    "        #out34 = torch.sigmoid(self.fc2(out34) )  \n",
    "        out34=self.fc2(out34)\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "        \n",
    "         # 融合两个分支特征\n",
    "        t1=self.relu(self.t_linear1(self.t))\n",
    "        t2=torch.sigmoid(self.t_linear2(t1))\n",
    "        \n",
    "        out = t2 * out12 + (1 - t2) * out34\n",
    "        \n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n",
    "class InteractionModel_5(torch.nn.Module):\n",
    "    def __init__(self, protein_model,molecular_model,smile_model,protein_embd_dim,mol_embd_dim,num_tasks, device,smile_embd_dim, dropout=0.2,aggr = \"mean\"):\n",
    "        super( InteractionModel_5, self ).__init__() \n",
    "        self.protein_embd_dim=protein_embd_dim\n",
    "        self.mol_embd_dim=mol_embd_dim\n",
    "        self.smile_embd_dim=smile_embd_dim\n",
    "        self.protein_model=protein_model.to(device)\n",
    "        self.alphabet=protein_model.alphabet##########.to(device)\n",
    "        self.batch_converter=self.alphabet.get_batch_converter()\n",
    "        self.device=device\n",
    "        #self.molecular.node_representation=molecular.model.gnn()\n",
    "        #self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "        #\n",
    "        self.protein_sequence_representations = torch.tensor([1,2])\n",
    "        #self.molecular_model############微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_model=molecular_model.gnn#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_pool=molecular_model.pool#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        #self.mole_linear=torch.nn.Linear(mol_embd_dim,mol_embd_dim)\n",
    "        self.mole_linear=torch.nn.Linear(mol_embd_dim,256)\n",
    "        self.mol_pred_linear = torch.nn.Linear(protein_embd_dim+mol_embd_dim, num_tasks)\n",
    "        #self.protein_linear=torch.nn.Linear(protein_embd_dim,protein_embd_dim)\n",
    "        self.protein_linear=torch.nn.Linear(protein_embd_dim,256)\n",
    "        self.protein_relu=nn.ReLU()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.protein_embd_dim=protein_embd_dim\n",
    "        #self.cross_attention = CrossAttentionLayer(input_dim1, input_dim2)\n",
    "        #self.mol_cross_attention = CrossAttentionLayer(protein_embd_dim, mol_embd_dim)\n",
    "        self.mol_cross_attention = CrossAttentionLayer(256,256)\n",
    "        #self.fc1 = nn.Linear(protein_embd_dim+ mol_embd_dim, 256)\n",
    "        self.fc1 = nn.Linear(256+256, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.desc_skip_connection = True \n",
    "        self.fcs = []  # nn.ModuleList()\n",
    "        #print('dropout is {}'.format(dropout))\n",
    "        \n",
    "        self.smile_model=smile_model\n",
    "        #self.smile_linear=nn.Linear(smile_embd_dim,smile_embd_dim)\n",
    "        self.smile_linear=nn.Linear(smile_embd_dim,256)\n",
    "        self.smile_pred_linear = torch.nn.Linear(protein_embd_dim+smile_embd_dim, num_tasks)\n",
    "        #self.smile_cross_attention = CrossAttentionLayer(protein_embd_dim, smile_embd_dim)\n",
    "        self.smile_cross_attention = CrossAttentionLayer(256, 256)\n",
    "        #self.fc3 = nn.Linear(protein_embd_dim+ smile_embd_dim, 256)\n",
    "        self.fc3 = nn.Linear(256+256, 256)\n",
    "        #self.layer_norm_seq = nn.LayerNorm(protein_embd_dim)  # 默认对最后一个维度初始化\n",
    "        #self.layer_norm_mol = nn.LayerNorm(mol_embd_dim)  # 默认对最后一个维度初始化\n",
    "        #self.layer_norm_smile = nn.LayerNorm(smile_embd_dim)  # 默认对最后一个维度初始化\n",
    "        self.layer_norm_seq = nn.LayerNorm(256)  # 默认对最后一个维度初始化\n",
    "        self.layer_norm_mol = nn.LayerNorm(256)  # 默认对最后一个维度初始化\n",
    "        self.layer_norm_smile = nn.LayerNorm(256)  # 默认对最后一个维度初始化\n",
    "        \n",
    "        #self.fc11 = nn.Linear(smile_embd_dim, smile_embd_dim)\n",
    "        self.fc11 = nn.Linear(smile_embd_dim,256)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #self.fc22 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "        #self.dropout2 = nn.Dropout(dropout)\n",
    "        #self.relu2 = nn.GELU()\n",
    "        #self.final = nn.Linear(smile_embd_dim, 1)\n",
    "        #self.pmodel=PMLP().to(device)\n",
    "        #self.dropout = nn.Dropout(p=0.5)  # dropout训练\n",
    "        self.dropout = nn.Dropout(p=0.3)  # dropout训练\n",
    "        # 定义可学习参数 t\n",
    "        \n",
    "        '''\n",
    "        self.conv1=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1,out_channels=64,kernel_size=5,stride=1),\n",
    "            nn.Conv1d(in_channels=64,out_channels=128,kernel_size=5,stride=1),\n",
    "            nn.AvgPool1d(5)\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128,out_channels=64,kernel_size=5,stride=1),\n",
    "            nn.Conv1d(in_channels=64,out_channels=1,kernel_size=5,stride=1),\n",
    "            nn.AvgPool1d(5)\n",
    "        )\n",
    "        '''\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        self.t = nn.Parameter(torch.Tensor(1))\n",
    "        self.t_linear1=nn.Linear(1,50)\n",
    "        self.t_linear2=nn.Linear(50,1)\n",
    "        self.t.data.fill_(0.5)  # 初始化 t 为0.5\n",
    "        \n",
    "    #def forward(self, protein_data,*molecular_data):#******不能有，否则出错\n",
    "    def forward(self, protein_data,molecular_data,smile_data):#\n",
    "        \n",
    "        \n",
    "            \n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(protein_data)\n",
    "        batch_tokens=batch_tokens.to(self.device)######################\n",
    "        #print('batch_tokens:',batch_tokens.device.type)\n",
    "        batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "        #print('protein_batch_lens:',len(batch_lens),batch_lens)\n",
    "        results=None\n",
    "        protein_token_repressentations=None\n",
    "        u=None\n",
    "        with torch.no_grad():\n",
    "            #results = self.protein_model(batch_tokens, repr_layers=[6], return_contacts=True)###############3\n",
    "            results = self.protein_model(batch_tokens, repr_layers=[6], return_contacts=False)###############3\n",
    "        protein_token_representations = results[\"representations\"][6]\n",
    "        m,n,v=protein_token_representations.shape\n",
    "        #print('m,n,v:',m,n,v)\n",
    "        #print('protein_token_representations:',protein_token_representations.shape)\n",
    "        del results, batch_tokens\n",
    "        \n",
    "        \n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            if i==0:\n",
    "                u=protein_token_representations[i, 1 : tokens_len - 1].mean(0).reshape(-1,self.protein_embd_dim)\n",
    "                self.protein_sequence_representations=u\n",
    "                \n",
    "            else:\n",
    "                u=protein_token_representations[i, 1 : tokens_len - 1].mean(0).reshape(-1,self.protein_embd_dim)\n",
    "                self.protein_sequence_representations=torch.concat([self.protein_sequence_representations,u],dim=0)\n",
    "                \n",
    "        del protein_token_representations\n",
    "        del u\n",
    "        \n",
    "        self.protein_sequence_representations=self.dropout(self.protein_linear( self.protein_sequence_representations))############fine_tunning protein large model \n",
    "        self.protein_sequence_representations=self.protein_relu(self.protein_sequence_representations)############\n",
    "        \n",
    "        \n",
    "        molecular_node_reprresentation=None\n",
    "        with torch.no_grad():\n",
    "            molecular_node_representation=self.mole_model(molecular_data.x,molecular_data.edge_index,molecular_data.edge_attr)\n",
    "            \n",
    "        molecular_representation=self.mole_pool(molecular_node_representation,molecular_data.batch)\n",
    "        molecular_representation=self.relu(self.dropout(self.mole_linear(molecular_representation)))\n",
    "        del molecular_node_representation\n",
    "        \n",
    "        x1= self.protein_sequence_representations############for mol_bert\n",
    "        x3=self.protein_sequence_representations######################for molformer\n",
    "        x2=molecular_representation\n",
    "        \n",
    "        x1=self.layer_norm_seq(x1)\n",
    "        x2=self.layer_norm_mol(x2)\n",
    "        x3=self.layer_norm_seq(x3)\n",
    "        #fused_x1, fused_x2 =  self.mol_cross_attention(x1, x2,self.protein_embed_dim,self.mol_embd_dim)  \n",
    "        fused_x1, fused_x2 =  self.mol_cross_attention(x1, x2,256,256) \n",
    "        \n",
    "        x11_att=fused_x1\n",
    "        x22_att=fused_x2\n",
    "        \n",
    "        x11=torch.add(x11_att,x1)\n",
    "        x22=torch.add(x22_att,x2)\n",
    "        \n",
    "        \n",
    "        fused_x1, fused_x2 =  self.mol_cross_attention(x11, x22,256,256) \n",
    "        \n",
    "        x111_att=fused_x1\n",
    "        x222_att=fused_x2\n",
    "        \n",
    "        x111=torch.add(x111_att,x11)\n",
    "        x222=torch.add(x222_att,x22)\n",
    "        \n",
    "        \n",
    "        del x11_att, x22_att\n",
    "        del x1,x2,fused_x1,fused_x2\n",
    "        \n",
    "        del x111_att, x222_att\n",
    "        del x11,x22\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "        # 合并两个融合后的向量\n",
    "        combined12 = torch.cat([x111, x222], dim=1)   \n",
    "        \n",
    "        out12 = self.fc1(combined12)\n",
    "        out12 = self.dropout(self.relu(out12))\n",
    "        \n",
    "        out12=self.fc2(out12)\n",
    "        \n",
    "        \n",
    "        x5,mask=smile_data\n",
    "        \n",
    "        length_mask=LM(mask.sum(-1))\n",
    "        #print('length_mask:',length_mask.shape,length_mask)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_embeddings = self.smile_model.tok_emb(x5) # each index maps to a (learnable) vector\n",
    "            #print('token_embbeddings:',token_embeddings.shape)\n",
    "            x6 = self.smile_model.drop(token_embeddings)\n",
    "            x7 = self.smile_model.blocks(x6, length_mask=LM(mask.sum(-1)))\n",
    "            #x7 = self.smile_model.blocks(x6)\n",
    "            #print('smile_x7:',x7.shape)\n",
    "            token_embeddings = x7\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            #print('input_mask_expanded:',input_mask_expanded.shape)\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            #print('sum_embeddings:',sum_embeddings.shape)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            #print('sum_mask:',sum_mask.shape)\n",
    "            x4 = sum_embeddings / sum_mask####################################\n",
    "            #print('x4:',x4.shape)\n",
    "            #x4=self.smile_model(smile_data)\n",
    "            x4=self.relu1(self.dropout(self.fc11(x4)))\n",
    "            x4=self.layer_norm_smile(x4)\n",
    "        \n",
    "        #fused_x3, fused_x4 =  self.smile_cross_attention(x3, x4,self.protein_embd_dim,self.smile_embd_dim)  \n",
    "        fused_x3, fused_x4 =  self.smile_cross_attention(x3, x4,256,256)  \n",
    "        #fused_x3=F.softmax(fused_x3)\n",
    "        #fused_x4=F.softmax(fused_x4)\n",
    "        #print('x3:',x3.shape)\n",
    "        #print('x4:',x4.shape)\n",
    "        #print('fused_x3:',fused_x3.shape)\n",
    "        #print('fused_x3:',fused_x4.shape)\n",
    "        \n",
    "        #x33_att=x3*fused_x4\n",
    "        #x44_att=x4*fused_x3\n",
    "        \n",
    "        x33_att=fused_x3\n",
    "        x44_att=fused_x4\n",
    "        #print('x33_att:',x33_att.shape)\n",
    "        #print('x44_att:',x44_att.shape)\n",
    "        x33=torch.add(x33_att,x3)\n",
    "        x44=torch.add(x44_att,x4)\n",
    "        \n",
    "        \n",
    "        fused_x3, fused_x4 =  self.smile_cross_attention(x33, x44,256,256)  \n",
    "        \n",
    "        x333_att=fused_x3\n",
    "        x444_att=fused_x4\n",
    "        #print('x33_att:',x33_att.shape)\n",
    "        #print('x44_att:',x44_att.shape)\n",
    "        x333=torch.add(x333_att,x33)\n",
    "        x444=torch.add(x444_att,x44)\n",
    "        \n",
    "        del x33_att,x44_att,fused_x3,fused_x4, x33,x44,x3,x4, x333_att,x444_att\n",
    "        \n",
    "        # 合并两个融合后的向量\n",
    "        combined34 = torch.cat([x333, x444], dim=1)  \n",
    "        #del x33_att,x44_att,fused_x3,fused_x4, x33,x44,x3,x4\n",
    "        #all_features=torch.concat([self.protein_sequence_representations,molecular_representation],axis=1)\n",
    "        #print('all_features:',all_features.shape)\n",
    "        out34 = self.fc3(combined34)\n",
    "        out34 = self.dropout(self.relu(out34))\n",
    "        #out34 = torch.sigmoid(self.fc2(out34) )  \n",
    "        out34=self.fc2(out34)\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "        \n",
    "         # 融合两个分支特征\n",
    "        t1=self.relu(self.t_linear1(self.t))\n",
    "        t2=torch.sigmoid(self.t_linear2(t1))\n",
    "        \n",
    "        out = t2 * out12 + (1 - t2) * out34\n",
    "        \n",
    "        \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class InteractionModel_3(torch.nn.Module):\n",
    "    def __init__(self, protein_model,molecular_model,smile_model,protein_embd_dim,mol_embd_dim,num_tasks, device,smile_embd_dim, dropout=0.2,aggr = \"add\"):\n",
    "        super( InteractionModel_3, self ).__init__() \n",
    "        self.protein_model=protein_model.to(device)\n",
    "        self.alphabet=protein_model.alphabet##########.to(device)\n",
    "        self.batch_converter=self.alphabet.get_batch_converter()\n",
    "        self.device=device\n",
    "        #self.molecular.node_representation=molecular.model.gnn()\n",
    "        #self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "        #\n",
    "        self.protein_sequence_representations = torch.tensor([1,2])\n",
    "        #self.molecular_model############微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_model=molecular_model.gnn#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mole_pool=molecular_model.pool#nice微调，必须写成类成员  self，否则就不能微调\n",
    "        self.mol_pred_linear = torch.nn.Linear(protein_embd_dim+mol_embd_dim, num_tasks)\n",
    "        self.protein_linear=torch.nn.Linear(protein_embd_dim,protein_embd_dim)\n",
    "        self.protein_relu=nn.ReLU()\n",
    "        self.protein_embd_dim=protein_embd_dim\n",
    "        #self.cross_attention = CrossAttentionLayer(input_dim1, input_dim2)\n",
    "        self.mol_cross_attention = CrossAttentionLayer(protein_embd_dim, mol_embd_dim)\n",
    "        self.fc1 = nn.Linear(protein_embd_dim+ mol_embd_dim, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.desc_skip_connection = True \n",
    "        self.fcs = []  # nn.ModuleList()\n",
    "        #print('dropout is {}'.format(dropout))\n",
    "        \n",
    "        self.smile_model=smile_model\n",
    "        \n",
    "        \n",
    "        self.smile_pred_linear = torch.nn.Linear(protein_embd_dim+smile_embd_dim, num_tasks)\n",
    "        self.smile_cross_attention = CrossAttentionLayer(protein_embd_dim, smile_embd_dim)\n",
    "        self.fc3 = nn.Linear(protein_embd_dim+ smile_embd_dim, 256)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc11 = nn.Linear(smile_embd_dim, smile_embd_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.relu1 = nn.GELU()\n",
    "        #self.fc22 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "        #self.dropout2 = nn.Dropout(dropout)\n",
    "        #self.relu2 = nn.GELU()\n",
    "        #self.final = nn.Linear(smile_embd_dim, 1)\n",
    "        self.pmodel=PMLP().to(device)\n",
    "        \n",
    "        \n",
    "    #def forward(self, protein_data,*molecular_data):#******不能有，否则出错\n",
    "    def forward(self, protein_data,molecular_data,smile_data):#\n",
    "        ############protein\n",
    "        #SSprint('self.device:',self.device)\n",
    "        #print('protein_data:',protein_data)\n",
    "        #print('potein_data,molecular_data,smile_data:',molecular_data.shape,smile_data.shape)    \n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(protein_data)\n",
    "        batch_tokens=batch_tokens.to(self.device)######################\n",
    "        #print('batch_tokens:',batch_tokens.device.type)\n",
    "        batch_lens = (batch_tokens != self.alphabet.padding_idx).sum(1)\n",
    "        print('protein_batch_lens:',len(batch_lens),batch_lens)\n",
    "        with torch.no_grad():\n",
    "            results = self.protein_model(batch_tokens, repr_layers=[33], return_contacts=True)###############3\n",
    "        \n",
    "        protein_token_representations = results[\"representations\"][33]\n",
    "        m,n,v=protein_token_representations.shape\n",
    "        print('m,n,v:',m,n,v)\n",
    "        #print('protein_token_representations:',protein_token_representations.shape)\n",
    "        \n",
    "        \n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            if i==0:\n",
    "                u=protein_token_representations[i, 1 : tokens_len - 1].mean(0).reshape(-1,self.protein_embd_dim)\n",
    "                #u=protein_token_representations[i, 1 : tokens_len - 1].mean(0)############why 1,not 0\n",
    "                #self.protein_sequence_representations=protein_token_representations[i, 1 : tokens_len - 1].mean(0)\n",
    "                self.protein_sequence_representations=u\n",
    "                print('self.protein_sequence_representations:',self.protein_sequence_representations.shape)\n",
    "                #self.protein_sequence_representations.reshape(-1,self.protein_embd_dim)\n",
    "            else:\n",
    "                u=protein_token_representations[i, 1 : tokens_len - 1].mean(0).reshape(-1,self.protein_embd_dim)\n",
    "                #u=protein_token_representations[i, 1 : tokens_len - 1].mean(0)\n",
    "                                #self.protein_sequence_representations=torch.stack([self.protein_sequence_representations,protein_token_representations[i,1:tokens_len-1].mean(0)],dim=1)###############dim=0,dim=1\n",
    "                self.protein_sequence_representations=torch.concat([self.protein_sequence_representations,u],dim=0)\n",
    "                            \n",
    "                #print('self.protein_sequence_representations:',self.protein_sequence_representations.shape)\n",
    "                #self.protein_sequence_representations.append(protein_token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "                #print('self.protein_sequence_representations:',self.protein_sequence_representations.shape)\n",
    "                #print('self.protein_sequence_representations:',self.protein_sequence_representations.shape)\n",
    "            \n",
    "        ################molecular\n",
    "        \n",
    "        #self.protein_sequence_representations=protein_token_representations[:,1:tokens_len-1].mean(0)\n",
    "        print('self.protein_sequence_representations:',self.protein_sequence_representations.shape)\n",
    "        self.protein_sequence_representations=self.protein_linear( self.protein_sequence_representations)############fine_tunning protein large model \n",
    "        self.protein_sequence_representations=self.protein_relu(self.protein_sequence_representations)############\n",
    "        #set up optimizer\n",
    "        #different learning rate for different part of GNN\n",
    "        molecular_node_representation=self.mole_model(molecular_data.x,molecular_data.edge_index,molecular_data.edge_attr)\n",
    "        #print('molecular_node_representation:',molecular_node_representation.shape)\n",
    "        molecular_representation=self.mole_pool(molecular_node_representation,molecular_data.batch)\n",
    "        #print('molecular_representation:',molecular_representation.shape)\n",
    "        #print('molecular_batch:',len(molecular_data.batch))\n",
    "        #molecular_representation=self.molecular.features(*molecular_data)\n",
    "        x1= self.protein_sequence_representations############for mol_bert\n",
    "        x3=self.protein_sequence_representations######################for molformer\n",
    "        x2=molecular_representation\n",
    "        fused_x1, fused_x2 =  self.mol_cross_attention(x1, x2)  \n",
    "        \n",
    "        fused_x1=F.softmax(fused_x1)\n",
    "        fused_x2=F.softmax(fused_x2)\n",
    "        #print('x1:',x1.shape)\n",
    "        #print('x2:',x2.shape)\n",
    "        #print('fused_x1:',fused_x1.shape)\n",
    "        #print('fused_x2:',fused_x2.shape)\n",
    "        \n",
    "        x11_att=x1*fused_x2\n",
    "        x22_att=x2*fused_x1\n",
    "        \n",
    "        #print('x11_att:',x11_att.shape)\n",
    "        #print('x22_att:',x22_att.shape)\n",
    "        x11=torch.add(x11_att,x1)\n",
    "        x22=torch.add(x22_att,x2)\n",
    "        # 合并两个融合后的向量\n",
    "        combined12 = torch.cat([x11, x22], dim=1)   \n",
    "        #all_features=torch.concat([self.protein_sequence_representations,molecular_representation],axis=1)\n",
    "        #print('all_features:',all_features.shape)\n",
    "        out12 = self.fc1(combined12)\n",
    "        out12 = self.relu(out12)\n",
    "        out12 = self.fc2(out12)                           \n",
    "        #out=self.pred_linear(all_features)\n",
    "        print('out12:',out12)\n",
    "        \n",
    "        x5,mask=smile_data\n",
    "        print('x5:',x5.shape)\n",
    "        print('mask:',mask.shape)\n",
    "        length_mask=LM(mask.sum(-1))\n",
    "        print('length_mask:',length_mask.shape)\n",
    "        token_embeddings = self.smile_model.tok_emb(x5) # each index maps to a (learnable) vector\n",
    "        print('token_embbeddings:',token_embeddings.shape)\n",
    "        x6 = self.smile_model.drop(token_embeddings)\n",
    "        #x7 = self.smile_model.blocks(x6, length_mask=LM(mask.sum(-1)))\n",
    "        x7 = self.smile_model.blocks(x6)\n",
    "        token_embeddings = x7\n",
    "        input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        print('input_mask_expanded:',input_mask_expanded.shape)\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        print('sum_embeddings:',sum_embeddings.shape)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        print('sum_mask:',sum_mask.shape)\n",
    "        x4 = sum_embeddings / sum_mask####################################\n",
    "        print('x4:',x4.shape)\n",
    "        #x4=self.smile_model(smile_data)\n",
    "        x4=self.relu1(self.fc11(x4))\n",
    "        \n",
    "        fused_x3, fused_x4 =  self.smile_cross_attention(x3, x4)  \n",
    "        \n",
    "        fused_x3=F.softmax(fused_x3)\n",
    "        fused_x4=F.softmax(fused_x4)\n",
    "        #print('x3:',x3.shape)\n",
    "        #print('x4:',x4.shape)\n",
    "        #print('fused_x3:',fused_x3.shape)\n",
    "        #print('fused_x3:',fused_x4.shape)\n",
    "        \n",
    "        x33_att=x3*fused_x4\n",
    "        x44_att=x4*fused_x3\n",
    "        \n",
    "        #print('x33_att:',x33_att.shape)\n",
    "        #print('x44_att:',x44_att.shape)\n",
    "        x33=torch.add(x33_att,x3)\n",
    "        x44=torch.add(x44_att,x4)\n",
    "        # 合并两个融合后的向量\n",
    "        combined34 = torch.cat([x33, x44], dim=1)   \n",
    "        #all_features=torch.concat([self.protein_sequence_representations,molecular_representation],axis=1)\n",
    "        #print('all_features:',all_features.shape)\n",
    "        out34 = self.fc3(combined34)\n",
    "        out34 = self.relu(out34)\n",
    "        out34 = self.fc2(out34)   \n",
    "        \n",
    "        #print('out12:',out12.shape,out12)\n",
    "        u=self.pmodel(out12)\n",
    "        #print('u:',u.shape,u)\n",
    "        out=u*out12+(1-u)*out34\n",
    "        \n",
    "        #out=out12*0.5+out34*0.5\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "class SequenceModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super( SequenceModel, self ).__init__() \n",
    "        self.protein_model, self.protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.batch_converter=self.protein_alphabet.get_batch_converter()\n",
    "        \n",
    "        \n",
    "        self.protein_sequence_representations = []\n",
    "        \n",
    "        \n",
    "    def forward(self, protein_data):\n",
    "        ############protein\n",
    "        print('protein_data:',protein_data)\n",
    "            \n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter(protein_data)\n",
    "        batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "        with torch.no_grad():\n",
    "            results = protein_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        \n",
    "        protein_token_representations = results[\"representations\"][33]\n",
    "        print('protein_token_representations:',protein_token_representations)\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            self.protein_sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "            \n",
    "        \n",
    "        return self.protein_sequence_representations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
