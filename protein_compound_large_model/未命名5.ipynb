{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d18d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Dataset, InMemoryDataset\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, root, name='all', transform=None,\n",
    "                 pre_transform=None, pre_filter=None, num_workers=4):\n",
    "        self.dataset_names = ['all' , 'training', 'validation', 'test']\n",
    "        assert name in self.dataset_names, \"'name' should be chosen from 'all', 'training', 'validation', and 'test'. \"\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.output_files = [f for f in self.processed_paths if f.split(os.sep)[-3] == name]  \n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        all_files = glob.glob(osp.join(self.raw_dir, '*', '*', '*'))\n",
    "        return [osp.join(*f.split(os.sep)[-3:]) for f in all_files if osp.isfile(f)]  \n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [osp.splitext(osp.join(*f.split(os.sep)[-3:]))[0]+'.pt' for f in self.raw_file_names]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def read_events(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    # Define a function that processes a single raw_path\n",
    "    def process_raw_path(self,raw_path):\n",
    "        data = self.read_events(raw_path)\n",
    "        data.file_id = osp.basename(raw_path)\n",
    "        data.label = [raw_path.split(os.sep)[-2]]\n",
    "        data.y = torch.tensor([self.categories.index(data.label[0])])\n",
    "\n",
    "        if self.pre_filter is not None and not self.pre_filter(data):\n",
    "            return None\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        idx = self.raw_paths.index(raw_path)\n",
    " \n",
    "        torch.save(data, self.processed_paths[idx])\n",
    "        # print(f\"{idx} {self.raw_file_names[idx]}' is processed and saved as '{self.processed_file_names[idx]}'.\")\n",
    "\n",
    "    \n",
    "    def process_raw_paths_batch(self,raw_paths):\n",
    "        for idx_and_raw_path in raw_paths:\n",
    "            _, raw_path = idx_and_raw_path\n",
    "            data = self.read_events(raw_path)\n",
    "            data.file_id = osp.basename(raw_path)\n",
    "            data.label = [raw_path.split(os.sep)[-2]]\n",
    "            data.y = torch.tensor([self.categories.index(data.label[0])])\n",
    "  \n",
    "            if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                continue\n",
    "            \n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "            \n",
    "            tail_raw_path = raw_path.split(os.sep)[-3:]\n",
    "            tail_raw_path[-1] = osp.splitext(tail_raw_path[-1])[0] + '.pt'\n",
    "            torch.save(data, osp.join(self.processed_dir, *tail_raw_path))\n",
    "\n",
    "\n",
    "    def divide_list_into_consecutive_groups(self, input_list, num_groups):\n",
    "        group_size = len(input_list) // num_groups\n",
    "        remaining = len(input_list) % num_groups\n",
    "\n",
    "        groups = []\n",
    "        start_index = 0\n",
    "\n",
    "        for _ in range(num_groups):\n",
    "            group_end = start_index + group_size + (1 if remaining > 0 else 0)\n",
    "            groups.append([(i, item) for i, item in enumerate(input_list[start_index:group_end], start=start_index)])\n",
    "            start_index = group_end\n",
    "            remaining -= 1\n",
    "\n",
    "        return groups\n",
    "    \n",
    "       \n",
    "    @property\n",
    "    def categories(self):\n",
    "        return sorted(os.listdir(osp.join(self.raw_dir, 'all')))\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        return len(self.categories)\n",
    "    \n",
    "    def process(self):\n",
    "        processed_paths_all = [f for f in self.processed_paths if f.split(os.sep)[-3] == 'all']\n",
    "        if  len(processed_paths_all) == 0 or not all([osp.exists(f) for f in processed_paths_all]):\n",
    "            raw_paths_all = [f for f in self.raw_paths if f.split(os.sep)[-3] == 'all']\n",
    "            self.process_all(raw_paths_all)\n",
    " \n",
    "        for name in self.dataset_names:\n",
    "            if name != 'all':\n",
    "                for category in self.categories:\n",
    "                    if not osp.exists(osp.join(self.processed_dir, name, category)):\n",
    "                        os.makedirs(osp.join(self.processed_dir, name, category))\n",
    "        \n",
    "                name_files = [f for f in self.processed_paths if f.split(os.sep)[-3] == name]\n",
    "                for file in name_files:\n",
    "                    src_path = osp.join(self.processed_dir, 'all', *file.split(os.sep)[-2:])\n",
    "                    try:\n",
    "                        os.symlink(osp.relpath(src_path,osp.dirname(file)),file)\n",
    "                    except FileExistsError:\n",
    "                        os.unlink(file)\n",
    "                        os.symlink(osp.relpath(src_path,osp.dirname(file)),file)\n",
    "                        \n",
    "                print(f\"Sym links for '{name}' dataset are created again.\")\n",
    "        \n",
    "    \n",
    "    def process_all(self,raw_paths_all):\n",
    "        for category in self.categories:\n",
    "            if not os.path.exists(osp.join(self.processed_dir,'all', category)):\n",
    "                os.makedirs(osp.join(self.processed_dir,'all', category))\n",
    "        \n",
    "        num_workers = self.num_workers\n",
    "        \n",
    "        # groups = [[] for _ in range(num_workers)]\n",
    "\n",
    "        # for index, raw_path in enumerate(self.raw_paths):\n",
    "        #     group_index = index % num_workers\n",
    "        #     groups[group_index].append((index, raw_path))\n",
    "        \n",
    "        groups = self.divide_list_into_consecutive_groups(raw_paths_all,num_workers)\n",
    "        \n",
    "        processes = []\n",
    "        \n",
    "        for group_index in range(num_workers):\n",
    "            process = multiprocessing.Process(target=self.process_raw_paths_batch, args=(groups[group_index],))\n",
    "            processes.append(process)\n",
    "            process.start()\n",
    "\n",
    "        for process in processes:\n",
    "            process.join()   \n",
    "                \n",
    "        # with Pool(processes=num_workers) as pool:\n",
    "        #     pool.map(self.process_raw_path, all_name_files)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.output_files)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        data = torch.load(self.output_files[idx])\n",
    "        return data       \n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}({len(self)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb684176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseInMemoryDataset(InMemoryDataset):    \n",
    "    def __init__(self, root, name='all', transform=None,\n",
    "                 pre_transform=None, pre_filter=None, num_workers=None):\n",
    "        self.dataset_names = ['all' , 'training', 'validation', 'test']\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        assert name in self.dataset_names, \"'name' should be chosen from 'all', 'training', 'validation', and 'test'. \"\n",
    "        path = self.processed_paths[self.dataset_names.index(name)]\n",
    "        self.data, self.slices = torch.load(path)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.dataset_names     \n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [name + '_filenames_sign_polarity.pt' for name in self.dataset_names]\n",
    "\n",
    "    def read_events(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        for i, path in enumerate(self.processed_paths):\n",
    "            torch.save(self.process_set(self.raw_paths[i]), path)\n",
    "            print(f\"'{self.raw_file_names[i]}' is processed and saved as '{self.processed_file_names[i]}'.\")\n",
    "\n",
    "    def process_set(self, raw_path):\n",
    "        categories = glob.glob(osp.join(raw_path, '*', ''))\n",
    "        categories = sorted([x.split(os.sep)[-2] for x in categories])\n",
    "\n",
    "        data_list = []\n",
    "        for target, category in enumerate(categories):\n",
    "            folder = osp.join(raw_path, category)\n",
    "            paths = glob.glob(f'{folder}{os.sep}*')\n",
    "            for path in tqdm(paths):\n",
    "                data = self.read_events(path)\n",
    "                data.file_id = osp.basename(path)\n",
    "                data.label = [category]\n",
    "                data.y = torch.tensor([target])\n",
    "                data_list.append(data)\n",
    "            print(f'category {category} is done!')\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [d for d in data_list if self.pre_filter(d)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(d) for d in data_list]\n",
    "        \n",
    "        return self.collate(data_list)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}({len(self)})'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
