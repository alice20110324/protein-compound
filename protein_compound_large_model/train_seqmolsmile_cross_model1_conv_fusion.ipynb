{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b99ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Not from scratch\n",
      "rese:model_gin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:217: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['lang_model.embed.weight', 'lang_model.embed.bias', 'lang_model.ln_f.weight', 'lang_model.ln_f.bias', 'lang_model.head.weight']\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [20:02:29] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[20:02:29] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:29] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 12 13 14\n",
      "RDKit ERROR: \n",
      "[20:02:29] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 12 13 14\n",
      "\n",
      "RDKit ERROR: [20:02:29] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[20:02:29] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [20:02:29] Explicit valence for atom # 30 N, 4, is greater than permitted\n",
      "[20:02:29] Explicit valence for atom # 30 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:29] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[20:02:29] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:30] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "[20:02:30] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [20:02:30] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "[20:02:30] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:30] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[20:02:30] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:30] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[20:02:30] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [20:02:30] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[20:02:30] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:30] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[20:02:30] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:30] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[20:02:30] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [20:02:31] Can't kekulize mol.  Unkekulized atoms: 8 10 11 13 15\n",
      "RDKit ERROR: \n",
      "[20:02:31] Can't kekulize mol.  Unkekulized atoms: 8 10 11 13 15\n",
      "\n",
      "RDKit ERROR: [20:02:31] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[20:02:31] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [20:02:31] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[20:02:31] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:31] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[20:02:31] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [20:02:31] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[20:02:31] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:31] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[20:02:31] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:31] Can't kekulize mol.  Unkekulized atoms: 9 13 15 16 21\n",
      "RDKit ERROR: \n",
      "[20:02:31] Can't kekulize mol.  Unkekulized atoms: 9 13 15 16 21\n",
      "\n",
      "RDKit ERROR: [20:02:31] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "RDKit ERROR: \n",
      "[20:02:31] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "\n",
      "RDKit ERROR: [20:02:31] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[20:02:31] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:32] Can't kekulize mol.  Unkekulized atoms: 3 5 7\n",
      "RDKit ERROR: \n",
      "[20:02:32] Can't kekulize mol.  Unkekulized atoms: 3 5 7\n",
      "\n",
      "RDKit ERROR: [20:02:32] Can't kekulize mol.  Unkekulized atoms: 35 37 38 39 40\n",
      "RDKit ERROR: \n",
      "[20:02:32] Can't kekulize mol.  Unkekulized atoms: 35 37 38 39 40\n",
      "\n",
      "RDKit ERROR: [20:02:32] Explicit valence for atom # 36 N, 4, is greater than permitted\n",
      "[20:02:32] Explicit valence for atom # 36 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:32] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[20:02:32] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:32] Explicit valence for atom # 31 N, 4, is greater than permitted\n",
      "[20:02:32] Explicit valence for atom # 31 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:32] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "RDKit ERROR: \n",
      "[20:02:32] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "\n",
      "RDKit ERROR: [20:02:32] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[20:02:32] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [20:02:32] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[20:02:32] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [20:02:33] Can't kekulize mol.  Unkekulized atoms: 22 24 25 26 27\n",
      "RDKit ERROR: \n",
      "[20:02:33] Can't kekulize mol.  Unkekulized atoms: 22 24 25 26 27\n",
      "\n",
      "RDKit ERROR: [20:02:33] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "RDKit ERROR: \n",
      "[20:02:33] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "\n",
      "RDKit ERROR: [20:02:33] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[20:02:33] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:34] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "[20:02:34] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:34] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[20:02:34] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [20:02:34] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[20:02:34] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [20:02:34] Can't kekulize mol.  Unkekulized atoms: 8 11 12\n",
      "[20:02:34] Can't kekulize mol.  Unkekulized atoms: 8 11 12\n",
      "\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [20:02:34] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[20:02:34] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [20:02:34] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[20:02:34] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:34] Can't kekulize mol.  Unkekulized atoms: 22 23 25\n",
      "RDKit ERROR: \n",
      "[20:02:34] Can't kekulize mol.  Unkekulized atoms: 22 23 25\n",
      "\n",
      "RDKit ERROR: [20:02:34] Can't kekulize mol.  Unkekulized atoms: 9 10 12\n",
      "RDKit ERROR: \n",
      "[20:02:34] Can't kekulize mol.  Unkekulized atoms: 9 10 12\n",
      "\n",
      "RDKit ERROR: [20:02:34] Can't kekulize mol.  Unkekulized atoms: 11 16 17\n",
      "RDKit ERROR: \n",
      "[20:02:34] Can't kekulize mol.  Unkekulized atoms: 11 16 17\n",
      "\n",
      "RDKit ERROR: [20:02:35] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[20:02:35] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:35] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[20:02:35] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [20:02:35] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[20:02:35] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:35] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "RDKit ERROR: \n",
      "[20:02:35] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "\n",
      "RDKit ERROR: [20:02:35] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[20:02:35] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [20:02:36] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "RDKit ERROR: \n",
      "[20:02:36] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "\n",
      "RDKit ERROR: [20:02:36] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[20:02:36] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:37] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[20:02:37] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:37] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "[20:02:37] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[20:02:39] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "[20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "RDKit ERROR: \n",
      "[20:02:39] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "RDKit ERROR: \n",
      "[20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[20:02:39] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "[20:02:39] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[20:02:39] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "\n",
      "[20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "RDKit ERROR: \n",
      "[20:02:39] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[20:02:39] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[20:02:39] Explicit valence for atom # 16 C, 6, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====epoch 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 64 elements not 1580",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 626\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m====epoch \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch))\n\u001b[0;32m--> 626\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_mol_smile_train_multi_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m====Evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_train:\n",
      "Cell \u001b[0;32mIn[1], line 502\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, epoch, model, device, loader, optimizer)\u001b[0m\n\u001b[1;32m    498\u001b[0m C\u001b[38;5;241m=\u001b[39m(D,E)\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m#print('D!!!!!!!!!!!!!!!!:',D)\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m#print('E#####################:',E)\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m#pred=model(seq_data_list,B,C)#model is error\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m pred\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#model is error\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m#pred=pred.to(torch.float32)\u001b[39;00m\n\u001b[1;32m    504\u001b[0m y_true \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mview(pred\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/esm-main/SeqMolSmile_cross_model1_conv_fusion.py:507\u001b[0m, in \u001b[0;36mInteractionModel_4.forward\u001b[0;34m(self, protein_data, molecular_data, smile_data)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m#print('combined12:',combined12.shape)\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m#all_features=torch.concat([self.protein_sequence_representations,molecular_representation],axis=1)\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m#print('all_features:',all_features.shape)\u001b[39;00m\n\u001b[1;32m    506\u001b[0m combined12\u001b[38;5;241m=\u001b[39mcombined12\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotein_embd_dim\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmol_embd_dim)\n\u001b[0;32m--> 507\u001b[0m out12\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined12\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m#print('out12:',out12.shape)\u001b[39;00m\n\u001b[1;32m    509\u001b[0m out12\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out12)\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 64 elements not 1580"
     ]
    }
   ],
   "source": [
    "from loader1 import MoleculeDatasetBig, SeqDataset,SeqMolDataset,SmileDataset#########################\n",
    "import torch\n",
    "import torch\n",
    "#import args\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "#from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from splitters import scaffold_split,scaffold_split_1\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "import esm\n",
    "\n",
    "#from SeqMolModel import InteractionModel,InteractionModel_1,SequenceModel,InteractionModel_4\n",
    "#from SeqMolSmile import InteractionModel_4\n",
    "#from SeqMolModel import InteractionModel_4\n",
    "from SeqMolSmile_cross_model1_conv_fusion import InteractionModel_4\n",
    "print(torch.cuda.is_available())\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda._initialized = True\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')#0000\n",
    "parser.add_argument('--batch_size', type=int, default=64,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=150,\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "parser.add_argument('--decay', type=float, default=0,\n",
    "                        help='weight decay (default: 0)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "parser.add_argument('--dataset', type=str, default = 'affinity', help='root directory of dataset. For now, only classification.')\n",
    "#parser.add_argument('--input_model_file', type=str, default = 'None', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--input_model_file', type=str, default = 'Mole-BERT', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--filename', type=str, default = '', help='output filename')\n",
    "parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "parser.add_argument('--runseed', type=int, default=0, help = \"Seed for minibatch selection, random initialization.\")\n",
    "parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "parser.add_argument('--eval_train', type=int, default = 1, help='evaluating training or not')\n",
    "parser.add_argument('--num_workers', type=int, default = 4, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default = 12, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_layer', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--d_dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--n_embd', type=int, default = 768, help='number of workers for dataset loading')\n",
    "parser.add_argument('--dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--lr_start', type=float, default =  3e-5, help='number of workers for dataset loading')\n",
    "parser.add_argument('--max_epochs', type=int, default = 500, help='number of workers for dataset loading')\n",
    "parser.add_argument('--num_feats', type=int, default = 32, help='number of workers for dataset loading')\n",
    "parser.add_argument('--checkpoint_every', type=int, default = 100, help='number of workers for dataset loading')\n",
    "parser.add_argument('--seed_path', type=str, default =  'data/checkpoints/N-Step-Checkpoint_3_30000.ckpt', help='number of workers for dataset loading')\n",
    "parser.add_argument('--dims', type=list, default = [ 768, 768, 768, 1], help='number of workers for dataset loading')\n",
    "\n",
    "args = parser.parse_args(args=[])###############33\n",
    "\n",
    "\n",
    "#load pretained model of Mole-Bert\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.runseed)\n",
    "\n",
    "num_tasks=1\n",
    "# Load ESM-2 model\n",
    "protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "protein_model.to(device)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "#print(protein_alphabet)\n",
    "#alphabet = esm.Alphabet.from_architecture(model_data[\"args\"].arch)\n",
    "#batch_converter = alphabet.get_batch_converter()\n",
    "protein_model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "#self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "molecular_model = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "###################################\n",
    "if not args.input_model_file == \"None\":###############\n",
    "    print('Not from scratch')\n",
    "    molecular_model.from_pretrained('model_gin/{}.pth'.format(args.input_model_file))\n",
    "    print('rese:model_gin')\n",
    "molecular_model.to(device)\n",
    "for i,p in enumerate(molecular_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "\n",
    "'''\n",
    "model_param_group = []\n",
    "model_param_group.append({\"params\": molecular_model.gnn.parameters()})\n",
    "if args.graph_pooling == \"attention\":\n",
    "    model_param_group.append({\"params\": molecular_model.pool.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "'''\n",
    "'''\n",
    "\n",
    "CSDNDreamcatcherCC 4.0 BY-SA\n",
    "https://blog.csdn.net/Wind_2028/article/details/120541017   \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "model_param_group.append({\"params\": molecular_model.graph_pred_linear.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "optimizer = optim.Adam(model_param_group, lr=0.01, weight_decay=args.decay)#############\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
    "#from finetune.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "#from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
    "from functools import partial\n",
    "from apex import optimizers\n",
    "import subprocess\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "#from utils import normalize_smiles\n",
    "import sys\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "from tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "#from utils import normalize_smiles\n",
    "# create a function (this my favorite choice)\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(LightningModule, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        #self.hparams = config\n",
    "        #self.mode = config.mode\n",
    "        self.save_hyperparameters(config)\n",
    "        self.tokenizer=tokenizer\n",
    "        '''\n",
    "        self.min_loss = {\n",
    "            self.hparams.measure_name + \"min_valid_loss\": torch.finfo(torch.float32).max,\n",
    "            self.hparams.measure_name + \"min_epoch\": 0,\n",
    "        }\n",
    "        '''\n",
    "        # Word embeddings layer\n",
    "        n_vocab, d_emb = len(tokenizer.vocab), config.n_embd\n",
    "        # input embedding stem\n",
    "        \n",
    "        builder = rotate_builder.from_kwargs(\n",
    "            n_layers=config.n_layer,\n",
    "            n_heads=config.n_head,\n",
    "            query_dimensions=config.n_embd//config.n_head,\n",
    "            value_dimensions=config.n_embd//config.n_head,\n",
    "            feed_forward_dimensions=config.n_embd,\n",
    "            attention_type='linear',\n",
    "            feature_map=partial(GeneralizedRandomFeatures, n_dims=config.num_feats),\n",
    "            activation='gelu',\n",
    "            )\n",
    "        self.pos_emb = None\n",
    "        self.tok_emb = nn.Embedding(n_vocab, config.n_embd)\n",
    "        #print('self.tok_emb:',self.tok_emb)\n",
    "        self.drop = nn.Dropout(config.d_dropout)\n",
    "        \n",
    "        ## transformer\n",
    "        self.blocks = builder.get()\n",
    "        #self.lang_model = self.lm_layer(config.n_embd, n_vocab)\n",
    "        #self.train_config = config\n",
    "        #if we are starting from scratch set seeds\n",
    "        #########################################\n",
    "        # protein_emb_dim, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "        #########################################\n",
    "        '''\n",
    "        self.fcs = []  \n",
    "        self.loss = torch.nn.L1Loss()\n",
    "        self.net = self.Net(\n",
    "            config.n_embd, dims=config.dims, dropout=config.dropout,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        dims = [150, 50, 50, 2]\n",
    "\n",
    "\n",
    "        def __init__(self, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.desc_skip_connection = True \n",
    "            self.fcs = []  # nn.ModuleList()\n",
    "            #print('dropout is {}'.format(dropout))\n",
    "\n",
    "            self.fc1 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.relu1 = nn.GELU()\n",
    "            self.fc2 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.relu2 = nn.GELU()\n",
    "            self.final = nn.Linear(smiles_embed_dim, 1)\n",
    "\n",
    "        def forward(self, smiles_emb):\n",
    "            x_out = self.fc1(smiles_emb)\n",
    "            x_out = self.dropout1(x_out)\n",
    "            x_out = self.relu1(x_out)\n",
    "\n",
    "            if self.desc_skip_connection is True:\n",
    "                x_out = x_out + smiles_emb\n",
    "\n",
    "            z = self.fc2(x_out)\n",
    "            z = self.dropout2(z)\n",
    "            z = self.relu2(z)\n",
    "            if self.desc_skip_connection is True:\n",
    "                z = self.final(z + x_out)\n",
    "            else:\n",
    "                z = self.final(z)\n",
    "\n",
    "            return z\n",
    "\n",
    "    class lm_layer(nn.Module):\n",
    "        def __init__(self, n_embd, n_vocab):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Linear(n_embd, n_embd)\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.head = nn.Linear(n_embd, n_vocab, bias=False)\n",
    "        def forward(self, tensor):\n",
    "            tensor = self.embed(tensor)\n",
    "            tensor = F.gelu(tensor)\n",
    "            tensor = self.ln_f(tensor)\n",
    "            tensor = self.head(tensor)\n",
    "            return tensor\n",
    "\n",
    "    def get_loss(self, smiles_emb, measures):\n",
    "\n",
    "        z_pred = self.net.forward(smiles_emb).squeeze()\n",
    "        measures = measures.float()\n",
    "\n",
    "        return self.loss(z_pred, measures), z_pred, measures\n",
    "    \n",
    "    \n",
    "margs = args\n",
    "tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "seed.seed_everything(margs.seed)\n",
    "if margs.seed_path == '':\n",
    "    #print(\"# training from scratch\")\n",
    "    smile_model = LightningModule(margs, tokenizer)\n",
    "else:\n",
    "    #print(\"# loaded pre-trained model from {args.seed_path}\")\n",
    "    smile_model = LightningModule(margs, tokenizer).load_from_checkpoint(margs.seed_path, strict=False, config=margs, tokenizer=tokenizer, vocab=len(tokenizer.vocab))#########################33\n",
    "\n",
    "#print('model:',smile_model)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(smile_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "    \n",
    "\n",
    "#num_tasks=1\n",
    "model= InteractionModel_4(protein_model=protein_model,molecular_model=molecular_model,smile_model=smile_model,protein_embd_dim=1280,num_tasks=1,device=device,mol_embd_dim=300,smile_embd_dim=768) \n",
    "model.to(device)\n",
    "#print(model)#nice#num_tasks=1\n",
    "\n",
    "\n",
    "###################when doing geometric.data process, if there is some changes, please delete the directory process and let it generate again\n",
    "\n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset, dataset=args.dataset)###########################\n",
    "#print('args.dataset:',args.dataset)\n",
    "#print(gnn_dataset)\n",
    "#for i,gnn_data in enumerate(gnn_dataset):\n",
    "    #print(gnn_data)\n",
    "    \n",
    "seq_dataset=SeqDataset('dataset/affinity/processed/sequence.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#seq_dataset[2]\n",
    "\n",
    "def collate(batch):\n",
    "    #print('collate_batch:',batch)\n",
    "    #print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    #print('tokens[1]_mask:',tokens[1])\n",
    "    #print('colate###########################')\n",
    "    #for i,m in enumerate(tokens):\n",
    "            \n",
    "    #print('collate_tokens_input_ids:',tokens['input_ids'])\n",
    "    #print('colate_tokens_attention_mask:',tokens['attention_mask'])\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "    \n",
    "smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#####test with chartGPT  DataSet extends two parents' classes\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MultiDatasetMixin:\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.dataset3=dataset3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset1), len(self.dataset2),len(self.dataset3))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dataset1[idx], self.dataset2[idx],self.dataset3[idx]\n",
    "\n",
    "class CustomMultiDataset(MultiDatasetMixin, Dataset):###############3extends two classes\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        MultiDatasetMixin.__init__(self, dataset1, dataset2,dataset3)\n",
    "\n",
    "\n",
    "#chartGPT\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
    "\n",
    "class MultiDataLoader:\n",
    "    def __init__(self, dataloader1, dataloader2,dataloader3):\n",
    "        self.dataloader1 = dataloader1\n",
    "        self.dataloader2 = dataloader2\n",
    "        self.dataloader3=dataloader3\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data1, data2,data3 in zip(self.dataloader1, self.dataloader2, self.dataloader3):\n",
    "            yield data1, data2, data3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader1), len(self.dataloader2), len(self.dataloader3))\n",
    "\n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.dataloader1.shuffle = shuffle\n",
    "        self.dataloader2.shuffle = shuffle\n",
    "        self.dataloader3.shuffle=shuffle\n",
    "\n",
    "        \n",
    "###########split train dataset validate dataset test dataset\n",
    "seq_gnn_smile_dataset=MultiDatasetMixin(seq_dataset,gnn_dataset,smiles_dataset)\n",
    "\n",
    "#print('seq_dataset:',seq_dataset)\n",
    "\n",
    "#seq_dataloader=DataLoader(seq_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.num_workers)\n",
    "'''\n",
    "for i ,seq in enumerate(seq_dataloader):\n",
    "    print(seq)\n",
    "'''\n",
    "if args.split == \"scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        \n",
    "        #print('smiles_list:',smiles_list)\n",
    "        mol_train_dataset, mol_valid_dataset, mol_test_dataset ,seq_train_dataset,seq_valid_dataset,seq_test_dataset,smile_train_dataset,smile_valid_dataset,smile_test_dataset= scaffold_split_1(seq_gnn_smile_dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)##########dataset\n",
    "        print(\"scaffold\")\n",
    "elif args.split == \"random\":\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random\")\n",
    "elif args.split == \"random_scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random scaffold\")\n",
    "else:\n",
    "        raise ValueError(\"Invalid split option.\")\n",
    "\n",
    "#print('++++++++++', mol_train_dataset[0])\n",
    "'''\n",
    "for i, mol in enumerate(mol_train_dataset):\n",
    "    print('mol:',mol)\n",
    "'''\n",
    "#seq_mol_train_dataset=MultiDatasetMixini(seq_train_dataset,mol_train_dataset)\n",
    "#seq_mol_valid_dataset=SeqMolDataset(seq_valid_dataset,mol_valid_dataset)\n",
    "#seq_mol_test_dataset=SeqMolDataset(seq_train_dataset,mol_test_dataset)\n",
    "seq_train_dataloader1 = DataLoader(seq_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_train_dataloader2 = GeometricDataLoader(mol_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_train_dataloader3=DataLoader(smile_train_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "seq_mol_smile_train_multi_loader = MultiDataLoader(seq_train_dataloader1, mol_train_dataloader2,smile_train_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_train_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "print('seq_train_dataset#########:',seq_train_dataset)\n",
    "\n",
    "for i,seq in enumerate(seq_train_dataloader1):\n",
    "    print(seq)\n",
    "'''\n",
    "seq_valid_dataloader1 = DataLoader(seq_valid_dataset, batch_size=args.batch_size,shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_valid_dataloader2 = GeometricDataLoader(mol_valid_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_valid_dataloader3=DataLoader(smile_valid_dataset, batch_size=args.batch_size,collate_fn=collate,shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "'''\n",
    "for i,m in enumerate(smile_train_dataloader3):\n",
    "    #print('m@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:',m)\n",
    "    break\n",
    "'''\n",
    "seq_mol_smile_valid_multi_loader = MultiDataLoader(seq_valid_dataloader1, mol_valid_dataloader2,smile_valid_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_valid_multi_loader.set_shuffle(True)\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_valid_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "for i ,(seq,mol,smile) in enumerate(seq_mol_smile_train_multi_loader):\n",
    "    print(seq)\n",
    "    print(mol)\n",
    "    print(smile)\n",
    "'''\n",
    "'''\n",
    "print('mol_dataloader:')\n",
    "for mol in mol_train_dataloader2:\n",
    "    print(mol)\n",
    "for seq in seq_train_dataloader1:\n",
    "    print(seq)\n",
    " '''   \n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset+'/test/', dataset=args.dataset)\n",
    "seq_dataset=SeqDataset('dataset/affinity/test/processed/sequence.csv')\n",
    "smiles_dataset=SmileDataset('dataset/affinity/test/processed/smiles.csv')\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_test_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "\n",
    "\n",
    "\n",
    "def train(args, epoch, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    save_pt='results/model2/'\n",
    "    #epoch_iter = tqdm(loader, desc=\"Iteration\")\n",
    "    for step, (A,B,C) in enumerate(loader):\n",
    "        \n",
    "        seq_data_list=[]\n",
    "        seq=A\n",
    "        lenth=len(seq)\n",
    "        for m , s in enumerate(seq):\n",
    "            seq_data_list.append((str(m),s))\n",
    "        B=B.to(device)\n",
    "        D,E=C\n",
    "        D=D.to(device)\n",
    "        E=E.to(device)\n",
    "        C=(D,E)\n",
    "        #print('D!!!!!!!!!!!!!!!!:',D)\n",
    "        #print('E#####################:',E)\n",
    "        #pred=model(seq_data_list,B,C)#model is error\n",
    "        pred=model(seq_data_list,B,C)#model is error\n",
    "        #pred=pred.to(torch.float32)\n",
    "        y_true = B.y.view(pred.shape).to(torch.float32)\n",
    "        #loss = criterion(pred, y_true)\n",
    "        \n",
    "        loss1=criterion(pred,y_true)\n",
    "        #loss2=criterion(u12,u34)\n",
    "        #print('loss1{0},loss2{1}:',loss1,loss2)\n",
    "        #loss=loss1+loss2\n",
    "        loss=loss1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #epoch_iter.set_description(f\"Epoch: {epoch} tloss: {loss:.4f}\")\n",
    "        nn=(epoch+1)//10\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'training Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "            torch.save(model, save_pt+f'full_model_{nn}.pt')\n",
    "    #return loss.item\n",
    "def eval(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            D,E=C\n",
    "            D=D.to(device)\n",
    "            E=E.to(device)##################\n",
    "            C=(D,E)\n",
    "        \n",
    "        \n",
    "            #pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            pred=model(seq_data_list,B,C)#model is error\n",
    "            y_true=B.y.view(pred.shape).to(torch.float32)\n",
    "            val_loss = criterion(pred, y_true)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "\n",
    "    #y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "    #y_scores = torch.cat(y_scores, dim = 0).detach().cpu().numpy()\n",
    "    '''\n",
    "    roc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        #AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:,i] == 1) > 0 and np.sum(y_true[:,i] == -1) > 0:\n",
    "            is_valid = y_true[:,i]**2 > 0\n",
    "            roc_list.append(roc_auc_score((y_true[is_valid,i] + 1)/2, y_scores[is_valid,i]))\n",
    "    if len(roc_list)==0:#########################\n",
    "        return 0\n",
    "    if len(roc_list) < y_true.shape[1]:\n",
    "        print(\"Some target is missing!\")\n",
    "        miss_ratio=(1 - float(len(roc_list))/y_true.shape[1])\n",
    "        print(\"Missing ratio: %f\" %(1 - float(len(roc_list))/y_true.shape[1]))\n",
    "    '''\n",
    "    \n",
    "    return val_loss.item()\n",
    "def test(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            D,E=C\n",
    "            D=D.to(device)\n",
    "            E=E.to(device)##################\n",
    "            C=(D,E)\n",
    "        \n",
    "        \n",
    "            #pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            pred=model(seq_data_list,B,C)#model is error\n",
    "            y_true=B.y.view(pred.shape).to(torch.float32)\n",
    "            test_loss = mse_criterion(pred, y_true)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "\n",
    "    \n",
    "    \n",
    "    return test_loss.item()\n",
    "\n",
    "results_save_file='results/model2/results_save_model1.txt'\n",
    "\n",
    "\n",
    "import torch, gc\n",
    "#criterion = nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "criterion=nn.SmoothL1Loss()\n",
    "mse_criterion=nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "if not args.filename == \"\":\n",
    "    fname = 'runs/seq_mol_finetune_cls_runseed' + str(args.runseed) + '/' + args.filename\n",
    "    #delete the directory if there exists one\n",
    "    if os.path.exists(fname):\n",
    "        shutil.rmtree(fname)\n",
    "        print(\"removed the existing file.\")\n",
    "    writer = SummaryWriter(fname)\n",
    "\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    print(\"====epoch \" + str(epoch))\n",
    "        \n",
    "    train(args, epoch, model, device, seq_mol_smile_train_multi_loader, optimizer)\n",
    "\n",
    "    print(\"====Evaluation\")\n",
    "    if args.eval_train:\n",
    "        train_loss = eval(args, model, device, seq_mol_smile_train_multi_loader)\n",
    "    else:\n",
    "        print(\"omit the training accuracy computation\")\n",
    "        train_loss = 0\n",
    "    val_loss = eval(args, model, device, seq_mol_smile_valid_multi_loader)\n",
    "    test_loss = test(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "    with open(results_save_file, 'w+') as f:\n",
    "        f.write(str(epoch)+'\\t'+str(train_loss)+'\\t'+str(val_loss)+'\\n')\n",
    "        #f.write(epoch)\n",
    "        #f.write('\\t')\n",
    "        #f.write(train_loss)\n",
    "        #f.write('\\t')\n",
    "        #f.write(val_loss)\n",
    "        #f.write('\\n')\n",
    "        \n",
    "    print(\"train: %f val: %f test: %f\" %(train_loss, val_loss, test_loss))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5862b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5261, 0.6541, 0.2496]],\n",
      "\n",
      "        [[0.6983, 0.1007, 0.4210]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.rand((2,1,3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d46065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "y=x.squeeze(2)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a956dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(4).reshape(2,2).float()\n",
    "c=torch.arange(4)\n",
    "print(c)\n",
    "b=nn.Sigmoid()\n",
    "d=b(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eeeac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# input tensor of size (batch_size, num_features)\n",
    "input_tensor = torch.randn(64, 128)\n",
    "print(input_tensor.shape)\n",
    "# apply BatchNorm1d\n",
    "bn = nn.BatchNorm1d(128)\n",
    "output_tensor = bn(input_tensor)\n",
    "print(output_tensor.shape) # (64, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd0171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
