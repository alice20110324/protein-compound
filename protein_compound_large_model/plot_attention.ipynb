{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb96ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Not from scratch\n",
      "rese:model_gin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Pre-processed data found: /media/ext_disk/zhenfang/dataset/davis/processed/davis_train_mols.pt, loading ...\n",
      "seq_train_dataset: <class 'pandas.core.series.Series'>\n",
      "seq_train_dataset: <class 'loader.SeqDataset'>\n",
      "scaffold\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndef test(args, model, device, loader):\\n    for data, target in val_loader:\\n                output = model(data)\\n                val_loss += criterion(output, target).item()\\n                pred = output.argmax(dim=1, keepdim=True)\\n                correct += pred.eq(target.view_as(pred)).sum().item()\\n    print(f'Fold {fold}, Training Loss: {train_loss}, Validation Loss: {val_loss}, Accuracy: {100. * correct / len(val_loader.dataset)}%')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "from loader1 import  TestbedDataset,MoleculeDatasetBig, SeqDataset,SeqMolDataset,SmileDataset#########################\n",
    "import torch\n",
    "import torch\n",
    "#import args\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "#from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from splitters import scaffold_split,scaffold_split_1\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "#import esm2_t33_650M_UR50D\n",
    "import esm\n",
    "\n",
    "#from SeqMolModel import InteractionModel,InteractionModel_1,SequenceModel,InteractionModel_4\n",
    "#from SeqMolSmile import InteractionModel_4\n",
    "#from SeqMolModel import InteractionModel_4\n",
    "from SeqMolSmile_model1_2_layer import InteractionModel_4,InteractionModel_5\n",
    "#from SeqMolSmile_without_cross import InteractionModel_4\n",
    "print(torch.cuda.is_available())\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda._initialized = True\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int, default=1,\n",
    "                        help='which gpu to use if any (default: 0)')#0000\n",
    "parser.add_argument('--batch_size', type=int, default=10,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=101,\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "parser.add_argument('--decay', type=float, default=0,\n",
    "                        help='weight decay (default: 0)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "parser.add_argument('--dataset', type=str, default = 'davis', help='root directory of dataset. For now, only classification.')\n",
    "#parser.add_argument('--input_model_file', type=str, default = 'None', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--input_model_file', type=str, default = 'Mole-BERT', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--filename', type=str, default = '', help='output filename')\n",
    "parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "parser.add_argument('--runseed', type=int, default=0, help = \"Seed for minibatch selection, random initialization.\")\n",
    "parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "parser.add_argument('--eval_train', type=int, default = 1, help='evaluating training or not')\n",
    "parser.add_argument('--num_workers', type=int, default = 4, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default = 12, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_layer', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--d_dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--n_embd', type=int, default = 768, help='number of workers for dataset loading')\n",
    "parser.add_argument('--dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--lr_start', type=float, default =  3e-5, help='number of workers for dataset loading')\n",
    "parser.add_argument('--max_epochs', type=int, default = 500, help='number of workers for dataset loading')\n",
    "parser.add_argument('--num_feats', type=int, default = 32, help='number of workers for dataset loading')\n",
    "parser.add_argument('--checkpoint_every', type=int, default = 100, help='number of workers for dataset loading')\n",
    "parser.add_argument('--seed_path', type=str, default =  'data/checkpoints/N-Step-Checkpoint_3_30000.ckpt', help='number of workers for dataset loading')\n",
    "parser.add_argument('--dims', type=list, default = [ 768, 768, 768, 1], help='number of workers for dataset loading')\n",
    "\n",
    "args = parser.parse_args(args=[])###############33\n",
    "\n",
    "import torchvision.models as models\n",
    "#load pretained model of Mole-Bert\n",
    "\n",
    "#model_path='esm2_t33_650M_UR50D.pt'\n",
    "##esm.load_state_dict(torch.load('esm2_t33_650M_UR50D.pt'))\n",
    "#model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "##model_path = r\"esm2_t33_650M_UR50D.pt\"\n",
    "#model,alphabet=esm.pretrained(model_location=model_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.runseed)\n",
    "\n",
    "num_tasks=1\n",
    "# Load ESM-2 model\n",
    "\n",
    "#protein_model, protein_alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "protein_model, protein_alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "protein_model.to(device)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "#print(protein_alphabet)\n",
    "#alphabet = esm.Alphabet.from_architecture(model_data[\"args\"].arch)\n",
    "#batch_converter = alphabet.get_batch_converter()\n",
    "protein_model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "#self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "molecular_model = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "###################################\n",
    "if not args.input_model_file == \"None\":###############\n",
    "    print('Not from scratch')\n",
    "    molecular_model.from_pretrained('model_gin/{}.pth'.format(args.input_model_file))\n",
    "    print('rese:model_gin')\n",
    "molecular_model.to(device)\n",
    "for i,p in enumerate(molecular_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "\n",
    "'''\n",
    "model_param_group = []\n",
    "model_param_group.append({\"params\": molecular_model.gnn.parameters()})\n",
    "if args.graph_pooling == \"attention\":\n",
    "    model_param_group.append({\"params\": molecular_model.pool.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "\n",
    "\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「Dreamcatcher风」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/Wind_2028/article/details/120541017   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_param_group.append({\"params\": molecular_model.graph_pred_linear.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "optimizer = optim.Adam(model_param_group, lr=0.01, weight_decay=args.decay)#############\n",
    "'''\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
    "#from finetune.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "#from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
    "from functools import partial\n",
    "from apex import optimizers\n",
    "import subprocess\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "#from utils import normalize_smiles\n",
    "import sys\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "from tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "#from utils import normalize_smiles\n",
    "# create a function (this my favorite choice)\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(LightningModule, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        #self.hparams = config\n",
    "        #self.mode = config.mode\n",
    "        self.save_hyperparameters(config)\n",
    "        self.tokenizer=tokenizer\n",
    "        '''\n",
    "        self.min_loss = {\n",
    "            self.hparams.measure_name + \"min_valid_loss\": torch.finfo(torch.float32).max,\n",
    "            self.hparams.measure_name + \"min_epoch\": 0,\n",
    "        }\n",
    "        '''\n",
    "        # Word embeddings layer\n",
    "        n_vocab, d_emb = len(tokenizer.vocab), config.n_embd\n",
    "        # input embedding stem\n",
    "        \n",
    "        builder = rotate_builder.from_kwargs(\n",
    "            n_layers=config.n_layer,\n",
    "            n_heads=config.n_head,\n",
    "            query_dimensions=config.n_embd//config.n_head,\n",
    "            value_dimensions=config.n_embd//config.n_head,\n",
    "            feed_forward_dimensions=config.n_embd,\n",
    "            attention_type='linear',\n",
    "            feature_map=partial(GeneralizedRandomFeatures, n_dims=config.num_feats),\n",
    "            activation='gelu',\n",
    "            )\n",
    "        self.pos_emb = None\n",
    "        self.tok_emb = nn.Embedding(n_vocab, config.n_embd)\n",
    "        #print('self.tok_emb:',self.tok_emb)\n",
    "        self.drop = nn.Dropout(config.d_dropout)\n",
    "        \n",
    "        ## transformer\n",
    "        self.blocks = builder.get()\n",
    "        #self.lang_model = self.lm_layer(config.n_embd, n_vocab)\n",
    "        #self.train_config = config\n",
    "        #if we are starting from scratch set seeds\n",
    "        #########################################\n",
    "        # protein_emb_dim, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "        #########################################\n",
    "        '''\n",
    "        self.fcs = []  \n",
    "        self.loss = torch.nn.L1Loss()\n",
    "        self.net = self.Net(\n",
    "            config.n_embd, dims=config.dims, dropout=config.dropout,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        dims = [150, 50, 50, 2]\n",
    "\n",
    "\n",
    "        def __init__(self, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.desc_skip_connection = True \n",
    "            self.fcs = []  # nn.ModuleList()\n",
    "            #print('dropout is {}'.format(dropout))\n",
    "\n",
    "            self.fc1 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.relu1 = nn.GELU()\n",
    "            self.fc2 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.relu2 = nn.GELU()\n",
    "            self.final = nn.Linear(smiles_embed_dim, 1)\n",
    "\n",
    "        def forward(self, smiles_emb):\n",
    "            x_out = self.fc1(smiles_emb)\n",
    "            x_out = self.dropout1(x_out)\n",
    "            x_out = self.relu1(x_out)\n",
    "\n",
    "            if self.desc_skip_connection is True:\n",
    "                x_out = x_out + smiles_emb\n",
    "\n",
    "            z = self.fc2(x_out)\n",
    "            z = self.dropout2(z)\n",
    "            z = self.relu2(z)\n",
    "            if self.desc_skip_connection is True:\n",
    "                z = self.final(z + x_out)\n",
    "            else:\n",
    "                z = self.final(z)\n",
    "\n",
    "            return z\n",
    "\n",
    "    class lm_layer(nn.Module):\n",
    "        def __init__(self, n_embd, n_vocab):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Linear(n_embd, n_embd)\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.head = nn.Linear(n_embd, n_vocab, bias=False)\n",
    "        def forward(self, tensor):\n",
    "            tensor = self.embed(tensor)\n",
    "            tensor = F.gelu(tensor)\n",
    "            tensor = self.ln_f(tensor)\n",
    "            tensor = self.head(tensor)\n",
    "            return tensor\n",
    "\n",
    "    def get_loss(self, smiles_emb, measures):\n",
    "\n",
    "        z_pred = self.net.forward(smiles_emb).squeeze()\n",
    "        measures = measures.float()\n",
    "\n",
    "        return self.loss(z_pred, measures), z_pred, measures\n",
    "    \n",
    "    \n",
    "margs = args\n",
    "tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "seed.seed_everything(margs.seed)\n",
    "if margs.seed_path == '':\n",
    "    #print(\"# training from scratch\")\n",
    "    smile_model = LightningModule(margs, tokenizer)\n",
    "else:\n",
    "    #print(\"# loaded pre-trained model from {args.seed_path}\")\n",
    "    smile_model = LightningModule(margs, tokenizer).load_from_checkpoint(margs.seed_path, strict=False, config=margs, tokenizer=tokenizer, vocab=len(tokenizer.vocab))#########################33\n",
    "\n",
    "#print('model:',smile_model)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(smile_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "    \n",
    "\n",
    "#num_tasks=1\n",
    "model= InteractionModel_5(protein_model=protein_model,molecular_model=molecular_model,smile_model=smile_model,protein_embd_dim=320,num_tasks=1,device=device,mol_embd_dim=300,smile_embd_dim=768) \n",
    "model.to(device)\n",
    "#print(model)#nice#num_tasks=1\n",
    "\n",
    "\n",
    "###################when doing geometric.data process, if there is some changes, please delete the directory process and let it generate again\n",
    "\n",
    "gnn_dataset =  TestbedDataset(root=\"/media/ext_disk/zhenfang/dataset/\" + args.dataset, dataset=args.dataset)###########################转换成了分子图的格式\n",
    "#print('args.dataset:',args.dataset)\n",
    "#print(gnn_dataset)\n",
    "'''\n",
    "for i,gnn_data in enumerate(gnn_dataset):\n",
    "    print(i,gnn_data)\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "def list_files(directory,s):##########\n",
    "    for filename in os.listdir(directory):\n",
    "        path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(path) and path.endswith(s):\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "directory_path = '/media/ext_disk/zhenfang/dataset/davis/processed/'\n",
    "seq_file=list_files(directory_path,'sequence.csv')\n",
    "if seq_file!=None:\n",
    "    seq_dataset=SeqDataset(seq_file)\n",
    "'''\n",
    "for i,seq_data in enumerate(seq_dataset):\n",
    "    print(i,seq_data)\n",
    "'''\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#seq_dataset[2]\n",
    "\n",
    "def collate(batch):\n",
    "    #print('collate_batch:',batch)\n",
    "    #print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    #print('tokens[1]_mask:',tokens[1])\n",
    "    #print('colate###########################')\n",
    "    #for i,m in enumerate(tokens):\n",
    "            \n",
    "    #print('collate_tokens_input_ids:',tokens['input_ids'])\n",
    "    #print('colate_tokens_attention_mask:',tokens['attention_mask'])\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "\n",
    "\n",
    "smile_file=list_files(directory_path,'smiles.csv')\n",
    "if smile_file!=None:\n",
    "    smiles_dataset=SmileDataset(smile_file)\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#####test with chartGPT  DataSet extends two parents' classes\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MultiDatasetMixin:\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.dataset3=dataset3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset1), len(self.dataset2),len(self.dataset3))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dataset1[idx], self.dataset2[idx],self.dataset3[idx]\n",
    "\n",
    "class CustomMultiDataset(MultiDatasetMixin, Dataset):###############3extends two classes\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        MultiDatasetMixin.__init__(self, dataset1, dataset2,dataset3)\n",
    "\n",
    "\n",
    "#chartGPT太厉害了，杀了我吧\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
    "\n",
    "class CustomMultiDataLoader:\n",
    "    def __init__(self, customDataset,batch_size,num_workers,collate,shuffle=False):\n",
    "        self.customDataset=customDataset\n",
    "        self.dataset1=self.customDataset.dataset1\n",
    "        self.dataset2=self.customDataset.dataset2\n",
    "        self.dataset3=self.customDataset.dataset3\n",
    "        self.batch_size=batch_size\n",
    "        self.num_workers=num_workers\n",
    "        self.collate=collate\n",
    "        self.shuffle=shuffle\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dataloader1=DataLoader(dataset=self.dataset1, batch_size=batch_size, num_workers=num_workers,shuffle=shuffle)\n",
    "        self.dataloader2=GeometricDataLoader(dataset=self.dataset2, batch_size=batch_size, num_workers=num_workers,shuffle=shuffle)\n",
    "        self.dataloader3=DataLoader(dataset=self.dataset3, batch_size=batch_size,num_workers=num_workers,collate_fn=collate,shuffle=shuffle)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data1, data2,data3 in zip(self.dataloader1, self.dataloader2, self.dataloader3):\n",
    "            yield data1, data2, data3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader1), len(self.dataloader2), len(self.dataloader3))\n",
    "\n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.dataloader1.shuffle = shuffle\n",
    "        self.dataloader2.shuffle = shuffle\n",
    "        self.dataloader3.shuffle=shuffle\n",
    "\n",
    "\n",
    "class MultiDataLoader:\n",
    "    def __init__(self, dataloader1, dataloader2,dataloader3):\n",
    "        self.dataloader1 = dataloader1\n",
    "        self.dataloader2 = dataloader2\n",
    "        self.dataloader3=dataloader3\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data1, data2,data3 in zip(self.dataloader1, self.dataloader2, self.dataloader3):\n",
    "            yield data1, data2, data3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader1), len(self.dataloader2), len(self.dataloader3))\n",
    "\n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.dataloader1.shuffle = shuffle\n",
    "        self.dataloader2.shuffle = shuffle\n",
    "        self.dataloader3.shuffle=shuffle\n",
    "\n",
    "        \n",
    "###########split train dataset validate dataset test dataset\n",
    "seq_gnn_smile_dataset=MultiDatasetMixin(seq_dataset,gnn_dataset,smiles_dataset)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "for i , seq_gnn_smile in enumerate(seq_gnn_smile_dataset):\n",
    "    print(i)\n",
    "'''\n",
    "#print('seq_dataset:',seq_dataset)\n",
    "\n",
    "#seq_dataloader=DataLoader(seq_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.num_workers)\n",
    "'''\n",
    "for i ,seq in enumerate(seq_dataloader):\n",
    "    print(seq)\n",
    "'''\n",
    "if args.split == \"scaffold\":\n",
    "        smiles_list = pd.read_csv(smile_file, header=None)[0].tolist()\n",
    "        \n",
    "        #print('smiles_list:',smiles_list)\n",
    "        mol_train_dataset, mol_valid_dataset, mol_test_dataset ,seq_train_dataset,seq_valid_dataset,seq_test_dataset,smile_train_dataset,smile_valid_dataset,smile_test_dataset= scaffold_split_1(seq_gnn_smile_dataset, smiles_list, null_value=0, frac_train=0.1,frac_valid=0.1, frac_test=0.8)##########dataset\n",
    "        print(\"scaffold\")\n",
    "elif args.split == \"random\":\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.1,frac_valid=0.1, frac_test=0.8, seed = args.seed)\n",
    "        #print(\"random\")\n",
    "elif args.split == \"random_scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random scaffold\")\n",
    "else:\n",
    "        raise ValueError(\"Invalid split option.\")\n",
    "\n",
    "#print('++++++++++', mol_train_dataset[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "#seq_mol_train_dataset=MultiDatasetMixini(seq_train_dataset,mol_train_dataset)\n",
    "#seq_mol_valid_dataset=SeqMolDataset(seq_valid_dataset,mol_valid_dataset)\n",
    "#seq_mol_test_dataset=SeqMolDataset(seq_train_dataset,mol_test_dataset)\n",
    "seq_train_dataloader1 = DataLoader(seq_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_train_dataloader2 = GeometricDataLoader(mol_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_train_dataloader3=DataLoader(smile_train_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "seq_mol_smile_train_multi_loader = MultiDataLoader(seq_train_dataloader1, mol_train_dataloader2,smile_train_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_train_multi_loader.set_shuffle(True)\n",
    "\n",
    "seq_valid_dataloader1 = DataLoader(seq_valid_dataset, batch_size=args.batch_size,shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_valid_dataloader2 = GeometricDataLoader(mol_valid_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_valid_dataloader3=DataLoader(smile_valid_dataset, batch_size=args.batch_size,collate_fn=collate,shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_valid_multi_loader = MultiDataLoader(seq_valid_dataloader1, mol_valid_dataloader2,smile_valid_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_valid_multi_loader.set_shuffle(True)\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_valid_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "\n",
    "seq_mol_smile_train_dataset=CustomMultiDataset(seq_train_dataset,mol_train_dataset,smile_train_dataset)\n",
    "seq_mol_smile_test_dataset=CustomMultiDataset(seq_test_dataset,mol_test_dataset,smile_test_dataset)\n",
    "from torch.utils.data import  Subset\n",
    "#from torch_geometric.data import Subset as GeoSubset\n",
    "def subsetDataset(dataset,idx):\n",
    "    d1=dataset.dataset1\n",
    "    d2=dataset.dataset2\n",
    "    d3=dataset.dataset3\n",
    "    \n",
    "    sub1=Subset(d1,idx)\n",
    "    sub2=Subset(d2,idx)\n",
    "    sub3=Subset(d3,idx)\n",
    "    return CustomMultiDataset(d1, d2, d3)\n",
    "    \n",
    "criterion=nn.SmoothL1Loss()\n",
    "mse_criterion=nn.MSELoss()\n",
    "save_pt='/media/ext_disk/zhenfang/davis/results/model1/cross/'\n",
    "results_save_file='/media/ext_disk/zhenfang/davis/results/model1/cross/results.txt'\n",
    "def train(args, epoch, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    #save_pt='results/davis/model1/'\n",
    "    #epoch_iter = tqdm(loader, desc=\"Iteration\")\n",
    "    loss=0\n",
    "    train_loss_list=[]\n",
    "    for step, (A,B,C) in enumerate(loader):\n",
    "        \n",
    "        seq_data_list=[]\n",
    "        seq=A\n",
    "        lenth=len(seq)\n",
    "        for m , s in enumerate(seq):\n",
    "            seq_data_list.append((str(m),s))\n",
    "        B=B.to(device)\n",
    "        D,E=C\n",
    "        D=D.to(device)\n",
    "        E=E.to(device)\n",
    "        C=(D,E)\n",
    "        #print('A!!!!!!!!!!!!!!!!:',seq_data_list)\n",
    "        #print('B!!!!!!!!!!!!!!!!:',B)\n",
    "        #print('D!!!!!!!!!!!!!!!!:',D)\n",
    "        #print('E#####################:',E)\n",
    "        pred=model(seq_data_list,B,C)#model is error\n",
    "        #pred=model(seq_data_list,B,C)#model is error\n",
    "        #pred=pred.to(torch.float32)\n",
    "        #print('pred value:',pred)\n",
    "        y_true = B.y.view(pred.shape).to(torch.float32)\n",
    "        #print('y_true value:',y_true)\n",
    "        #loss = criterion(pred, y_true)\n",
    "        \n",
    "        #loss1=criterion(pred,y_true)\n",
    "        loss1=mse_criterion(pred,y_true)\n",
    "        #loss2=criterion(u12,u34)\n",
    "        #print('loss1{0},loss2{1}:',loss1,loss2)\n",
    "        #loss=loss1+loss2\n",
    "        loss=loss1\n",
    "        #print('epoch, train_loss',epoch,loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #epoch_iter.set_description(f\"Epoch: {epoch} tloss: {loss:.4f}\")\n",
    "        #nn=(epoch+1)//10\n",
    "        #print(f'training Epoch [{epoch+1}], Loss: {loss.item():.4f}')\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'training Epoch: {epoch+1}, train_Loss: {loss.item():.4f}')\n",
    "            #torch.save(model, save_pt+f'full_model_{0}.pt'.format(epoch))\n",
    "        train_loss_cpu=loss.detach().cpu().numpy()\n",
    "        train_loss_list.append(train_loss_cpu)\n",
    "    mean_loss=np.mean(train_loss_list)\n",
    "    #print('epoch, train_loss',epoch+1, mean_loss)\n",
    "    return mean_loss\n",
    "def test(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    #val_loss=0\n",
    "    val_loss_list=[]\n",
    "    loss=0\n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            D,E=C\n",
    "            D=D.to(device)\n",
    "            E=E.to(device)##################\n",
    "            C=(D,E)\n",
    "            \n",
    "            pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            y_true=B.y.view(pred.shape).to(torch.float32)\n",
    "            val_loss = mse_criterion(pred, y_true)\n",
    "            val_loss_cpu=val_loss.cpu().numpy()##############\n",
    "            #print('val_loss_cpu:',val_loss_cpu)\n",
    "            val_loss_list.append(val_loss_cpu)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "    loss=np.mean(val_loss_list)\n",
    "    \n",
    "    return loss\n",
    "'''\n",
    "def test(args, model, device, loader):\n",
    "    for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print(f'Fold {fold}, Training Loss: {train_loss}, Validation Loss: {val_loss}, Accuracy: {100. * correct / len(val_loader.dataset)}%')\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e933f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t torch.Size([1])\n",
      "mole_linear.weight torch.Size([256, 300])\n",
      "mole_linear.bias torch.Size([256])\n",
      "mol_pred_linear.weight torch.Size([1, 620])\n",
      "mol_pred_linear.bias torch.Size([1])\n",
      "protein_linear.weight torch.Size([256, 320])\n",
      "protein_linear.bias torch.Size([256])\n",
      "mol_cross_attention.linear1.weight torch.Size([256, 256])\n",
      "mol_cross_attention.linear1.bias torch.Size([256])\n",
      "mol_cross_attention.linear2.weight torch.Size([256, 256])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "mol_cross_attention.linear2.bias torch.Size([256])\n",
      "fc1.weight torch.Size([256, 512])\n",
      "fc1.bias torch.Size([256])\n",
      "fc2.weight torch.Size([1, 256])\n",
      "fc2.bias torch.Size([1])\n",
      "smile_linear.weight torch.Size([256, 768])\n",
      "smile_linear.bias torch.Size([256])\n",
      "smile_pred_linear.weight torch.Size([1, 1088])\n",
      "smile_pred_linear.bias torch.Size([1])\n",
      "smile_cross_attention.linear1.weight torch.Size([256, 256])\n",
      "smile_cross_attention.linear1.bias torch.Size([256])\n",
      "smile_cross_attention.linear2.weight torch.Size([256, 256])\n",
      "smile_cross_attention.linear2.bias torch.Size([256])\n",
      "fc3.weight torch.Size([256, 512])\n",
      "fc3.bias torch.Size([256])\n",
      "layer_norm_seq.weight torch.Size([256])\n",
      "layer_norm_seq.bias torch.Size([256])\n",
      "layer_norm_mol.weight torch.Size([256])\n",
      "layer_norm_mol.bias torch.Size([256])\n",
      "layer_norm_smile.weight torch.Size([256])\n",
      "layer_norm_smile.bias torch.Size([256])\n",
      "fc11.weight torch.Size([256, 768])\n",
      "fc11.bias torch.Size([256])\n",
      "t_linear1.weight torch.Size([50, 1])\n",
      "t_linear1.bias torch.Size([50])\n",
      "t_linear2.weight torch.Size([1, 50])\n",
      "t_linear2.bias torch.Size([1])\n",
      "Parameter containing:\n",
      "tensor([[-0.0564,  0.0358,  0.0203,  ..., -0.0339, -0.0376,  0.0131],\n",
      "        [-0.0217, -0.0016, -0.0049,  ..., -0.0157,  0.0219, -0.0115],\n",
      "        [-0.0400, -0.0016,  0.0088,  ..., -0.0034,  0.0267, -0.0062],\n",
      "        ...,\n",
      "        [ 0.0429, -0.0204,  0.0108,  ...,  0.0355, -0.0215,  0.0355],\n",
      "        [-0.0134, -0.0557, -0.0379,  ...,  0.0414, -0.0571, -0.0531],\n",
      "        [-0.0469, -0.0208,  0.0496,  ..., -0.0306, -0.0039,  0.0321]],\n",
      "       device='cuda:1', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    " \n",
    "# 假设你有一个保存好的模型文件'model.pth'\n",
    "model_path = '/media/ext_disk/zhenfang/davis/results/model1/cross/full_model_4_100.pt'\n",
    " \n",
    "# 定义你的模型，例如一个简单的线性模型\n",
    "\n",
    " \n",
    "# 实例化模型\n",
    "model = model\n",
    " \n",
    "# 加载模型参数\n",
    "#checkpoint = torch.load(model_path)\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    " \n",
    "# 打印模型名称和参数\n",
    "#print(\"Model Name:\", model.__class__.__name__)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.size())\n",
    "        if name=='mol_cross_attention.linear2.weight':\n",
    "            mol_cross_attention=param\n",
    "            print(type(param))\n",
    "            torch.save(mol_cross_attention, 'mol_cross_attention.pt')\n",
    "        if name=='smile_cross_attention.linear2.weight':\n",
    "            smile_cross_attention=param\n",
    "            torch.save(smile_cross_attention,'smile_cross_attention.pt')\n",
    "            \n",
    "            \n",
    "            \n",
    "mol_cross_attention=torch.load('mol_cross_attention.pt')\n",
    "print(mol_cross_attention)\n",
    "smile_cross_attention=torch.load('smile_cross_attention.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8e02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# 假设我们有一个CxHxW的特征图\n",
    "#features = torch.randn(1, 20, 20)  # 示例特征图: 1个通道，20x20\n",
    "features= mol_cross_attention.reshape(1,256,256) \n",
    "#features= mol_cross_attention\n",
    "#features=features.cpu().detach().numpy()\n",
    "features=features[0,:10,:10]\n",
    "# 定义一个将特征图转换为热力图的函数\n",
    "def make_heatmap(heatmap):\n",
    "    # 使用torch.softmax对特征图的每个通道进行归一化\n",
    "    heatmap = nn.functional.softmax(heatmap, dim=0)\n",
    "    heatmap=heatmap.cpu().detach().numpy()\n",
    "    # 将归一化后的热力图转换为RGB图像\n",
    "    #heatmap = heatmap.cpu().numpy()[0]  # 取出单个样本的热力图\n",
    "    cmap = plt.get_cmap('jet')  # 选择一个颜色映射\n",
    "    cmap_bg = cmap(0)  # 颜色映射的背景颜色\n",
    "    heatmap = cmap(heatmap * 255)  # 对热力图进行颜色编码\n",
    "    #heatmap = np.delete(heatmap, 3, 2)  # 删除alpha通道\n",
    "    # 将背景颜色添加回热力图\n",
    "    #heatmap_on_bg = np.where(heatmap[:, :, 0] == 0, cmap_bg[0] * 255, heatmap)\n",
    "    #return heatmap_on_bg\n",
    "    return heatmap\n",
    " \n",
    "# 生成热力图\n",
    "heatmap = make_heatmap(features)\n",
    "#features=features.cpu().detach().numpy()\n",
    "# 显示热力图\n",
    "plt.imshow(heatmap.astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06592de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# 创建一个随机的NumPy数组\n",
    "features= mol_cross_attention.reshape(256,256) \n",
    "features=features.cpu().detach().numpy()\n",
    "features=features[:20,:20]\n",
    "#array = np.random.rand(10, 10)  # 生成一个10x10的随机数组，数值在0到1之间\n",
    " \n",
    "# 使用imshow显示热力图\n",
    "plt.imshow(features, cmap='hot', interpolation='nearest')  # cmap参数设置为'hot'以显示传统的红-黄-蓝热图\n",
    "plt.colorbar()  # 显示颜色条以便了解各种颜色代表的数值范围\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "294628ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.641731    3.5794153   2.0313368   5.425617   -2.6484787  -3.4234815\n",
      "   3.4488618  -5.783211    4.756242   -1.942011  ]\n",
      " [-2.169089   -0.15924498 -0.49203187  0.22038892 -4.445072    1.8724792\n",
      "  -3.3621125  -2.4909153  -2.8466873  -2.2052414 ]\n",
      " [-3.9952345  -0.16011372  0.88298094  5.682206    1.7749727   5.456613\n",
      "   2.0359018  -4.44639     4.7330513  -4.7677054 ]\n",
      " [-2.1023378  -5.5037565   1.851146   -1.229836   -3.0897386  -3.8039474\n",
      "  -0.34277737  1.1032588   2.0837307   5.563301  ]\n",
      " [ 0.49641207  1.9577973  -0.44841766 -2.4065218  -3.4734502   4.5444307\n",
      "   3.0882068  -3.6129026   2.8123827  -5.566804  ]\n",
      " [ 0.72535205 -6.110815    0.36961362  5.7141566  -0.5703367  -3.2498546\n",
      "   2.1696157  -0.99867284 -5.947285   -2.5525444 ]\n",
      " [ 1.4891363   2.367709   -4.27636    -2.7943194  -5.6919007   0.7268913\n",
      "   5.1269226  -2.5215774  -1.0651231   1.3774514 ]\n",
      " [ 4.1979985  -4.9134536  -4.974879   -2.892279    1.0983117   2.4400406\n",
      "   0.32061934 -5.86468    -1.6656809  -2.4440975 ]\n",
      " [ 1.2948364  -4.867787    2.2484832  -1.8577851  -0.93133974 -5.1277413\n",
      "   3.1609654   2.24384    -3.0840635  -0.76610595]\n",
      " [-4.1382084   0.241974   -1.487773    3.4689255   0.7447101  -0.72401464\n",
      "  -2.6558578  -6.0988264   0.8174896   5.2432075 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# 创建一个注意力矩阵\n",
    "features= mol_cross_attention.reshape(256,256) \n",
    "features=features.cpu().detach().numpy()\n",
    "features=features[:10,:10]*100\n",
    "print(features)\n",
    "#attention_matrix = np.random.rand(8, 8)  # 示例中使用随机数据\n",
    " \n",
    "# 使用seaborn的heatmap函数绘制热图\n",
    "sns.heatmap(features, cmap='Blues', annot=True, fmt='.2f')\n",
    " \n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe60ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "243cb12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# 创建一个注意力矩阵\n",
    "attention_matrix = np.random.rand(8, 8)  # 示例中使用随机数据\n",
    " \n",
    "# 使用seaborn的heatmap函数绘制热图\n",
    "sns.heatmap(attention_matrix, cmap='Blues', annot=True, fmt='.2f')\n",
    " \n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dbbe5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "  \n",
    "#练习的数据：\n",
    "data=np.arange(25).reshape(5,5)\n",
    "data=pd.DataFrame(data)\n",
    "  \n",
    "#绘制热度图：\n",
    "plot=sns.heatmap(data)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f003100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.random.rand(10, 10)\n",
    "\n",
    "sns.heatmap(data, cmap='YlGnBu', annot=True)\n",
    "plt.title('Heatmap Example') # 设置图形标题\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61e83793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "harvest = np.array([[0.8, 2.4, 2.5, 3.9, 0.0, 4.0, 0.0],\n",
    "                    [2.4, 0.0, 4.0, 1.0, 2.7, 0.0, 0.0],\n",
    "                    [1.1, 2.4, 0.8, 4.3, 1.9, 4.4, 0.0],\n",
    "                    [0.6, 0.0, 0.3, 0.0, 3.1, 0.0, 0.0],\n",
    "                    [0.7, 1.7, 0.6, 2.6, 2.2, 6.2, 0.0],\n",
    "                    [1.3, 1.2, 0.0, 0.0, 0.0, 3.2, 5.1],\n",
    "                    [0.1, 2.0, 0.0, 1.4, 0.0, 1.9, 6.3]])\n",
    "\n",
    "plt.imshow(harvest)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#作者：黑马程序员Python\n",
    "#链接：https://www.zhihu.com/question/362018090/answer/2811456173\n",
    "#来源：知乎\n",
    "#著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eec4b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# 创建一些示例数据\n",
    "data = np.random.rand(10, 10)\n",
    " \n",
    "# 绘制热力图\n",
    "sns.heatmap(data, cmap='viridis', annot=True)\n",
    " \n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd40a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 生成随机数据\n",
    "data = np.random.rand(10, 10)\n",
    "\n",
    "# 绘制热力图\n",
    "plt.pcolormesh(data, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e714ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
