{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9406c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bace\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/bace/raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2244\u001b[0m\n\u001b[1;32m   2241\u001b[0m \u001b[38;5;66;03m# test MoleculeDataset object\u001b[39;00m\n\u001b[1;32m   2242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2244\u001b[0m     \u001b[43mcreate_all_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 2231\u001b[0m, in \u001b[0;36mcreate_all_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2229\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset_name\n\u001b[1;32m   2230\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/processed\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2231\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMoleculeDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataset)\n\u001b[1;32m   2235\u001b[0m dataset \u001b[38;5;241m=\u001b[39m MoleculeDataset(root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/chembl_filtered\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchembl_filtered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 401\u001b[0m, in \u001b[0;36mMoleculeDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, dataset, empty)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m root\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMoleculeDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_filter \u001b[38;5;241m=\u001b[39m transform, pre_transform, pre_filter\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m empty:\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:57\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     51\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     log: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m ):\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch_geometric/data/dataset.py:94\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices: Optional[Sequence] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_download:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process()\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch_geometric/data/dataset.py:195\u001b[0m, in \u001b[0;36mDataset._download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m files_exist(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_paths\u001b[49m):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     makedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/wuyou_pytorch/lib/python3.8/site-packages/torch_geometric/data/dataset.py:171\u001b[0m, in \u001b[0;36mDataset.raw_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_paths\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The absolute filepaths that must be present in order to skip\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    downloading.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_file_names\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Prevent a common source of error in which `file_names` are not\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# defined as a property.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, Callable):\n",
      "Cell \u001b[0;32mIn[2], line 441\u001b[0m, in \u001b[0;36mMoleculeDataset.raw_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_file_names\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 441\u001b[0m     file_name_list \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# assert len(file_name_list) == 1     # currently assume we have a\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# # single raw file\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file_name_list\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/bace/raw'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import AllChem#################\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect\n",
    "from torch.utils import data\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Batch\n",
    "from itertools import repeat, product, chain\n",
    "\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "# allowable node and edge features\n",
    "allowable_features = {\n",
    "    'possible_atomic_num_list' : list(range(1, 119)),\n",
    "    'possible_formal_charge_list' : [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5],########原子价电子\n",
    "    'possible_chirality_list' : [#手性\n",
    "        Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "        Chem.rdchem.ChiralType.CHI_OTHER\n",
    "    ],\n",
    "    'possible_hybridization_list' : [#杂交反应\n",
    "        Chem.rdchem.HybridizationType.S,\n",
    "        Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2, Chem.rdchem.HybridizationType.UNSPECIFIED\n",
    "    ],\n",
    "    'possible_numH_list' : [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'possible_implicit_valence_list' : [0, 1, 2, 3, 4, 5, 6],\n",
    "    'possible_degree_list' : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'possible_bonds' : [\n",
    "        Chem.rdchem.BondType.SINGLE,\n",
    "        Chem.rdchem.BondType.DOUBLE,\n",
    "        Chem.rdchem.BondType.TRIPLE,\n",
    "        Chem.rdchem.BondType.AROMATIC#芳香\n",
    "    ],\n",
    "    'possible_bond_dirs' : [ # only for double bond stereo information\n",
    "        Chem.rdchem.BondDir.NONE,\n",
    "        Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "        Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "    ]\n",
    "}\n",
    "\n",
    "def mol_to_graph_data_obj_simple(mol):\n",
    "    \"\"\"\n",
    "    Converts rdkit mol object to graph Data object required by the pytorch\n",
    "    geometric package. NB: Uses simplified atom and bond features, and represent\n",
    "    as indices\n",
    "    :param mol: rdkit mol object\n",
    "    :return: graph data object with the attributes: x, edge_index, edge_attr\n",
    "    \"\"\"\n",
    "    # atoms\n",
    "    num_atom_features = 2   # atom type,  chirality tag\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():#####################\n",
    "        atom_feature = [allowable_features['possible_atomic_num_list'].index(\n",
    "            atom.GetAtomicNum())] + [allowable_features[\n",
    "            'possible_chirality_list'].index(atom.GetChiralTag())]\n",
    "        atom_features_list.append(atom_feature)\n",
    "    x = torch.tensor(np.array(atom_features_list), dtype=torch.long)\n",
    "    #print('atom attributes')\n",
    "    # bonds\n",
    "    num_bond_features = 2   # bond type, bond direction\n",
    "    if len(mol.GetBonds()) > 0: # mol has bonds\n",
    "        edges_list = []\n",
    "        edge_features_list = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_feature = [allowable_features['possible_bonds'].index(\n",
    "                bond.GetBondType())] + [allowable_features[\n",
    "                                            'possible_bond_dirs'].index(\n",
    "                bond.GetBondDir())]\n",
    "            edges_list.append((i, j))\n",
    "            edge_features_list.append(edge_feature)\n",
    "            edges_list.append((j, i))\n",
    "            edge_features_list.append(edge_feature)\n",
    "\n",
    "        # data.edge_index: Graph connectivity in COO format with shape [2, num_edges]\n",
    "        edge_index = torch.tensor(np.array(edges_list).T, dtype=torch.long)\n",
    "\n",
    "        # data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "        edge_attr = torch.tensor(np.array(edge_features_list),\n",
    "                                 dtype=torch.long)\n",
    "    else:   # mol has no bonds\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, num_bond_features), dtype=torch.long)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return data\n",
    "\n",
    "def graph_data_obj_to_mol_simple(data_x, data_edge_index, data_edge_attr):\n",
    "    \"\"\"\n",
    "    Convert pytorch geometric data obj to rdkit mol object. NB: Uses simplified\n",
    "    atom and bond features, and represent as indices.\n",
    "    :param: data_x:\n",
    "    :param: data_edge_index:\n",
    "    :param: data_edge_attr\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    # atoms\n",
    "    atom_features = data_x.cpu().numpy()\n",
    "    num_atoms = atom_features.shape[0]\n",
    "    for i in range(num_atoms):\n",
    "        atomic_num_idx, chirality_tag_idx = atom_features[i]\n",
    "        atomic_num = allowable_features['possible_atomic_num_list'][atomic_num_idx]\n",
    "        chirality_tag = allowable_features['possible_chirality_list'][chirality_tag_idx]\n",
    "        atom = Chem.Atom(atomic_num)\n",
    "        atom.SetChiralTag(chirality_tag)\n",
    "        mol.AddAtom(atom)\n",
    "\n",
    "    # bonds\n",
    "    edge_index = data_edge_index.cpu().numpy()\n",
    "    edge_attr = data_edge_attr.cpu().numpy()\n",
    "    num_bonds = edge_index.shape[1]\n",
    "    for j in range(0, num_bonds, 2):\n",
    "        begin_idx = int(edge_index[0, j])\n",
    "        end_idx = int(edge_index[1, j])\n",
    "        bond_type_idx, bond_dir_idx = edge_attr[j]\n",
    "        bond_type = allowable_features['possible_bonds'][bond_type_idx]\n",
    "        bond_dir = allowable_features['possible_bond_dirs'][bond_dir_idx]\n",
    "        mol.AddBond(begin_idx, end_idx, bond_type)\n",
    "        # set bond direction\n",
    "        new_bond = mol.GetBondBetweenAtoms(begin_idx, end_idx)\n",
    "        new_bond.SetBondDir(bond_dir)\n",
    "\n",
    "    # Chem.SanitizeMol(mol) # fails for COC1=CC2=C(NC(=N2)[S@@](=O)CC2=NC=C(\n",
    "    # C)C(OC)=C2C)C=C1, when aromatic bond is possible\n",
    "    # when we do not have aromatic bonds\n",
    "    # Chem.SanitizeMol(mol, sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "\n",
    "    return mol\n",
    "\n",
    "def graph_data_obj_to_nx_simple(data):\n",
    "    \"\"\"\n",
    "    Converts graph Data object required by the pytorch geometric package to\n",
    "    network x data object. NB: Uses simplified atom and bond features,\n",
    "    and represent as indices. NB: possible issues with recapitulating relative\n",
    "    stereochemistry since the edges in the nx object are unordered.\n",
    "    :param data: pytorch geometric Data object\n",
    "    :return: network x object\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # atoms\n",
    "    atom_features = data.x.cpu().numpy()\n",
    "    num_atoms = atom_features.shape[0]\n",
    "    for i in range(num_atoms):\n",
    "        atomic_num_idx, chirality_tag_idx = atom_features[i]\n",
    "        G.add_node(i, atom_num_idx=atomic_num_idx, chirality_tag_idx=chirality_tag_idx)\n",
    "        pass\n",
    "\n",
    "    # bonds\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    edge_attr = data.edge_attr.cpu().numpy()\n",
    "    num_bonds = edge_index.shape[1]\n",
    "    for j in range(0, num_bonds, 2):\n",
    "        begin_idx = int(edge_index[0, j])\n",
    "        end_idx = int(edge_index[1, j])\n",
    "        bond_type_idx, bond_dir_idx = edge_attr[j]\n",
    "        if not G.has_edge(begin_idx, end_idx):\n",
    "            G.add_edge(begin_idx, end_idx, bond_type_idx=bond_type_idx,\n",
    "                       bond_dir_idx=bond_dir_idx)\n",
    "\n",
    "    return G\n",
    "\n",
    "def nx_to_graph_data_obj_simple(G):\n",
    "    \"\"\"\n",
    "    Converts nx graph to pytorch geometric Data object. Assume node indices\n",
    "    are numbered from 0 to num_nodes - 1. NB: Uses simplified atom and bond\n",
    "    features, and represent as indices. NB: possible issues with\n",
    "    recapitulating relative stereochemistry since the edges in the nx\n",
    "    object are unordered.\n",
    "    :param G: nx graph obj\n",
    "    :return: pytorch geometric Data object\n",
    "    \"\"\"\n",
    "    # atoms\n",
    "    num_atom_features = 2  # atom type,  chirality tag\n",
    "    atom_features_list = []\n",
    "    for _, node in G.nodes(data=True):\n",
    "        atom_feature = [node['atom_num_idx'], node['chirality_tag_idx']]\n",
    "        atom_features_list.append(atom_feature)\n",
    "    x = torch.tensor(np.array(atom_features_list), dtype=torch.long)\n",
    "\n",
    "    # bonds\n",
    "    num_bond_features = 2  # bond type, bond direction\n",
    "    if len(G.edges()) > 0:  # mol has bonds\n",
    "        edges_list = []\n",
    "        edge_features_list = []\n",
    "        for i, j, edge in G.edges(data=True):\n",
    "            edge_feature = [edge['bond_type_idx'], edge['bond_dir_idx']]\n",
    "            edges_list.append((i, j))\n",
    "            edge_features_list.append(edge_feature)\n",
    "            edges_list.append((j, i))\n",
    "            edge_features_list.append(edge_feature)\n",
    "\n",
    "        # data.edge_index: Graph connectivity in COO format with shape [2, num_edges]\n",
    "        edge_index = torch.tensor(np.array(edges_list).T, dtype=torch.long)\n",
    "        #attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "        edge_attr = torch.tensor(np.array(edge_features_list),\n",
    "                                 dtype=torch.long)\n",
    "    else:   # mol has no bonds\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, num_bond_features), dtype=torch.long)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_gasteiger_partial_charges(mol, n_iter=12):\n",
    "    \"\"\"\n",
    "    Calculates list of gasteiger partial charges for each atom in mol object.\n",
    "    :param mol: rdkit mol object\n",
    "        # data.edge_\n",
    "    :param n_iter: number of iterations. Default 12\n",
    "    :return: list of computed partial charges for each atom.\n",
    "    \"\"\"\n",
    "    Chem.rdPartialCharges.ComputeGasteigerCharges(mol, nIter=n_iter,\n",
    "                                                  throwOnParamFailure=True)\n",
    "    partial_charges = [float(a.GetProp('_GasteigerCharge')) for a in\n",
    "                       mol.GetAtoms()]\n",
    "    return partial_charges\n",
    "\n",
    "def create_standardized_mol_id(smiles):\n",
    "    \"\"\"\n",
    "\n",
    "    :param smiles:\n",
    "    :return: inchi\n",
    "    \"\"\"\n",
    "    if check_smiles_validity(smiles):\n",
    "        # remove stereochemistry\n",
    "        smiles = AllChem.MolToSmiles(AllChem.MolFromSmiles(smiles),\n",
    "                                     isomericSmiles=False)\n",
    "        mol = AllChem.MolFromSmiles(smiles)\n",
    "        if mol != None: # to catch weird issue with O=C1O[al]2oc(=O)c3ccc(cn3)c3ccccc3c3cccc(c3)c3ccccc3c3cc(C(F)(F)F)c(cc3o2)-c2ccccc2-c2cccc(c2)-c2ccccc2-c2cccnc21\n",
    "            if '.' in smiles: # if multiple species, pick largest molecule\n",
    "                mol_species_list = split_rdkit_mol_obj(mol)\n",
    "                largest_mol = get_largest_mol(mol_species_list)\n",
    "                inchi = AllChem.MolToInchi(largest_mol)\n",
    "            else:\n",
    "                inchi = AllChem.MolToInchi(mol)\n",
    "            return inchi\n",
    "        else:\n",
    "            return\n",
    "    else:\n",
    "        return\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader    \n",
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,seqPath):\n",
    "        if type(seqPath)==str:\n",
    "            self.df_file=pd.read_csv(seqPath,sep=',',header=None)\n",
    "            self.sequence_list=self.df_file.iloc[:,0]\n",
    "        else:\n",
    "            self.df=pd.DataFrame(seqPath)\n",
    "            self.sequence_list=self.df.iloc[:,0].tolist()\n",
    "        #self.sequence_tensor=torch.tensor(self.sequence_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sequence=self.sequence_list[idx]\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequence_list)\n",
    "\n",
    "'''   \n",
    "class SmileDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,smilePath):\n",
    "        if type(smilePath)==str:\n",
    "            self.df_file=pd.read_csv(smilePath,sep=',',header=None)\n",
    "            self.smile_list=self.df_file.iloc[:,0]\n",
    "        else:\n",
    "            self.df=pd.DataFrame(smilePath)\n",
    "            self.smile_list=self.df.iloc[:,0].tolist()\n",
    "        #self.sequence_tensor=torch.tensor(self.sequence_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        smile=self.smile_list[idx]\n",
    "        \n",
    "        return smile\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.smile_list)\n",
    "\n",
    "''' \n",
    "class SmileDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,smilePath):\n",
    "        if type(smilePath)==str:\n",
    "            self.df_file=pd.read_csv(smilePath,sep=',',header=None)\n",
    "            self.smile_list=self.df_file.iloc[:,0]\n",
    "        else:\n",
    "            self.df=pd.DataFrame(smilePath)\n",
    "            self.smile_list=self.df.iloc[:,0].tolist()\n",
    "        #self.sequence_tensor=torch.tensor(self.sequence_list)\n",
    "        #print('self.smile_list:',self.smile_list)\n",
    "        self.original_smiles = []\n",
    "        self.original_canonical_map = {\n",
    "            smi: normalize_smiles(smi, canonical=True, isomeric=False) for smi in self.smile_list\n",
    "        }\n",
    "\n",
    "        #self.tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        for i in range(len(self.smile_list)):\n",
    "            smi = self.smile_list[i]\n",
    "            if smi in self.original_canonical_map.keys():\n",
    "                self.original_smiles.append(smi)\n",
    "                #print('smi:',len(smi))\n",
    "            #else:\n",
    "                #print('smile not in canonical_map')\n",
    "        #print('self.original_smiles:',self.original_smiles)\n",
    "        #print(f\"Embeddings not found for {len(self.smile_list) - len(self.original_smiles)} molecules\")\n",
    "        self.original_smiles_1=pd.Series(self.original_smiles)\n",
    "        self.original_canonical_map_1=pd.Series(self.original_canonical_map)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #smile=self.smile_list[idx]\n",
    "        #print('idx:',idx)\n",
    "        original_smiles = self.original_smiles_1[idx]\n",
    "        #print('original_smies:',original_smiles)\n",
    "        canonical_smiles = self.original_canonical_map_1[original_smiles]\n",
    "        #print('canonical_smile_len:',len(canonical_smiles))\n",
    "        return canonical_smiles\n",
    "        #return smile\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_smiles)        \n",
    "        \n",
    "class MyDataset(InMemoryDataset):\n",
    "    def __init__(self, datasetA, datasetB):\n",
    "        self.datasetA = datasetA\n",
    "        self.datasetB = datasetB\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "    # def __getitem__(self, index):\n",
    "    #     xA = self.datasetA[index]\n",
    "    #     xB = self.datasetB[index]\n",
    "    #     return xA, xB\n",
    "\n",
    "    # def get(self, idx):\n",
    "    def __getitem__(self, idx):\n",
    "        dataA = Data()\n",
    "        dataB = Data()\n",
    "        for key in self.data.keys:\n",
    "            item, slices = self.data[key], self.slices[key]\n",
    "            s = list(repeat(slice(None), item.dim()))\n",
    "            s[data.__cat_dim__(key, item)] = slice(slices[idx],\n",
    "                                                    slices[idx + 1])\n",
    "            dataA[key] = item[s]\n",
    "            dataB[key] = item[s]\n",
    "        return dataA, dataB\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datasetA)\n",
    "\n",
    "class MoleculeDataset(InMemoryDataset):#InMemoryDataset转换成分子图\n",
    "#class MoleculeDataset(Dataset):#InMemoryDataset转换成分子图\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 #data = None,\n",
    "                 #slices = None,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None,\n",
    "                 dataset='zinc250k',\n",
    "                 empty=False):\n",
    "        \"\"\"\n",
    "        Adapted from qm9.py. Disabled the download functionality\n",
    "        :param root: directory of the dataset, containing a raw and processed\n",
    "        dir. The raw dir should contain the file containing the smiles, and the\n",
    "        processed dir can either empty or a previously processed file\n",
    "        :param dataset: name of the dataset. Currently only implemented for\n",
    "        zinc250k, chembl_with_labels, tox21, hiv, bace, bbbp, clintox, esol,\n",
    "        freesolv, lipophilicity, muv, pcba, sider, toxcast\n",
    "        :param empty: if True, then will not load any data obj. For\n",
    "        initializing empty dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.root = root\n",
    "        super(MoleculeDataset, self).__init__(root, transform, pre_transform,\n",
    "                                                 pre_filter)\n",
    "        self.transform, self.pre_transform, self.pre_filter = transform, pre_transform, pre_filter\n",
    "\n",
    "        if not empty:\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "            #self.sequence=torch.load(self.processed_paths[1])\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        #sequence=''\n",
    "        data = Data()\n",
    "        # data_masked = self.transform(data)\n",
    "        # print('data', self.data.edge_index)\n",
    "        # print('slices', self.slices)\n",
    "        for key in self.data.keys:\n",
    "            #print('key:',key)\n",
    "            item, slices = self.data[key], self.slices[key]\n",
    "            #print('item,slices:',item,slices)\n",
    "            s = list(repeat(slice(None), item.dim()))\n",
    "            s[data.__cat_dim__(key, item)] = slice(slices[idx],\n",
    "                                                    slices[idx + 1])\n",
    "            data[key] = item[s]\n",
    "        #sequence=self.sequence[idx]\n",
    "        # print(data.edge_index)\n",
    "        # transform = MaskAtom(num_atom_type = 119, num_edge_type = 5, mask_rate = 0.15, mask_edge=0)\n",
    "        # if self.transform:\n",
    "        #     data_masked = transform(data)\n",
    "        # with torch.no_grad():\n",
    "        #     atom_ids = tokenizer.get_codebook_indices(data.x, data.edge_index, data.edge_attr)\n",
    "        # dataset = {'data': data, 'label':atom_ids}\n",
    "        # print('data.x', data.x)\n",
    "        # with torch.no_grad():\n",
    "        #     # atom_ids = tokenizer.get_codebook_indices(data.x, data.edge_index, data.edge_attr)\n",
    "        #     atom_ids = torch.zeros(data.x.size(0))\n",
    "\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        file_name_list = os.listdir(self.raw_dir)\n",
    "        # assert len(file_name_list) == 1     # currently assume we have a\n",
    "        # # single raw file\n",
    "        return file_name_list\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'geometric_data_processed.pt'\n",
    "\n",
    "    def download(self):\n",
    "        raise NotImplementedError('Must indicate valid location of raw data. '\n",
    "                                  'No download allowed')\n",
    "\n",
    "    def process(self):\n",
    "        data_smiles_list = []\n",
    "        \n",
    "        data_list = []\n",
    "        data_sequence_list=[]\n",
    "        #print('process:===================')\n",
    "        if self.dataset == 'zinc_standard_agent':\n",
    "            #print('zinc_standard_agent:==============')\n",
    "            input_path = self.raw_paths[0]\n",
    "            input_df = pd.read_csv(input_path, sep=',', compression='gzip',\n",
    "                                   dtype='str')\n",
    "            smiles_list = list(input_df['smiles'])\n",
    "            zinc_id_list = list(input_df['zinc_id'])\n",
    "            for i in range(len(smiles_list)):\n",
    "                #print(i)\n",
    "                s = smiles_list[i]\n",
    "                # each example contains a single species\n",
    "                try:\n",
    "                    rdkit_mol = AllChem.MolFromSmiles(s)\n",
    "                    if rdkit_mol != None:  # ignore invalid mol objects\n",
    "                        # # convert aromatic bonds to double bonds\n",
    "                        # Chem.SanitizeMol(rdkit_mol,\n",
    "                        #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                        data = mol_to_graph_data_obj_simple(rdkit_mol)###########################\n",
    "                        # manually add mol id\n",
    "                        id = int(zinc_id_list[i].split('ZINC')[1].lstrip('0'))\n",
    "                        data.id = torch.tensor(\n",
    "                            [id])  # id here is zinc id value, stripped of\n",
    "                        # leading zeros\n",
    "                        data_list.append(data)\n",
    "                        data_smiles_list.append(smiles_list[i])\n",
    "                        print('data_smiles_liat')\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        elif self.dataset == 'chembl_filtered':\n",
    "            print('chembl_filtered==============')\n",
    "            ### get downstream test molecules.\n",
    "            from splitters import scaffold_split\n",
    "\n",
    "            ### \n",
    "            downstream_dir = [\n",
    "            'dataset/bace',\n",
    "            'dataset/bbbp',\n",
    "            'dataset/clintox',\n",
    "            'dataset/esol',\n",
    "            'dataset/freesolv',\n",
    "            'dataset/hiv',\n",
    "            'dataset/lipophilicity',\n",
    "            'dataset/muv',\n",
    "            # 'dataset/pcba/processed/smiles.csv',\n",
    "            'dataset/sider',\n",
    "            'dataset/tox21',\n",
    "            'dataset/toxcast',\n",
    "            'dataset/affinity'\n",
    "            ]\n",
    "\n",
    "            downstream_inchi_set = set()\n",
    "            for d_path in downstream_dir:\n",
    "                print(d_path)\n",
    "                dataset_name = d_path.split('/')[1]\n",
    "                downstream_dataset = MoleculeDataset(d_path, dataset=dataset_name)#\n",
    "                downstream_smiles = pd.read_csv(os.path.join(d_path,\n",
    "                                                             'processed', 'smiles.csv'),\n",
    "                                                header=None)[0].tolist()\n",
    "\n",
    "                assert len(downstream_dataset) == len(downstream_smiles)\n",
    "\n",
    "                _, _, _, (train_smiles, valid_smiles, test_smiles) = scaffold_split(downstream_dataset, downstream_smiles, task_idx=None, null_value=0,\n",
    "                                   frac_train=0.8,frac_valid=0.1, frac_test=0.1,\n",
    "                                   return_smiles=True)\n",
    "\n",
    "                ### remove both test and validation molecules\n",
    "                remove_smiles = test_smiles + valid_smiles\n",
    "\n",
    "                downstream_inchis = []\n",
    "                for smiles in remove_smiles:\n",
    "                    species_list = smiles.split('.')\n",
    "                    for s in species_list:  # record inchi for all species, not just\n",
    "                     # largest (by default in create_standardized_mol_id if input has\n",
    "                     # multiple species)\n",
    "                        inchi = create_standardized_mol_id(s)\n",
    "                        downstream_inchis.append(inchi)\n",
    "                downstream_inchi_set.update(downstream_inchis)\n",
    "\n",
    "            smiles_list, rdkit_mol_objs, folds, labels = \\\n",
    "                _load_chembl_with_labels_dataset(os.path.join(self.root, 'raw'))################333333333333333333\n",
    "\n",
    "            print('processing')####################\n",
    "            for i in range(len(rdkit_mol_objs)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                if rdkit_mol != None:\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    mw = Descriptors.MolWt(rdkit_mol)\n",
    "                    if 50 <= mw <= 900:\n",
    "                        inchi = create_standardized_mol_id(smiles_list[i])\n",
    "                        if inchi != None and inchi not in downstream_inchi_set:\n",
    "                            data = mol_to_graph_data_obj_simple(rdkit_mol)############\n",
    "                            # manually add mol id\n",
    "                            data.id = torch.tensor(\n",
    "                                [i])  # id here is the index of the mol in\n",
    "                            # the dataset\n",
    "                            data.y = torch.tensor(labels[i, :])\n",
    "                            # fold information\n",
    "                            if i in folds[0]:\n",
    "                                data.fold = torch.tensor([0])\n",
    "                            elif i in folds[1]:\n",
    "                                data.fold = torch.tensor([1])\n",
    "                            else:\n",
    "                                data.fold = torch.tensor([2])\n",
    "                            data_list.append(data)\n",
    "                            data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'tox21':\n",
    "            print('tox21================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_tox21_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]#\n",
    "                ## convert aromatic bonds to double bonds\n",
    "                #Chem.SanitizeMol(rdkit_mol,\n",
    "                                 #sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)#\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor(labels[i, :])\n",
    "                data_list.append(data)############\n",
    "                data_smiles_list.append(smiles_list[i])####################\n",
    "\n",
    "        elif self.dataset == 'hiv':\n",
    "            print('hiv====================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_hiv_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'bace':\n",
    "            print('bace====================')\n",
    "            smiles_list, rdkit_mol_objs, folds, labels = \\\n",
    "                _load_bace_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data.fold = torch.tensor([folds[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'bbbp':\n",
    "            print('bbbp===================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_bbbp_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                if rdkit_mol != None:\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])  # id here is the index of the mol in\n",
    "                    # the dataset\n",
    "                    data.y = torch.tensor([labels[i]])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'clintox':\n",
    "            print('clintox===================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_clintox_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                if rdkit_mol != None:\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])  # id here is the index of the mol in\n",
    "                    # the dataset\n",
    "                    data.y = torch.tensor(labels[i, :])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'esol':\n",
    "            print('esol================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_esol_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'freesolv':\n",
    "            print('freesolv================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_freesolv_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'lipophilicity':\n",
    "            print('lipophilicity================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_lipophilicity_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'muv':\n",
    "            print('muv======================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_muv_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor(labels[i, :])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'pcba':\n",
    "            print('pcba====================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_pcba_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor(labels[i, :])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'pcba_pretrain':\n",
    "            print('pcba_pretrain===============')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_pcba_dataset(self.raw_paths[0])\n",
    "            downstream_inchi = set(pd.read_csv(os.path.join(self.root,\n",
    "                                                            'downstream_mol_inchi_may_24_2019'),\n",
    "                                               sep=',', header=None)[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                if '.' not in smiles_list[i]:   # remove examples with\n",
    "                    # multiples species\n",
    "                    rdkit_mol = rdkit_mol_objs[i]\n",
    "                    mw = Descriptors.MolWt(rdkit_mol)\n",
    "                    if 50 <= mw <= 900:\n",
    "                        inchi = create_standardized_mol_id(smiles_list[i])\n",
    "                        if inchi != None and inchi not in downstream_inchi:\n",
    "                            # # convert aromatic bonds to double bonds\n",
    "                            # Chem.SanitizeMol(rdkit_mol,\n",
    "                            #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                            data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                            # manually add mol id\n",
    "                            data.id = torch.tensor(\n",
    "                                [i])  # id here is the index of the mol in\n",
    "                            # the dataset\n",
    "                            data.y = torch.tensor(labels[i, :])\n",
    "                            data_list.append(data)\n",
    "                            data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        # elif self.dataset == ''\n",
    "\n",
    "        elif self.dataset == 'sider':\n",
    "            print('sider=====================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_sider_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)#\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor(labels[i, :])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'toxcast':\n",
    "            print('txcast=================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_toxcast_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                if rdkit_mol != None:\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])  # id here is the index of the mol in\n",
    "                    # the dataset\n",
    "                    data.y = torch.tensor(labels[i, :])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'ptc_mr':\n",
    "            print('ptc_mr====================')\n",
    "            input_path = self.raw_paths[0]\n",
    "            input_df = pd.read_csv(input_path, sep=',', header=None, names=['id', 'label', 'smiles'])\n",
    "            smiles_list = input_df['smiles']\n",
    "            labels = input_df['label'].values\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                s = smiles_list[i]\n",
    "                rdkit_mol = AllChem.MolFromSmiles(s)\n",
    "                if rdkit_mol != None:  # ignore invalid mol objects\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])\n",
    "                    data.y = torch.tensor([labels[i]])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'mutag':\n",
    "            print('mutag====================')\n",
    "            smiles_path = os.path.join(self.root, 'raw', 'mutag_188_data.can')\n",
    "            # smiles_path = 'dataset/mutag/raw/mutag_188_data.can'\n",
    "            labels_path = os.path.join(self.root, 'raw', 'mutag_188_target.txt')\n",
    "            # labels_path = 'dataset/mutag/raw/mutag_188_target.txt'\n",
    "            smiles_list = pd.read_csv(smiles_path, sep=' ', header=None)[0]\n",
    "            labels = pd.read_csv(labels_path, header=None)[0].values\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                s = smiles_list[i]\n",
    "                rdkit_mol = AllChem.MolFromSmiles(s)\n",
    "                if rdkit_mol != None:  # ignore invalid mol objects\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])\n",
    "                    data.y = torch.tensor([labels[i]])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "                    \n",
    "        elif self.dataset == 'affinity':\n",
    "            #print('+++++++++++++++++++++++++++++')\n",
    "            input_path = self.raw_paths[0]\n",
    "            #input_path = self.raw_paths[1]\n",
    "            #input_df = pd.read_csv(input_path, sep=','，names=['id', 'label','sequence', 'smiles'])\n",
    "            input_df = pd.read_csv(input_path, sep=',')\n",
    "            \n",
    "            codes_list=input_df['code']\n",
    "            smiles_list = input_df['smiles']\n",
    "            labels = input_df['logKa'].values######################dropout\n",
    "            #print('labels:',type(labels),labels[3])\n",
    "            sequences=input_df['sequence']\n",
    "            #print('sequences:',sequences)\n",
    "            #sequences=np.array(sequences)\n",
    "            #sequences=torch.from_numpy(sequences)\n",
    "            #print('sequence:',type(sequences),sequences.shape)\n",
    "            #print('sequences[3]:',sequences[3])\n",
    "            for i in range(len(smiles_list)):\n",
    "                #print(\"============================================================================\")\n",
    "                s = smiles_list[i]\n",
    "                rdkit_mol = AllChem.MolFromSmiles(s)\n",
    "                if rdkit_mol != None:  # ignore invalid mol objects\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    #ss=smiles_list[i]\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)###################转化成了x.data,x.index_edge........\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])\n",
    "                    data.y = torch.tensor([labels[i]])\n",
    "                    #print('data.y:',data.y)\n",
    "                    #print('data.y:',data.y)\n",
    "                    #print('sequences:'sequences)\n",
    "                    sequence=sequences[i]#ValueError: too many dimensions 'str'：不要转化成tensor\n",
    "                    #print('sequence:',type(data.sequence),data.sequence.shape)##############################\n",
    "                    data_list.append(data)\n",
    "                    data_sequence_list.append(sequence)\n",
    "                    #data_list.append(sequence)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "                    \n",
    "\n",
    "        else:\n",
    "            raise ValueError('Invalid dataset name')\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        # write data_smiles_list in processed paths\n",
    "        #print('data_list:===================')\n",
    "        data_smiles_series = pd.Series(data_smiles_list)\n",
    "        data_smiles_series.to_csv(os.path.join(self.processed_dir,\n",
    "                                               'smiles.csv'), index=False,\n",
    "                                  header=False)\n",
    "        #data_sequence_series = pd.Series(data_sequence_list,columns=['sequence'])\n",
    "        data_sequence_series = pd.Series(data_sequence_list)\n",
    "        data_sequence_series.to_csv(os.path.join(self.processed_dir,\n",
    "                                               'sequence.csv'), index=False,\n",
    "                                  header=None)\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        \n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        #torch.save(sequence,self.processed_paths[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "class MoleculeDataset_1(Dataset):\n",
    "    def __init__(self, root, filename, test=False, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.filename = filename\n",
    "        super(MoleculeDataset_1, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "\n",
    "        if self.test:\n",
    "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "        \n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "        featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "        for index, row in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            # Featurize molecule\n",
    "            mol = Chem.MolFromSmiles(row[\"smiles\"])\n",
    "            f = featurizer._featurize(mol)\n",
    "            data = f.to_pyg_graph()\n",
    "            data.y = self._get_label(row[\"HIV_active\"])\n",
    "            data.smiles = row[\"smiles\"]\n",
    "            if self.test:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{index}.pt'))\n",
    "            else:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_{index}.pt'))\n",
    "            \n",
    "\n",
    "    def _get_label(self, label):\n",
    "        label = np.asarray([label])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        if self.test:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{idx}.pt'))\n",
    "        else:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_{idx}.pt'))        \n",
    "        return data\n",
    "        \n",
    "from torch_geometric.data import Dataset        \n",
    "class MoleculeDatasetBig(Dataset):#InMemoryDataset转换成分子图\n",
    "#class MoleculeDataset(Dataset):#InMemoryDataset转换成分子图\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 #data = None,\n",
    "                 #slices = None,\n",
    "                 test=False,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None,\n",
    "                 dataset='zinc250k',\n",
    "                 empty=False):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.test=test\n",
    "        self.root = root\n",
    "        super(MoleculeDatasetBig, self).__init__(root,test,transform,pre_transform,pre_filter)\n",
    "        self.transform, self.pre_transform, self.pre_filter = transform, pre_transform, pre_filter\n",
    "        self.data=dataset\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        print('root:',self.root)\n",
    "        return os.path.join(self.root, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return os.path.join(self.root, 'processed')\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        print('raw_dir:',self.raw_dir)\n",
    "        file_name_list = os.listdir(self.raw_dir)\n",
    "        # assert len(file_name_list) == 1     # currently assume we have a\n",
    "        # # single raw file\n",
    "        return file_name_list\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        #return 'geometric_data_processed.pt'\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "\n",
    "        if self.test:\n",
    "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "\n",
    "    def download(self):\n",
    "        raise NotImplementedError('Must indicate valid location of raw data. '\n",
    "                                  'No download allowed')\n",
    "\n",
    "    def process(self):\n",
    "        #idx=0\n",
    "        data_smiles_list = []\n",
    "        \n",
    "        data_list = []\n",
    "        data_sequence_list=[]\n",
    "        #print('process:===================')\n",
    "        if self.dataset == 'zinc_standard_agent':\n",
    "            #print('zinc_standard_agent:==============')\n",
    "            input_path = self.raw_paths[0]\n",
    "            input_df = pd.read_csv(input_path, sep=',', compression='gzip',\n",
    "                                   dtype='str')\n",
    "            smiles_list = list(input_df['smiles'])\n",
    "            zinc_id_list = list(input_df['zinc_id'])\n",
    "            for i in range(len(smiles_list)):\n",
    "                #print(i)\n",
    "                s = smiles_list[i]\n",
    "                # each example contains a single species\n",
    "                try:\n",
    "                    rdkit_mol = AllChem.MolFromSmiles(s)\n",
    "                    if rdkit_mol != None:  # ignore invalid mol objects\n",
    "                        # # convert aromatic bonds to double bonds\n",
    "                        # Chem.SanitizeMol(rdkit_mol,\n",
    "                        #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                        data = mol_to_graph_data_obj_simple(rdkit_mol)###########################\n",
    "                        # manually add mol id\n",
    "                        id = int(zinc_id_list[i].split('ZINC')[1].lstrip('0'))\n",
    "                        data.id = torch.tensor(\n",
    "                            [id])  # id here is zinc id value, stripped of\n",
    "                        # leading zeros\n",
    "                        data_list.append(data)\n",
    "                        data_smiles_list.append(smiles_list[i])\n",
    "                        print('data_smiles_liat')\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        elif self.dataset == 'chembl_filtered':\n",
    "            print('chembl_filtered==============')\n",
    "            ### get downstream test molecules.\n",
    "            from splitters import scaffold_split\n",
    "\n",
    "            ### \n",
    "            downstream_dir = [\n",
    "            'dataset/bace',\n",
    "            'dataset/bbbp',\n",
    "            'dataset/clintox',\n",
    "            'dataset/esol',\n",
    "            'dataset/freesolv',\n",
    "            'dataset/hiv',\n",
    "            'dataset/lipophilicity',\n",
    "            'dataset/muv',\n",
    "            # 'dataset/pcba/processed/smiles.csv',\n",
    "            'dataset/sider',\n",
    "            'dataset/tox21',\n",
    "            'dataset/toxcast',\n",
    "            'dataset/affinity'\n",
    "            ]\n",
    "\n",
    "            downstream_inchi_set = set()\n",
    "            for d_path in downstream_dir:\n",
    "                print(d_path)\n",
    "                dataset_name = d_path.split('/')[1]\n",
    "                downstream_dataset = MoleculeDatasetBig(d_path, dataset=dataset_name)#\n",
    "                downstream_smiles = pd.read_csv(os.path.join(d_path,\n",
    "                                                             'processed', 'smiles.csv'),\n",
    "                                                header=None)[0].tolist()\n",
    "\n",
    "                assert len(downstream_dataset) == len(downstream_smiles)\n",
    "\n",
    "                _, _, _, (train_smiles, valid_smiles, test_smiles) = scaffold_split(downstream_dataset, downstream_smiles, task_idx=None, null_value=0,\n",
    "                                   frac_train=0.8,frac_valid=0.1, frac_test=0.1,\n",
    "                                   return_smiles=True)\n",
    "\n",
    "                ### remove both test and validation molecules\n",
    "                remove_smiles = test_smiles + valid_smiles\n",
    "\n",
    "                downstream_inchis = []\n",
    "                for smiles in remove_smiles:\n",
    "                    species_list = smiles.split('.')\n",
    "                    for s in species_list:  # record inchi for all species, not just\n",
    "                     # largest (by default in create_standardized_mol_id if input has\n",
    "                     # multiple species)\n",
    "                        inchi = create_standardized_mol_id(s)\n",
    "                        downstream_inchis.append(inchi)\n",
    "                downstream_inchi_set.update(downstream_inchis)\n",
    "\n",
    "            smiles_list, rdkit_mol_objs, folds, labels = \\\n",
    "                _load_chembl_with_labels_dataset(os.path.join(self.root, 'raw'))################333333333333333333\n",
    "\n",
    "            print('processing')####################\n",
    "            for i in range(len(rdkit_mol_objs)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                if rdkit_mol != None:\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    mw = Descriptors.MolWt(rdkit_mol)\n",
    "                    if 50 <= mw <= 900:\n",
    "                        inchi = create_standardized_mol_id(smiles_list[i])\n",
    "                        if inchi != None and inchi not in downstream_inchi_set:\n",
    "                            data = mol_to_graph_data_obj_simple(rdkit_mol)############\n",
    "                            # manually add mol id\n",
    "                            data.id = torch.tensor(\n",
    "                                [i])  # id here is the index of the mol in\n",
    "                            # the dataset\n",
    "                            data.y = torch.tensor(labels[i, :])\n",
    "                            # fold information\n",
    "                            if i in folds[0]:\n",
    "                                data.fold = torch.tensor([0])\n",
    "                            elif i in folds[1]:\n",
    "                                data.fold = torch.tensor([1])\n",
    "                            else:\n",
    "                                data.fold = torch.tensor([2])\n",
    "                            data_list.append(data)\n",
    "                            data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'tox21':\n",
    "            print('tox21================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_tox21_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]#\n",
    "                ## convert aromatic bonds to double bonds\n",
    "                #Chem.SanitizeMol(rdkit_mol,\n",
    "                                 #sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)#\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor(labels[i, :])\n",
    "                data_list.append(data)############\n",
    "                data_smiles_list.append(smiles_list[i])####################\n",
    "\n",
    "        elif self.dataset == 'hiv':\n",
    "            print('hiv====================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_hiv_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'bace':\n",
    "            print('bace====================')\n",
    "            smiles_list, rdkit_mol_objs, folds, labels = \\\n",
    "                _load_bace_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data.fold = torch.tensor([folds[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'bbbp':\n",
    "            print('bbbp===================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_bbbp_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                if rdkit_mol != None:\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])  # id here is the index of the mol in\n",
    "                    # the dataset\n",
    "                    data.y = torch.tensor([labels[i]])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'clintox':\n",
    "            print('clintox===================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_clintox_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                if rdkit_mol != None:\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])  # id here is the index of the mol in\n",
    "                    # the dataset\n",
    "                    data.y = torch.tensor(labels[i, :])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'esol':\n",
    "            print('esol================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_esol_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'freesolv':\n",
    "            print('freesolv================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_freesolv_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'lipophilicity':\n",
    "            print('lipophilicity================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_lipophilicity_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor([labels[i]])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'muv':\n",
    "            print('muv======================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_muv_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor(labels[i, :])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'pcba':\n",
    "            print('pcba====================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_pcba_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor(labels[i, :])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'pcba_pretrain':\n",
    "            print('pcba_pretrain===============')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_pcba_dataset(self.raw_paths[0])\n",
    "            downstream_inchi = set(pd.read_csv(os.path.join(self.root,\n",
    "                                                            'downstream_mol_inchi_may_24_2019'),\n",
    "                                               sep=',', header=None)[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                if '.' not in smiles_list[i]:   # remove examples with\n",
    "                    # multiples species\n",
    "                    rdkit_mol = rdkit_mol_objs[i]\n",
    "                    mw = Descriptors.MolWt(rdkit_mol)\n",
    "                    if 50 <= mw <= 900:\n",
    "                        inchi = create_standardized_mol_id(smiles_list[i])\n",
    "                        if inchi != None and inchi not in downstream_inchi:\n",
    "                            # # convert aromatic bonds to double bonds\n",
    "                            # Chem.SanitizeMol(rdkit_mol,\n",
    "                            #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                            data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                            # manually add mol id\n",
    "                            data.id = torch.tensor(\n",
    "                                [i])  # id here is the index of the mol in\n",
    "                            # the dataset\n",
    "                            data.y = torch.tensor(labels[i, :])\n",
    "                            data_list.append(data)\n",
    "                            data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        # elif self.dataset == ''\n",
    "\n",
    "        elif self.dataset == 'sider':\n",
    "            print('sider=====================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_sider_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                # Chem.SanitizeMol(rdkit_mol,\n",
    "                #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)#\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor(labels[i, :])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'toxcast':\n",
    "            print('txcast=================')\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_toxcast_dataset(self.raw_paths[0])\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                if rdkit_mol != None:\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])  # id here is the index of the mol in\n",
    "                    # the dataset\n",
    "                    data.y = torch.tensor(labels[i, :])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'ptc_mr':\n",
    "            print('ptc_mr====================')\n",
    "            input_path = self.raw_paths[0]\n",
    "            input_df = pd.read_csv(input_path, sep=',', header=None, names=['id', 'label', 'smiles'])\n",
    "            smiles_list = input_df['smiles']\n",
    "            labels = input_df['label'].values\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                s = smiles_list[i]\n",
    "                rdkit_mol = AllChem.MolFromSmiles(s)\n",
    "                if rdkit_mol != None:  # ignore invalid mol objects\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])\n",
    "                    data.y = torch.tensor([labels[i]])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        elif self.dataset == 'mutag':\n",
    "            print('mutag====================')\n",
    "            smiles_path = os.path.join(self.root, 'raw', 'mutag_188_data.can')\n",
    "            # smiles_path = 'dataset/mutag/raw/mutag_188_data.can'\n",
    "            labels_path = os.path.join(self.root, 'raw', 'mutag_188_target.txt')\n",
    "            # labels_path = 'dataset/mutag/raw/mutag_188_target.txt'\n",
    "            smiles_list = pd.read_csv(smiles_path, sep=' ', header=None)[0]\n",
    "            labels = pd.read_csv(labels_path, header=None)[0].values\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                s = smiles_list[i]\n",
    "                rdkit_mol = AllChem.MolFromSmiles(s)\n",
    "                if rdkit_mol != None:  # ignore invalid mol objects\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])\n",
    "                    data.y = torch.tensor([labels[i]])\n",
    "                    data_list.append(data)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "                    \n",
    "        elif self.dataset == 'affinity':\n",
    "            #print('+++++++++++++++++++++++++++++')\n",
    "            self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "            input_path = self.raw_paths[0]\n",
    "            #input_path = self.raw_paths[1]\n",
    "            #input_df = pd.read_csv(input_path, sep=','，names=['id', 'label','sequence', 'smiles'])\n",
    "            \n",
    "            input_df = pd.read_csv(input_path, sep=',')\n",
    "            #self.data=input_df\n",
    "            codes_list=input_df['code']\n",
    "            smiles_list = input_df['smiles']\n",
    "            labels = input_df['logKa'].values######################dropout\n",
    "            #print('labels:',type(labels),labels[3])\n",
    "            sequences=input_df['sequence']\n",
    "            #print('sequences:',sequences)\n",
    "            #sequences=np.array(sequences)\n",
    "            #sequences=torch.from_numpy(sequences)\n",
    "            #print('sequence:',type(sequences),sequences.shape)\n",
    "            #print('sequences[3]:',sequences[3])\n",
    "            for i in range(len(smiles_list)):\n",
    "                #print(\"============================================================================\")\n",
    "                s = smiles_list[i]\n",
    "                rdkit_mol = AllChem.MolFromSmiles(s)\n",
    "                if rdkit_mol != None:  # ignore invalid mol objects\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    #ss=smiles_list[i]\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)###################转化成了x.data,x.index_edge........\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])\n",
    "                    data.y = torch.tensor([labels[i]])\n",
    "                    #print('data.y:',data.y)\n",
    "                    #print('data.y:',data.y)\n",
    "                    #print('sequences:'sequences)\n",
    "                    sequence=sequences[i]#ValueError: too many dimensions 'str'：不要转化成tensor\n",
    "                    #print('sequence:',type(data.sequence),data.sequence.shape)##############################\n",
    "                    data_list.append(data)\n",
    "                    data_sequence_list.append(sequence)\n",
    "                    #data_list.append(sequence)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "                    \n",
    "                    if self.test:\n",
    "                        torch.save(data, os.path.join(self.processed_dir, f'data_test_{i}.pt'))\n",
    "                    else:\n",
    "                        torch.save(data, os.path.join(self.processed_dir, f'data_{i}.pt'))\n",
    "            \n",
    "                        \n",
    "\n",
    "        else:\n",
    "            raise ValueError('Invalid dataset name')\n",
    "        '''\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "        '''\n",
    "        # write data_smiles_list in processed paths\n",
    "        #print('data_list:===================')\n",
    "        data_smiles_series = pd.Series(data_smiles_list)\n",
    "        data_smiles_series.to_csv(os.path.join(self.processed_dir,\n",
    "                                               'smiles.csv'), index=False,\n",
    "                                  header=False)\n",
    "        #data_sequence_series = pd.Series(data_sequence_list,columns=['sequence'])\n",
    "        data_sequence_series = pd.Series(data_sequence_list)\n",
    "        data_sequence_series.to_csv(os.path.join(self.processed_dir,\n",
    "                                               'sequence.csv'), index=False,\n",
    "                                  header=None)\n",
    "        \n",
    "        #data, slices = self.collate(data_list)\n",
    "        '''\n",
    "        if self.test:\n",
    "            torch.save((data, slices), self.processed_paths[0])\n",
    "        '''\n",
    "        \n",
    "    def len(self):\n",
    "        #return len(self.data_list)\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        ## if iterable class is passed, return dataset objection\n",
    "        print('idx:',idx)\n",
    "        \n",
    "        ''''\n",
    "        if hasattr(index, \"__iter__\"):\n",
    "            dataset = MoleculeFingerprintDataset(self.root, self.dataset, self.radius, self.size, chirality=self.chirality)\n",
    "            dataset.data_list = [self.data_list[i] for i in index]\n",
    "            return dataset\n",
    "        else:\n",
    "            return self.data_list[index]\n",
    "        '''\n",
    "        if self.test:\n",
    "            data = torch.load(os.path.join(self.processed_dir, f'data_test_{idx}.pt'))\n",
    "        else:\n",
    "            data = torch.load(os.path.join(self.processed_dir, 'data_{idx}.pt'))        \n",
    "        return data\n",
    "        \n",
    "# NB: only properly tested when dataset_1 is chembl_with_labels and dataset_2\n",
    "# is pcba_pretrain\n",
    "\n",
    "class SeqMolDataset(SeqDataset, MoleculeDataset):\n",
    "    \n",
    "    def __init__(self, datasetA,datasetB):\n",
    "        #super(SeqMolDataset, self).__init__()\n",
    "        self.datasetA=datasetA\n",
    "        self.datasetB=datasetB\n",
    "        #self.data,self.slices=datasetB\n",
    "    def __getitem__(self,idx):\n",
    "        sequence=datasetA[idx]\n",
    "        '''\n",
    "        data = Data()\n",
    "        # data_masked = self.transform(data)\n",
    "        # print('data', self.data.edge_index)\n",
    "        # print('slices', self.slices)\n",
    "        for key in self.data.keys:\n",
    "            print('key:',key)\n",
    "            item, slices = self.data[key], self.slices[key]\n",
    "            print('item,slices:',item,slices)\n",
    "            s = list(repeat(slice(None), item.dim()))\n",
    "            s[data.__cat_dim__(key, item)] = slice(slices[idx],\n",
    "                                                    slices[idx + 1])\n",
    "            data[key] = item[s]\n",
    "       '''\n",
    "        data=datasetB[idx]\n",
    "        return sequence,data\n",
    "    def __len__(self):\n",
    "        return len(self.datasetA)\n",
    "        \n",
    "def merge_dataset_objs(dataset_1, dataset_2):\n",
    "    \"\"\"\n",
    "    Naively merge 2 molecule dataset objects, and ignore identities of\n",
    "    molecules. Assumes both datasets have multiple y labels, and will pad\n",
    "    accordingly. ie if dataset_1 has obj_1 with y dim 1310 and dataset_2 has\n",
    "    obj_2 with y dim 128, then the resulting obj_1 and obj_2 will have dim\n",
    "    1438, where obj_1 have the last 128 cols with 0, and obj_2 have\n",
    "    the first 1310 cols with 0.\n",
    "    :return: pytorch geometric dataset obj, with the x, edge_attr, edge_index,\n",
    "    new y attributes only\n",
    "    \"\"\"\n",
    "    d_1_y_dim = dataset_1[0].y.size()[0]\n",
    "    d_2_y_dim = dataset_2[0].y.size()[0]\n",
    "\n",
    "    data_list = []\n",
    "    # keep only x, edge_attr, edge_index, padded_y then append\n",
    "    for d in dataset_1:\n",
    "        old_y = d.y\n",
    "        new_y = torch.cat([old_y, torch.zeros(d_2_y_dim, dtype=torch.long)])\n",
    "        data_list.append(Data(x=d.x, edge_index=d.edge_index,\n",
    "                              edge_attr=d.edge_attr, y=new_y))\n",
    "\n",
    "    for d in dataset_2:\n",
    "        old_y = d.y\n",
    "        new_y = torch.cat([torch.zeros(d_1_y_dim, dtype=torch.long), old_y.long()])\n",
    "        data_list.append(Data(x=d.x, edge_index=d.edge_index,\n",
    "                              edge_attr=d.edge_attr, y=new_y))\n",
    "\n",
    "    # create 'empty' dataset obj. Just randomly pick a dataset and root path\n",
    "    # that has already been processed\n",
    "    new_dataset = MoleculeDataset(root='dataset/chembl_with_labels',\n",
    "                                  dataset='chembl_with_labels', empty=True)\n",
    "    # collate manually\n",
    "    new_dataset.data, new_dataset.slices = new_dataset.collate(data_list)\n",
    "\n",
    "    return new_dataset\n",
    "\n",
    "def create_circular_fingerprint(mol, radius, size, chirality):\n",
    "    \"\"\"\n",
    "\n",
    "    :param mol:\n",
    "    :param radius:\n",
    "    :param size:\n",
    "    :param chirality:\n",
    "    :return: np array of morgan fingerprint\n",
    "    \"\"\"\n",
    "    fp = GetMorganFingerprintAsBitVect(mol, radius,\n",
    "                                       nBits=size, useChirality=chirality)\n",
    "    return np.array(fp)\n",
    "\n",
    "class MoleculeFingerprintDataset(data.Dataset):\n",
    "    def __init__(self, root, dataset, radius, size, chirality=True):\n",
    "        \"\"\"\n",
    "        Create dataset object containing list of dicts, where each dict\n",
    "        contains the circular fingerprint of the molecule, label, id,\n",
    "        and possibly precomputed fold information\n",
    "        :param root: directory of the dataset, containing a raw and\n",
    "        processed_fp dir. The raw dir should contain the file containing the\n",
    "        smiles, and the processed_fp dir can either be empty or a\n",
    "        previously processed file\n",
    "        :param dataset: name of dataset. Currently only implemented for\n",
    "        tox21, hiv, chembl_with_labels\n",
    "        :param radius: radius of the circular fingerprints\n",
    "        :param size: size of the folded fingerprint vector\n",
    "        :param chirality: if True, fingerprint includes chirality information\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.root = root\n",
    "        self.radius = radius\n",
    "        self.size = size\n",
    "        self.chirality = chirality\n",
    "\n",
    "        self._load()\n",
    "\n",
    "    def _process(self):\n",
    "        data_smiles_list = []\n",
    "        data_list = []\n",
    "        if self.dataset == 'chembl_with_labels':\n",
    "            smiles_list, rdkit_mol_objs, folds, labels = \\\n",
    "                _load_chembl_with_labels_dataset(os.path.join(self.root, 'raw'))\n",
    "            print('processing')\n",
    "            for i in range(len(rdkit_mol_objs)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                if rdkit_mol != None:\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    fp_arr = create_circular_fingerprint(rdkit_mol,\n",
    "                                                         self.radius,\n",
    "                                                         self.size, self.chirality)\n",
    "                    fp_arr = torch.tensor(fp_arr)\n",
    "                    # manually add mol id\n",
    "                    id = torch.tensor([i])  # id here is the index of the mol in\n",
    "                    # the dataset\n",
    "                    y = torch.tensor(labels[i, :])\n",
    "                    # fold information\n",
    "                    if i in folds[0]:\n",
    "                        fold = torch.tensor([0])\n",
    "                    elif i in folds[1]:\n",
    "                        fold = torch.tensor([1])\n",
    "                    else:\n",
    "                        fold = torch.tensor([2])\n",
    "                    data_list.append({'fp_arr': fp_arr, 'id': id, 'y': y,\n",
    "                                      'fold': fold})\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "        elif self.dataset == 'tox21':\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_tox21_dataset(os.path.join(self.root, 'raw/tox21.csv'))\n",
    "            print('processing')\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                ## convert aromatic bonds to double bonds\n",
    "                fp_arr = create_circular_fingerprint(rdkit_mol,\n",
    "                                                        self.radius,\n",
    "                                                        self.size,\n",
    "                                                        self.chirality)\n",
    "                fp_arr = torch.tensor(fp_arr)\n",
    "\n",
    "                # manually add mol id\n",
    "                id = torch.tensor([i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                y = torch.tensor(labels[i, :])\n",
    "                data_list.append({'fp_arr': fp_arr, 'id': id, 'y': y})\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "        elif self.dataset == 'hiv':\n",
    "            smiles_list, rdkit_mol_objs, labels = \\\n",
    "                _load_hiv_dataset(os.path.join(self.root, 'raw/HIV.csv'))\n",
    "            print('processing')\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                fp_arr = create_circular_fingerprint(rdkit_mol,\n",
    "                                                        self.radius,\n",
    "                                                        self.size,\n",
    "                                                        self.chirality)\n",
    "                fp_arr = torch.tensor(fp_arr)\n",
    "\n",
    "                # manually add mol id\n",
    "                id = torch.tensor([i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                y = torch.tensor([labels[i]])\n",
    "                data_list.append({'fp_arr': fp_arr, 'id': id, 'y': y})\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "        elif self.dataset == 'affinity':\n",
    "            smiles_list, rdkit_mol_objs, labels,sequences_list = \\\n",
    "                _load_affinity_dataset(os.path.join(self.root, 'raw/interaction_protein_smile.csv'))\n",
    "            print('processing')\n",
    "            for i in range(len(smiles_list)):\n",
    "                print(i)\n",
    "                rdkit_mol = rdkit_mol_objs[i]\n",
    "                # # convert aromatic bonds to double bonds\n",
    "                fp_arr = create_circular_fingerprint(rdkit_mol,\n",
    "                                                        self.radius,\n",
    "                                                        self.size,\n",
    "                                                        self.chirality)\n",
    "                fp_arr = torch.tensor(fp_arr)\n",
    "\n",
    "                # manually add mol id\n",
    "                id = torch.tensor([i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                y = torch.tensor([labels[i]])\n",
    "                data_list.append({'fp_arr': fp_arr, 'id': id, 'y': y})\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "                data_sequences_list.append(sequences_list[i])\n",
    "        \n",
    "        elif self.dataset == 'affinity':\n",
    "            #print('+++++++++++++++++++++++++++++')\n",
    "            input_path = self.raw_paths[0]\n",
    "            #input_path = self.raw_paths[1]\n",
    "            #input_df = pd.read_csv(input_path, sep=','，names=['id', 'label','sequence', 'smiles'])\n",
    "            input_df = pd.read_csv(input_path, sep=',')\n",
    "            \n",
    "            codes_list=input_df['code']\n",
    "            smiles_list = input_df['smiles']\n",
    "            labels = input_df['logKa'].values######################dropout\n",
    "            #print('labels:',type(labels),labels[3])\n",
    "            sequences=input_df['sequence']\n",
    "            #print('sequences:',sequences)\n",
    "            #sequences=np.array(sequences)\n",
    "            #sequences=torch.from_numpy(sequences)\n",
    "            #print('sequence:',type(sequences),sequences.shape)\n",
    "            #print('sequences[3]:',sequences[3])\n",
    "            for i in range(len(smiles_list)):\n",
    "                #print(\"============================================================================\")\n",
    "                s = smiles_list[i]\n",
    "                rdkit_mol = AllChem.MolFromSmiles(s)\n",
    "                if rdkit_mol != None:  # ignore invalid mol objects\n",
    "                    # # convert aromatic bonds to double bonds\n",
    "                    # Chem.SanitizeMol(rdkit_mol,\n",
    "                    #                  sanitizeOps=Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "                    #ss=smiles_list[i]\n",
    "                    data = mol_to_graph_data_obj_simple(rdkit_mol)###################转化成了x.data,x.index_edge........\n",
    "                    # manually add mol id\n",
    "                    data.id = torch.tensor(\n",
    "                        [i])\n",
    "                    data.y = torch.tensor([labels[i]])\n",
    "                    #print('data.y:',data.y)\n",
    "                    #print('data.y:',data.y)\n",
    "                    #print('sequences:'sequences)\n",
    "                    sequence=sequences[i]#ValueError: too many dimensions 'str'：不要转化成tensor\n",
    "                    #print('sequence:',type(data.sequence),data.sequence.shape)##############################\n",
    "                    data_list.append(data)\n",
    "                    data_sequence_list.append(sequence)\n",
    "                    #data_list.append(sequence)\n",
    "                    data_smiles_list.append(smiles_list[i])\n",
    "                    \n",
    "\n",
    "        else:\n",
    "            raise ValueError('Invalid dataset name')\n",
    "\n",
    "        # save processed data objects and smiles\n",
    "        processed_dir = os.path.join(self.root, 'processed_fp')\n",
    "        data_smiles_series = pd.Series(data_smiles_list)\n",
    "        data_smiles_series.to_csv(os.path.join(processed_dir, 'smiles.csv'),\n",
    "                                  index=False,\n",
    "                                  header=False)\n",
    "        data_sequences_series.to_csv(os.path.join(processed_dir, 'sequences.csv'),\n",
    "                                  index=False,\n",
    "                                  header=False)\n",
    "        with open(os.path.join(processed_dir,\n",
    "                                    'fingerprint_data_processed.pkl'),\n",
    "                  'wb') as f:\n",
    "            pickle.dump(data_list, f)\n",
    "\n",
    "    def _load(self):\n",
    "        processed_dir = os.path.join(self.root, 'processed_fp')\n",
    "        # check if saved file exist. If so, then load from save\n",
    "        file_name_list = os.listdir(processed_dir)\n",
    "        if 'fingerprint_data_processed.pkl' in file_name_list:\n",
    "            with open(os.path.join(processed_dir,\n",
    "                                   'fingerprint_data_processed.pkl'),\n",
    "                      'rb') as f:\n",
    "                self.data_list = pickle.load(f)\n",
    "        # if no saved file exist, then perform processing steps, save then\n",
    "        # reload\n",
    "        else:\n",
    "            self._process()\n",
    "            self._load()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ## if iterable class is passed, return dataset objection\n",
    "        if hasattr(index, \"__iter__\"):\n",
    "            dataset = MoleculeFingerprintDataset(self.root, self.dataset, self.radius, self.size, chirality=self.chirality)\n",
    "            dataset.data_list = [self.data_list[i] for i in index]\n",
    "            return dataset\n",
    "        else:\n",
    "            return self.data_list[index]\n",
    "\n",
    "\n",
    "def _load_tox21_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    tasks = ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD',\n",
    "       'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']\n",
    "    labels = input_df[tasks]\n",
    "    # convert 0 to -1\n",
    "    labels = labels.replace(0, -1)\n",
    "    # convert nan to 0\n",
    "    labels = labels.fillna(0)\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return smiles_list, rdkit_mol_objs_list, labels.values\n",
    "\n",
    "def _load_hiv_dataset(input_path):\n",
    "    \"\"\"\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    labels = input_df['HIV_active']\n",
    "    # convert 0 to -1\n",
    "    labels = labels.replace(0, -1)\n",
    "    # there are no nans\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return smiles_list, rdkit_mol_objs_list, labels.values\n",
    "\n",
    "def _load_affinity_dataset(input_path):\n",
    "    \"\"\"\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    sequences_list=input_df['sequence']\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    labels = input_df['logKa']#####################dropout\n",
    "    # convert 0 to -1\n",
    "    #labels = labels.replace(0, -1)\n",
    "    # there are no nans\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    assert len(sequences_list) == len(labels)\n",
    "    return smiles_list, rdkit_mol_objs_list, labels.values, sequences_list\n",
    "\n",
    "def _load_bace_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array\n",
    "    containing indices for each of the 3 folds, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['mol']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    labels = input_df['Class']\n",
    "    # convert 0 to -1\n",
    "    labels = labels.replace(0, -1)\n",
    "    # there are no nans\n",
    "    folds = input_df['Model']\n",
    "    folds = folds.replace('Train', 0)   # 0 -> train\n",
    "    folds = folds.replace('Valid', 1)   # 1 -> valid\n",
    "    folds = folds.replace('Test', 2)    # 2 -> test\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    assert len(smiles_list) == len(folds)\n",
    "    return smiles_list, rdkit_mol_objs_list, folds.values, labels.values\n",
    "\n",
    "def _load_bbbp_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "\n",
    "    preprocessed_rdkit_mol_objs_list = [m if m != None else None for m in\n",
    "                                                          rdkit_mol_objs_list]\n",
    "    preprocessed_smiles_list = [AllChem.MolToSmiles(m) if m != None else\n",
    "                                None for m in preprocessed_rdkit_mol_objs_list]\n",
    "    labels = input_df['p_np']\n",
    "    # convert 0 to -1\n",
    "    labels = labels.replace(0, -1)\n",
    "    # there are no nans\n",
    "    assert len(smiles_list) == len(preprocessed_rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(preprocessed_smiles_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return preprocessed_smiles_list, preprocessed_rdkit_mol_objs_list, \\\n",
    "           labels.values\n",
    "\n",
    "def _load_clintox_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "\n",
    "    preprocessed_rdkit_mol_objs_list = [m if m != None else None for m in\n",
    "                                        rdkit_mol_objs_list]\n",
    "    preprocessed_smiles_list = [AllChem.MolToSmiles(m) if m != None else\n",
    "                                None for m in preprocessed_rdkit_mol_objs_list]\n",
    "    tasks = ['FDA_APPROVED', 'CT_TOX']\n",
    "    labels = input_df[tasks]\n",
    "    # convert 0 to -1\n",
    "    labels = labels.replace(0, -1)\n",
    "    # there are no nans\n",
    "    assert len(smiles_list) == len(preprocessed_rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(preprocessed_smiles_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return preprocessed_smiles_list, preprocessed_rdkit_mol_objs_list, \\\n",
    "           labels.values\n",
    "# input_path = 'dataset/clintox/raw/clintox.csv'\n",
    "# smiles_list, rdkit_mol_objs_list, labels = _load_clintox_dataset(input_path)\n",
    "\n",
    "def _load_esol_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels (regression task)\n",
    "    \"\"\"\n",
    "    # NB: some examples have multiple species\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    labels = input_df['measured log solubility in mols per litre']\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return smiles_list, rdkit_mol_objs_list, labels.values\n",
    "# input_path = 'dataset/esol/raw/delaney-processed.csv'\n",
    "# smiles_list, rdkit_mol_objs_list, labels = _load_esol_dataset(input_path)\n",
    "\n",
    "def _load_freesolv_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels (regression task)\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    labels = input_df['expt']\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return smiles_list, rdkit_mol_objs_list, labels.values\n",
    "\n",
    "\n",
    "def _load_lipophilicity_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels (regression task)\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    labels = input_df['exp']\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return smiles_list, rdkit_mol_objs_list, labels.values\n",
    "\n",
    "\n",
    "def _load_muv_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    tasks = ['MUV-466', 'MUV-548', 'MUV-600', 'MUV-644', 'MUV-652', 'MUV-689',\n",
    "       'MUV-692', 'MUV-712', 'MUV-713', 'MUV-733', 'MUV-737', 'MUV-810',\n",
    "       'MUV-832', 'MUV-846', 'MUV-852', 'MUV-858', 'MUV-859']\n",
    "    labels = input_df[tasks]\n",
    "    # convert 0 to -1\n",
    "    labels = labels.replace(0, -1)\n",
    "    # convert nan to 0\n",
    "    labels = labels.fillna(0)\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return smiles_list, rdkit_mol_objs_list, labels.values\n",
    "\n",
    "def _load_sider_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')#####################\n",
    "    print('read_sider_csv')\n",
    "    smiles_list = input_df['smiles']########smiles列\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]#AllChem.MolFromSmiles(s)\n",
    "    print('change smiles to mol')\n",
    "    tasks = ['Hepatobiliary disorders',\n",
    "       'Metabolism and nutrition disorders', 'Product issues', 'Eye disorders',\n",
    "       'Investigations', 'Musculoskeletal and connective tissue disorders',\n",
    "       'Gastrointestinal disorders', 'Social circumstances',\n",
    "       'Immune system disorders', 'Reproductive system and breast disorders',\n",
    "       'Neoplasms benign, malignant and unspecified (incl cysts and polyps)',\n",
    "       'General disorders and administration site conditions',\n",
    "       'Endocrine disorders', 'Surgical and medical procedures',\n",
    "       'Vascular disorders', 'Blood and lymphatic system disorders',\n",
    "       'Skin and subcutaneous tissue disorders',\n",
    "       'Congenital, familial and genetic disorders',\n",
    "       'Infections and infestations',\n",
    "       'Respiratory, thoracic and mediastinal disorders',\n",
    "       'Psychiatric disorders', 'Renal and urinary disorders',\n",
    "       'Pregnancy, puerperium and perinatal conditions',\n",
    "       'Ear and labyrinth disorders', 'Cardiac disorders',\n",
    "       'Nervous system disorders',\n",
    "       'Injury, poisoning and procedural complications']\n",
    "    labels = input_df[tasks]######other columns\n",
    "    \n",
    "    # convert 0 to -1\n",
    "    labels = labels.replace(0, -1)#################\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    #print('labels')\n",
    "    #print('labels:',labels)\n",
    "    labels=np.array(labels)\n",
    "    #print('labels_np:',labels)\n",
    "    #return smiles_list, rdkit_mol_objs_list, labels.value################\n",
    "    return smiles_list, rdkit_mol_objs_list, labels################\n",
    "def _load_toxcast_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    # NB: some examples have multiple species, some example smiles are invalid\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    # Some smiles could not be successfully converted\n",
    "    # to rdkit mol object so them to None\n",
    "    preprocessed_rdkit_mol_objs_list = [m if m != None else None for m in\n",
    "                                        rdkit_mol_objs_list]\n",
    "    preprocessed_smiles_list = [AllChem.MolToSmiles(m) if m != None else\n",
    "                                None for m in preprocessed_rdkit_mol_objs_list]\n",
    "    tasks = list(input_df.columns)[1:]\n",
    "    labels = input_df[tasks]\n",
    "    # convert 0 to -1\n",
    "    labels = labels.replace(0, -1)\n",
    "    # convert nan to 0\n",
    "    labels = labels.fillna(0)\n",
    "    assert len(smiles_list) == len(preprocessed_rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(preprocessed_smiles_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return preprocessed_smiles_list, preprocessed_rdkit_mol_objs_list, \\\n",
    "           labels.values\n",
    "\n",
    "def _load_chembl_with_labels_dataset(root_path):\n",
    "    \"\"\"\n",
    "    Data from 'Large-scale comparison of machine learning methods for drug target prediction on ChEMBL'\n",
    "    :param root_path: path to the folder containing the reduced chembl dataset\n",
    "    :return: list of smiles, preprocessed rdkit mol obj list, list of np.array\n",
    "    containing indices for each of the 3 folds, np.array containing the labels\n",
    "    \"\"\"\n",
    "    # adapted from https://github.com/ml-jku/lsc/blob/master/pythonCode/lstm/loadData.py\n",
    "    # first need to download the files and unzip:\n",
    "    # wget http://bioinf.jku.at/research/lsc/chembl20/dataPythonReduced.zip\n",
    "    # unzip and rename to chembl_with_labels\n",
    "    # wget http://bioinf.jku.at/research/lsc/chembl20/dataPythonReduced/chembl20Smiles.pckl\n",
    "    # into the dataPythonReduced directory\n",
    "    # wget http://bioinf.jku.at/research/lsc/chembl20/dataPythonReduced/chembl20LSTM.pckl\n",
    "\n",
    "    # 1. load folds and labels\n",
    "    f=open(os.path.join(root_path, 'folds0.pckl'), 'rb')\n",
    "    folds=pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    f=open(os.path.join(root_path, 'labelsHard.pckl'), 'rb')\n",
    "    targetMat=pickle.load(f)\n",
    "    sampleAnnInd=pickle.load(f)\n",
    "    targetAnnInd=pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    targetMat=targetMat\n",
    "    targetMat=targetMat.copy().tocsr()\n",
    "    targetMat.sort_indices()\n",
    "    targetAnnInd=targetAnnInd\n",
    "    targetAnnInd=targetAnnInd-targetAnnInd.min()\n",
    "\n",
    "    folds=[np.intersect1d(fold, sampleAnnInd.index.values).tolist() for fold in folds]\n",
    "    targetMatTransposed=targetMat[sampleAnnInd[list(chain(*folds))]].T.tocsr()\n",
    "    targetMatTransposed.sort_indices()\n",
    "    # # num positive examples in each of the 1310 targets\n",
    "    trainPosOverall=np.array([np.sum(targetMatTransposed[x].data > 0.5) for x in range(targetMatTransposed.shape[0])])\n",
    "    # # num negative examples in each of the 1310 targets\n",
    "    trainNegOverall=np.array([np.sum(targetMatTransposed[x].data < -0.5) for x in range(targetMatTransposed.shape[0])])\n",
    "    # dense array containing the labels for the 456331 molecules and 1310 targets\n",
    "    denseOutputData=targetMat.A # possible values are {-1, 0, 1}\n",
    "\n",
    "    # 2. load structures\n",
    "    f=open(os.path.join(root_path, 'chembl20LSTM.pckl'), 'rb')\n",
    "    rdkitArr=pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    assert len(rdkitArr) == denseOutputData.shape[0]\n",
    "    assert len(rdkitArr) == len(folds[0]) + len(folds[1]) + len(folds[2])\n",
    "\n",
    "    preprocessed_rdkitArr = []\n",
    "    print('preprocessing')\n",
    "    for i in range(len(rdkitArr)):\n",
    "        print(i)\n",
    "        m = rdkitArr[i]\n",
    "        if m == None:\n",
    "            preprocessed_rdkitArr.append(None)\n",
    "        else:\n",
    "            mol_species_list = split_rdkit_mol_obj(m)\n",
    "            if len(mol_species_list) == 0:\n",
    "                preprocessed_rdkitArr.append(None)\n",
    "            else:\n",
    "                largest_mol = get_largest_mol(mol_species_list)\n",
    "                if len(largest_mol.GetAtoms()) <= 2:\n",
    "                    preprocessed_rdkitArr.append(None)\n",
    "                else:\n",
    "                    preprocessed_rdkitArr.append(largest_mol)\n",
    "\n",
    "    assert len(preprocessed_rdkitArr) == denseOutputData.shape[0]\n",
    "\n",
    "    smiles_list = [AllChem.MolToSmiles(m) if m != None else None for m in\n",
    "                   preprocessed_rdkitArr]   # bc some empty mol in the\n",
    "    # rdkitArr zzz...\n",
    "\n",
    "    assert len(preprocessed_rdkitArr) == len(smiles_list)\n",
    "\n",
    "    return smiles_list, preprocessed_rdkitArr, folds, denseOutputData\n",
    "# root_path = 'dataset/chembl_with_labels'\n",
    "\n",
    "def check_smiles_validity(smiles):\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(smiles)\n",
    "        if m:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def split_rdkit_mol_obj(mol):\n",
    "    \"\"\"\n",
    "    Split rdkit mol object containing multiple species or one species into a\n",
    "    list of mol objects or a list containing a single object respectively\n",
    "    :param mol:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    smiles = AllChem.MolToSmiles(mol, isomericSmiles=True)\n",
    "    smiles_list = smiles.split('.')\n",
    "    mol_species_list = []\n",
    "    for s in smiles_list:\n",
    "        if check_smiles_validity(s):\n",
    "            mol_species_list.append(AllChem.MolFromSmiles(s))\n",
    "    return mol_species_list\n",
    "\n",
    "def get_largest_mol(mol_list):\n",
    "    \"\"\"\n",
    "    Given a list of rdkit mol objects, returns mol object containing the\n",
    "    largest num of atoms. If multiple containing largest num of atoms,\n",
    "    picks the first one\n",
    "    :param mol_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_atoms_list = [len(m.GetAtoms()) for m in mol_list]\n",
    "    largest_mol_idx = num_atoms_list.index(max(num_atoms_list))\n",
    "    return mol_list[largest_mol_idx]\n",
    "\n",
    "def create_all_datasets():\n",
    "    #### create dataset\n",
    "    downstream_dir = [\n",
    "            'bace',\n",
    "            'bbbp',\n",
    "            'clintox',\n",
    "            'esol',\n",
    "            'freesolv',\n",
    "            'hiv',\n",
    "            'lipophilicity',\n",
    "            'muv',\n",
    "            'sider',\n",
    "            'tox21',\n",
    "            'toxcast'\n",
    "            ]\n",
    "\n",
    "    for dataset_name in downstream_dir:\n",
    "        print(dataset_name)\n",
    "        root = \"dataset/\" + dataset_name\n",
    "        os.makedirs(root + \"/processed\", exist_ok=True)\n",
    "        dataset = MoleculeDataset(root, dataset=dataset_name)\n",
    "        print(dataset)\n",
    "\n",
    "\n",
    "    dataset = MoleculeDataset(root = \"dataset/chembl_filtered\", dataset=\"chembl_filtered\")\n",
    "    print(dataset)\n",
    "    dataset = MoleculeDataset(root = \"dataset/zinc_standard_agent\", dataset=\"zinc_standard_agent\")\n",
    "    print(dataset)\n",
    "\n",
    "\n",
    "# test MoleculeDataset object\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    create_all_datasets()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2eedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
