{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d94f30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not from scratch\n",
      "rese:model_gin\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n",
      "raw_dir: dataset/affinity/raw\n",
      "root: dataset/affinity\n",
      "root: dataset/affinity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [16:28:33] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "[16:28:33] Explicit valence for atom # 16 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:33] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 12 13 14\n",
      "RDKit ERROR: \n",
      "[16:28:33] Can't kekulize mol.  Unkekulized atoms: 7 9 10 11 12 13 14\n",
      "\n",
      "RDKit ERROR: [16:28:33] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:33] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:33] Explicit valence for atom # 30 N, 4, is greater than permitted\n",
      "[16:28:33] Explicit valence for atom # 30 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:33] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[16:28:33] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:33] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:33] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:34] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "[16:28:34] Explicit valence for atom # 21 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:34] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[16:28:34] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:34] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:34] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:34] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[16:28:34] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:34] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[16:28:34] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:34] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:34] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:35] Can't kekulize mol.  Unkekulized atoms: 8 10 11 13 15\n",
      "RDKit ERROR: \n",
      "[16:28:35] Can't kekulize mol.  Unkekulized atoms: 8 10 11 13 15\n",
      "\n",
      "RDKit ERROR: [16:28:35] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[16:28:35] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [16:28:35] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "[16:28:35] Explicit valence for atom # 4 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:35] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[16:28:35] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [16:28:35] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[16:28:35] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:35] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[16:28:35] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:35] Can't kekulize mol.  Unkekulized atoms: 9 13 15 16 21\n",
      "RDKit ERROR: \n",
      "[16:28:35] Can't kekulize mol.  Unkekulized atoms: 9 13 15 16 21\n",
      "\n",
      "RDKit ERROR: [16:28:35] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "RDKit ERROR: \n",
      "[16:28:35] Can't kekulize mol.  Unkekulized atoms: 15\n",
      "\n",
      "RDKit ERROR: [16:28:36] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[16:28:36] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:36] Can't kekulize mol.  Unkekulized atoms: 3 5 7\n",
      "RDKit ERROR: \n",
      "[16:28:36] Can't kekulize mol.  Unkekulized atoms: 3 5 7\n",
      "\n",
      "RDKit ERROR: [16:28:36] Can't kekulize mol.  Unkekulized atoms: 35 37 38 39 40\n",
      "RDKit ERROR: \n",
      "[16:28:36] Can't kekulize mol.  Unkekulized atoms: 35 37 38 39 40\n",
      "\n",
      "RDKit ERROR: [16:28:36] Explicit valence for atom # 36 N, 4, is greater than permitted\n",
      "[16:28:36] Explicit valence for atom # 36 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:36] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[16:28:36] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:36] Explicit valence for atom # 31 N, 4, is greater than permitted\n",
      "[16:28:36] Explicit valence for atom # 31 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:36] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "RDKit ERROR: \n",
      "[16:28:36] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "\n",
      "RDKit ERROR: [16:28:36] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:36] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:37] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:37] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:37] Can't kekulize mol.  Unkekulized atoms: 22 24 25 26 27\n",
      "RDKit ERROR: \n",
      "[16:28:37] Can't kekulize mol.  Unkekulized atoms: 22 24 25 26 27\n",
      "\n",
      "RDKit ERROR: [16:28:37] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "RDKit ERROR: \n",
      "[16:28:37] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "\n",
      "RDKit ERROR: [16:28:37] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "[16:28:37] Explicit valence for atom # 15 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:38] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "[16:28:38] Explicit valence for atom # 27 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:38] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "RDKit ERROR: \n",
      "[16:28:38] Can't kekulize mol.  Unkekulized atoms: 22 24 25 27 29\n",
      "\n",
      "RDKit ERROR: [16:28:38] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:38] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:38] Can't kekulize mol.  Unkekulized atoms: 8 11 12\n",
      "RDKit ERROR: \n",
      "[16:28:38] Can't kekulize mol.  Unkekulized atoms: 8 11 12\n",
      "\n",
      "RDKit ERROR: [16:28:38] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:38] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:38] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[16:28:38] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:38] Can't kekulize mol.  Unkekulized atoms: 22 23 25\n",
      "RDKit ERROR: \n",
      "[16:28:38] Can't kekulize mol.  Unkekulized atoms: 22 23 25\n",
      "\n",
      "RDKit ERROR: [16:28:38] Can't kekulize mol.  Unkekulized atoms: 9 10 12\n",
      "RDKit ERROR: \n",
      "[16:28:38] Can't kekulize mol.  Unkekulized atoms: 9 10 12\n",
      "\n",
      "RDKit ERROR: [16:28:38] Can't kekulize mol.  Unkekulized atoms: 11 16 17\n",
      "RDKit ERROR: \n",
      "[16:28:38] Can't kekulize mol.  Unkekulized atoms: 11 16 17\n",
      "\n",
      "RDKit ERROR: [16:28:39] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[16:28:39] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:39] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:39] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:39] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "[16:28:39] Explicit valence for atom # 18 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:39] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "RDKit ERROR: \n",
      "[16:28:39] Can't kekulize mol.  Unkekulized atoms: 4 6 7\n",
      "\n",
      "RDKit ERROR: [16:28:39] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:39] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n",
      "RDKit ERROR: [16:28:40] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "RDKit ERROR: \n",
      "[16:28:40] Can't kekulize mol.  Unkekulized atoms: 18 20 21 23 25\n",
      "\n",
      "RDKit ERROR: [16:28:40] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "[16:28:40] Explicit valence for atom # 1 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:41] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "[16:28:41] Explicit valence for atom # 2 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:41] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "RDKit ERROR: \n",
      "[16:28:41] Can't kekulize mol.  Unkekulized atoms: 10 12 13\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "RDKit ERROR: \n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "RDKit ERROR: \n",
      "[16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "[16:28:43] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "RDKit ERROR: \n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n",
      "raw_dir: dataset/affinity/test/raw\n",
      "root: dataset/affinity/test\n",
      "root: dataset/affinity/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 28 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 41 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 20 S, 7, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6,RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "RDKit ERROR: \n",
      " is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 13\n",
      "\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 8 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 2 29 30\n",
      "\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 10 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 13 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "RDKit ERROR: \n",
      "[16:28:43] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 0 1 4 7 8 9 10 14\n",
      "\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 24 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 17 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 2 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 34 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 22 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 21 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 12 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 26 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 52 C, 5, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 31 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 19 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 15 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 17 S, 7, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 32 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 24 C, 5, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 5 C, 6, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 12\n",
      "\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 19 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 19 S, 7, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 25 C, 6, is greater than permitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 20 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 25 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 7 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "RDKit ERROR: \n",
      "[16:28:43] Explicit valence for atom # 9 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 6 N, 4, is greater than permitted\n",
      "[16:28:43] Can't kekulize mol.  Unkekulized atoms: 12 13 14 15 16\n",
      "\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "RDKit ERROR: [16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 11 C, 6, is greater than permitted\n",
      "[16:28:43] Explicit valence for atom # 16 C, 6, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====epoch 1\n",
      "====Evaluation\n",
      "train: 754.940784 val: 869.573616 test: 536158.088314\n",
      "====epoch 2\n",
      "====Evaluation\n",
      "train: 891.872770 val: 846.615242 test: 515366.017952\n",
      "====epoch 3\n",
      "====Evaluation\n",
      "train: 4510.351265 val: 5074.931221 test: 13350943.842883\n",
      "====epoch 4\n",
      "====Evaluation\n",
      "train: 1687.788593 val: 1962.758003 test: 1776963.994361\n",
      "====epoch 5\n",
      "====Evaluation\n",
      "train: 1277.398324 val: 1313.954902 test: 918440.561844\n",
      "====epoch 6\n",
      "====Evaluation\n",
      "train: 139.191284 val: 145.298422 test: 16017.280905\n",
      "====epoch 7\n",
      "====Evaluation\n",
      "train: 753.164443 val: 801.657128 test: 349317.064844\n",
      "====epoch 8\n",
      "====Evaluation\n",
      "train: 496.293202 val: 544.037453 test: 152796.737342\n",
      "====epoch 9\n",
      "training Epoch [10/50], Loss: 398.7488\n",
      "====Evaluation\n",
      "train: 66.193565 val: 76.322569 test: 2088.584063\n",
      "====epoch 10\n",
      "====Evaluation\n",
      "train: 44.233861 val: 56.197813 test: 1224.381849\n",
      "====epoch 11\n",
      "====Evaluation\n",
      "train: 27.237556 val: 33.940355 test: 372.489544\n",
      "====epoch 12\n",
      "====Evaluation\n",
      "train: 13.409183 val: 17.389399 test: 103.502671\n",
      "====epoch 13\n",
      "====Evaluation\n",
      "train: 10.509213 val: 14.564011 test: 68.598428\n",
      "====epoch 14\n",
      "====Evaluation\n",
      "train: 10.928958 val: 14.991915 test: 70.934215\n",
      "====epoch 15\n",
      "====Evaluation\n",
      "train: 8.312499 val: 11.570143 test: 42.846111\n",
      "====epoch 16\n",
      "====Evaluation\n",
      "train: 6.104238 val: 9.137660 test: 28.171257\n",
      "====epoch 17\n",
      "====Evaluation\n",
      "train: 5.181876 val: 9.076876 test: 27.529703\n",
      "====epoch 18\n",
      "====Evaluation\n",
      "train: 5.116477 val: 9.011477 test: 26.847701\n",
      "====epoch 19\n",
      "training Epoch [20/50], Loss: 5.1165\n",
      "====Evaluation\n",
      "train: 5.047077 val: 8.942077 test: 26.133336\n",
      "====epoch 20\n",
      "====Evaluation\n",
      "train: 4.974195 val: 8.869195 test: 25.393481\n",
      "====epoch 21\n",
      "====Evaluation\n",
      "train: 4.898267 val: 8.793267 test: 24.634017\n",
      "====epoch 22\n",
      "====Evaluation\n",
      "train: 4.819669 val: 8.714669 test: 23.859989\n",
      "====epoch 23\n",
      "====Evaluation\n",
      "train: 4.738722 val: 8.633722 test: 23.075749\n",
      "====epoch 24\n",
      "====Evaluation\n",
      "train: 4.655706 val: 8.550706 test: 22.285062\n",
      "====epoch 25\n",
      "====Evaluation\n",
      "train: 4.570859 val: 8.465859 test: 21.491190\n",
      "====epoch 26\n",
      "====Evaluation\n",
      "train: 4.484393 val: 8.379393 test: 20.696973\n",
      "====epoch 27\n",
      "====Evaluation\n",
      "train: 4.396489 val: 8.291489 test: 19.904886\n",
      "====epoch 28\n",
      "====Evaluation\n",
      "train: 4.307309 val: 8.202309 test: 19.117089\n",
      "====epoch 29\n",
      "training Epoch [30/50], Loss: 4.3872\n",
      "====Evaluation\n",
      "train: 4.216994 val: 8.111994 test: 18.335472\n",
      "====epoch 30\n",
      "====Evaluation\n",
      "train: 4.125667 val: 8.020667 test: 17.561692\n",
      "====epoch 31\n",
      "====Evaluation\n",
      "train: 4.033439 val: 7.928439 test: 16.797201\n",
      "====epoch 32\n",
      "====Evaluation\n",
      "train: 3.940406 val: 7.835406 test: 16.043274\n",
      "====epoch 33\n",
      "====Evaluation\n",
      "train: 3.847504 val: 7.741654 test: 15.301033\n",
      "====epoch 34\n",
      "====Evaluation\n",
      "train: 3.752259 val: 7.647259 test: 14.571464\n",
      "====epoch 35\n",
      "====Evaluation\n",
      "train: 3.657289 val: 7.552289 test: 13.855435\n",
      "====epoch 36\n",
      "====Evaluation\n",
      "train: 3.561805 val: 7.456805 test: 13.153711\n",
      "====epoch 37\n",
      "====Evaluation\n",
      "train: 3.465859 val: 7.360859 test: 12.466968\n",
      "====epoch 38\n",
      "====Evaluation\n",
      "train: 3.369501 val: 7.264501 test: 11.795797\n",
      "====epoch 39\n",
      "training Epoch [40/50], Loss: 3.3695\n",
      "====Evaluation\n",
      "train: 3.272772 val: 7.167772 test: 11.140725\n",
      "====epoch 40\n",
      "====Evaluation\n",
      "train: 3.175712 val: 7.070712 test: 10.502214\n",
      "====epoch 41\n",
      "====Evaluation\n",
      "train: 3.078353 val: 6.973353 test: 9.880671\n",
      "====epoch 42\n",
      "====Evaluation\n",
      "train: 2.980728 val: 6.875728 test: 9.276457\n",
      "====epoch 43\n",
      "====Evaluation\n",
      "train: 2.882862 val: 6.777862 test: 8.689890\n",
      "====epoch 44\n",
      "====Evaluation\n",
      "train: 2.784781 val: 6.679781 test: 8.121249\n",
      "====epoch 45\n",
      "====Evaluation\n",
      "train: 2.686506 val: 6.581506 test: 7.570783\n",
      "====epoch 46\n",
      "====Evaluation\n",
      "train: 2.588550 val: 6.483057 test: 7.038711\n",
      "====epoch 47\n",
      "====Evaluation\n",
      "train: 2.492119 val: 6.384554 test: 6.525745\n",
      "====epoch 48\n",
      "====Evaluation\n",
      "train: 2.397382 val: 6.286131 test: 6.032582\n",
      "====epoch 49\n",
      "training Epoch [50/50], Loss: 2.3974\n",
      "====Evaluation\n",
      "train: 2.304449 val: 6.187911 test: 5.559745\n",
      "====epoch 50\n",
      "====Evaluation\n",
      "train: 2.213409 val: 6.090001 test: 5.107603\n",
      "====epoch 51\n",
      "====Evaluation\n",
      "train: 2.124335 val: 5.992497 test: 4.676393\n",
      "====epoch 52\n",
      "====Evaluation\n",
      "train: 2.037284 val: 5.895486 test: 4.266233\n",
      "====epoch 53\n",
      "====Evaluation\n",
      "train: 1.952300 val: 5.799045 test: 3.877140\n",
      "====epoch 54\n",
      "====Evaluation\n",
      "train: 1.869413 val: 5.703244 test: 3.509041\n",
      "====epoch 55\n",
      "====Evaluation\n",
      "train: 1.788645 val: 5.608142 test: 3.161789\n",
      "====epoch 56\n",
      "====Evaluation\n",
      "train: 1.710007 val: 5.513795 test: 2.835167\n",
      "====epoch 57\n",
      "====Evaluation\n",
      "train: 1.633505 val: 5.420253 test: 2.528904\n",
      "====epoch 58\n",
      "====Evaluation\n",
      "train: 1.559134 val: 5.327558 test: 2.242679\n",
      "====epoch 59\n",
      "training Epoch [60/50], Loss: 1.5591\n",
      "====Evaluation\n",
      "train: 1.486885 val: 5.235748 test: 1.976128\n",
      "====epoch 60\n",
      "====Evaluation\n",
      "train: 1.416744 val: 5.144859 test: 1.728853\n",
      "====epoch 61\n",
      "====Evaluation\n",
      "train: 1.348691 val: 5.054919 test: 1.500427\n",
      "====epoch 62\n",
      "====Evaluation\n",
      "train: 1.282704 val: 4.965956 test: 1.290396\n",
      "====epoch 63\n",
      "====Evaluation\n",
      "train: 1.218755 val: 4.877993 test: 1.098289\n",
      "====epoch 64\n",
      "====Evaluation\n",
      "train: 1.156816 val: 4.791050 test: 0.923618\n",
      "====epoch 65\n",
      "====Evaluation\n",
      "train: 1.097203 val: 4.705146 test: 0.765880\n",
      "====epoch 66\n",
      "====Evaluation\n",
      "train: 1.040767 val: 4.620397 test: 0.624727\n",
      "====epoch 67\n",
      "====Evaluation\n",
      "train: 0.987502 val: 4.536942 test: 0.499767\n",
      "====epoch 68\n",
      "====Evaluation\n",
      "train: 0.937288 val: 4.454890 test: 0.390487\n",
      "====epoch 69\n",
      "training Epoch [70/50], Loss: 0.9373\n",
      "====Evaluation\n",
      "train: 0.890258 val: 4.374350 test: 0.296317\n",
      "====epoch 70\n",
      "====Evaluation\n",
      "train: 0.847810 val: 4.295492 test: 0.216683\n",
      "====epoch 71\n",
      "====Evaluation\n",
      "train: 0.810440 val: 4.218659 test: 0.151056\n",
      "====epoch 72\n",
      "====Evaluation\n",
      "train: 0.777962 val: 4.144159 test: 0.098696\n",
      "====epoch 73\n",
      "====Evaluation\n",
      "train: 0.750128 val: 4.072263 test: 0.058691\n",
      "====epoch 74\n",
      "====Evaluation\n",
      "train: 0.726638 val: 4.003205 test: 0.030000\n",
      "====epoch 75\n",
      "====Evaluation\n",
      "train: 0.707154 val: 3.937188 test: 0.011489\n",
      "====epoch 76\n",
      "====Evaluation\n",
      "train: 0.691315 val: 3.874381 test: 0.001970\n",
      "====epoch 77\n",
      "====Evaluation\n",
      "train: 0.678744 val: 3.814924 test: 0.000227\n",
      "====epoch 78\n",
      "====Evaluation\n",
      "train: 0.669059 val: 3.758924 test: 0.005052\n",
      "====epoch 79\n",
      "training Epoch [80/50], Loss: 0.6691\n",
      "====Evaluation\n",
      "train: 0.661882 val: 3.706461 test: 0.015262\n",
      "====epoch 80\n",
      "====Evaluation\n",
      "train: 0.656847 val: 3.657588 test: 0.029726\n",
      "====epoch 81\n",
      "====Evaluation\n",
      "train: 0.653605 val: 3.612331 test: 0.047380\n",
      "====epoch 82\n",
      "====Evaluation\n",
      "train: 0.651828 val: 3.570691 test: 0.067241\n",
      "====epoch 83\n",
      "====Evaluation\n",
      "train: 0.651215 val: 3.532645 test: 0.088420\n",
      "====epoch 84\n",
      "====Evaluation\n",
      "train: 0.651493 val: 3.498150 test: 0.110124\n",
      "====epoch 85\n",
      "====Evaluation\n",
      "train: 0.652421 val: 3.467142 test: 0.131666\n",
      "====epoch 86\n",
      "====Evaluation\n",
      "train: 0.653785 val: 3.439539 test: 0.152460\n",
      "====epoch 87\n",
      "====Evaluation\n",
      "train: 0.655407 val: 3.415241 test: 0.172025\n",
      "====epoch 88\n",
      "====Evaluation\n",
      "train: 0.657135 val: 3.394136 test: 0.189978\n",
      "====epoch 89\n",
      "training Epoch [90/50], Loss: 0.6571\n",
      "====Evaluation\n",
      "train: 0.658847 val: 3.376096 test: 0.206029\n",
      "====epoch 90\n",
      "====Evaluation\n",
      "train: 0.660449 val: 3.360983 test: 0.219977\n",
      "====epoch 91\n",
      "====Evaluation\n",
      "train: 0.661869 val: 3.348651 test: 0.231697\n",
      "====epoch 92\n",
      "====Evaluation\n",
      "train: 0.663057 val: 3.338943 test: 0.241137\n",
      "====epoch 93\n",
      "====Evaluation\n",
      "train: 0.663986 val: 3.331700 test: 0.248303\n",
      "====epoch 94\n",
      "====Evaluation\n",
      "train: 0.664639 val: 3.326754 test: 0.253257\n",
      "====epoch 95\n",
      "====Evaluation\n",
      "train: 0.665019 val: 3.323938 test: 0.256099\n",
      "====epoch 96\n",
      "====Evaluation\n",
      "train: 0.665135 val: 3.323081 test: 0.256967\n",
      "====epoch 97\n",
      "====Evaluation\n",
      "train: 0.665009 val: 3.324013 test: 0.256023\n",
      "====epoch 98\n",
      "====Evaluation\n",
      "train: 0.664665 val: 3.326565 test: 0.253447\n",
      "====epoch 99\n",
      "training Epoch [100/50], Loss: 0.6647\n",
      "====Evaluation\n",
      "train: 0.664134 val: 3.330569 test: 0.249432\n",
      "====epoch 100\n",
      "====Evaluation\n",
      "train: 0.663448 val: 3.335861 test: 0.244173\n",
      "====epoch 101\n",
      "====Evaluation\n",
      "train: 0.662641 val: 3.342283 test: 0.237868\n",
      "====epoch 102\n",
      "====Evaluation\n",
      "train: 0.661746 val: 3.349678 test: 0.230709\n",
      "====epoch 103\n",
      "====Evaluation\n",
      "train: 0.660794 val: 3.357899 test: 0.222879\n",
      "====epoch 104\n",
      "====Evaluation\n",
      "train: 0.659814 val: 3.366803 test: 0.214551\n",
      "====epoch 105\n",
      "====Evaluation\n",
      "train: 0.658831 val: 3.376253 test: 0.205886\n",
      "====epoch 106\n",
      "====Evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.657869 val: 3.386123 test: 0.197027\n",
      "====epoch 107\n",
      "====Evaluation\n",
      "train: 0.656945 val: 3.396291 test: 0.188104\n",
      "====epoch 108\n",
      "====Evaluation\n",
      "train: 0.656075 val: 3.406644 test: 0.179231\n",
      "====epoch 109\n",
      "training Epoch [110/50], Loss: 0.6561\n",
      "====Evaluation\n",
      "train: 0.655271 val: 3.417078 test: 0.170504\n",
      "====epoch 110\n",
      "====Evaluation\n",
      "train: 0.654540 val: 3.427498 test: 0.162008\n",
      "====epoch 111\n",
      "====Evaluation\n",
      "train: 0.653887 val: 3.437816 test: 0.153808\n",
      "====epoch 112\n",
      "====Evaluation\n",
      "train: 0.653316 val: 3.447953 test: 0.145960\n",
      "====epoch 113\n",
      "====Evaluation\n",
      "train: 0.652824 val: 3.457837 test: 0.138505\n",
      "====epoch 114\n",
      "====Evaluation\n",
      "train: 0.652410 val: 3.467408 test: 0.131473\n",
      "====epoch 115\n",
      "====Evaluation\n",
      "train: 0.652070 val: 3.476609 test: 0.124885\n",
      "====epoch 116\n",
      "====Evaluation\n",
      "train: 0.651797 val: 3.485394 test: 0.118753\n",
      "====epoch 117\n",
      "====Evaluation\n",
      "train: 0.651587 val: 3.493724 test: 0.113081\n",
      "====epoch 118\n",
      "====Evaluation\n",
      "train: 0.651430 val: 3.501568 test: 0.107868\n",
      "====epoch 119\n",
      "training Epoch [120/50], Loss: 0.6514\n",
      "====Evaluation\n",
      "train: 0.651322 val: 3.508899 test: 0.103106\n",
      "====epoch 120\n",
      "====Evaluation\n",
      "train: 0.651253 val: 3.515700 test: 0.098784\n",
      "====epoch 121\n",
      "====Evaluation\n",
      "train: 0.651216 val: 3.521959 test: 0.094889\n",
      "====epoch 122\n",
      "====Evaluation\n",
      "train: 0.651206 val: 3.527668 test: 0.091404\n",
      "====epoch 123\n",
      "====Evaluation\n",
      "train: 0.651216 val: 3.532828 test: 0.088311\n",
      "====epoch 124\n",
      "====Evaluation\n",
      "train: 0.651239 val: 3.537441 test: 0.085591\n",
      "====epoch 125\n",
      "====Evaluation\n",
      "train: 0.651272 val: 3.541515 test: 0.083223\n",
      "====epoch 126\n",
      "====Evaluation\n",
      "train: 0.651309 val: 3.545064 test: 0.081189\n",
      "====epoch 127\n",
      "====Evaluation\n",
      "train: 0.651348 val: 3.548102 test: 0.079466\n",
      "====epoch 128\n",
      "====Evaluation\n",
      "train: 0.651385 val: 3.550649 test: 0.078037\n",
      "====epoch 129\n",
      "training Epoch [130/50], Loss: 0.6514\n",
      "====Evaluation\n",
      "train: 0.651418 val: 3.552727 test: 0.076881\n",
      "====epoch 130\n",
      "====Evaluation\n",
      "train: 0.651447 val: 3.554358 test: 0.075978\n",
      "====epoch 131\n",
      "====Evaluation\n",
      "train: 0.651469 val: 3.555570 test: 0.075312\n",
      "====epoch 132\n",
      "====Evaluation\n",
      "train: 0.651484 val: 3.556390 test: 0.074862\n",
      "====epoch 133\n",
      "====Evaluation\n",
      "train: 0.651493 val: 3.556846 test: 0.074613\n",
      "====epoch 134\n",
      "====Evaluation\n",
      "train: 0.651496 val: 3.556968 test: 0.074546\n",
      "====epoch 135\n",
      "====Evaluation\n",
      "train: 0.651492 val: 3.556785 test: 0.074646\n",
      "====epoch 136\n",
      "====Evaluation\n",
      "train: 0.651483 val: 3.556328 test: 0.074896\n",
      "====epoch 137\n",
      "====Evaluation\n",
      "train: 0.651470 val: 3.555627 test: 0.075281\n",
      "====epoch 138\n",
      "====Evaluation\n",
      "train: 0.651453 val: 3.554710 test: 0.075784\n",
      "====epoch 139\n",
      "training Epoch [140/50], Loss: 0.6515\n",
      "====Evaluation\n",
      "train: 0.651433 val: 3.553608 test: 0.076393\n",
      "====epoch 140\n",
      "====Evaluation\n",
      "train: 0.651412 val: 3.552347 test: 0.077091\n",
      "====epoch 141\n",
      "====Evaluation\n",
      "train: 0.651390 val: 3.550956 test: 0.077866\n",
      "====epoch 142\n",
      "====Evaluation\n",
      "train: 0.651367 val: 3.549459 test: 0.078703\n",
      "====epoch 143\n",
      "====Evaluation\n",
      "train: 0.651345 val: 3.547882 test: 0.079590\n",
      "====epoch 144\n",
      "====Evaluation\n",
      "train: 0.651323 val: 3.546248 test: 0.080515\n",
      "====epoch 145\n",
      "====Evaluation\n",
      "train: 0.651303 val: 3.544579 test: 0.081465\n",
      "====epoch 146\n",
      "====Evaluation\n",
      "train: 0.651285 val: 3.542894 test: 0.082430\n",
      "====epoch 147\n",
      "====Evaluation\n",
      "train: 0.651269 val: 3.541211 test: 0.083399\n",
      "====epoch 148\n",
      "====Evaluation\n",
      "train: 0.651255 val: 3.539549 test: 0.084362\n",
      "====epoch 149\n",
      "training Epoch [150/50], Loss: 0.6513\n",
      "====Evaluation\n",
      "train: 0.651242 val: 3.537921 test: 0.085310\n",
      "====epoch 150\n",
      "====Evaluation\n",
      "train: 0.651232 val: 3.536341 test: 0.086236\n"
     ]
    }
   ],
   "source": [
    "from loader1 import MoleculeDatasetBig, SeqDataset,SeqMolDataset,SmileDataset#########################\n",
    "import torch\n",
    "import torch\n",
    "#import args\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "#from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from splitters import scaffold_split,scaffold_split_1\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "import esm\n",
    "\n",
    "#from SeqMolModel import InteractionModel,InteractionModel_1,SequenceModel,InteractionModel_4\n",
    "#from SeqMolSmile import InteractionModel_4\n",
    "#from SeqMolModel import InteractionModel_4\n",
    "from SeqMolSmile_model2 import InteractionModel_4\n",
    "print(torch.cuda.is_available())\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda._initialized = True\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')#0000\n",
    "parser.add_argument('--batch_size', type=int, default=64,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=150,\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "parser.add_argument('--decay', type=float, default=0,\n",
    "                        help='weight decay (default: 0)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "parser.add_argument('--dataset', type=str, default = 'affinity', help='root directory of dataset. For now, only classification.')\n",
    "#parser.add_argument('--input_model_file', type=str, default = 'None', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--input_model_file', type=str, default = 'Mole-BERT', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--filename', type=str, default = '', help='output filename')\n",
    "parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "parser.add_argument('--runseed', type=int, default=0, help = \"Seed for minibatch selection, random initialization.\")\n",
    "parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "parser.add_argument('--eval_train', type=int, default = 1, help='evaluating training or not')\n",
    "parser.add_argument('--num_workers', type=int, default = 4, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default = 12, help='number of workers for dataset loading')\n",
    "\n",
    "parser.add_argument('--n_layer', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--d_dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--n_embd', type=int, default = 768, help='number of workers for dataset loading')\n",
    "parser.add_argument('--dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--lr_start', type=float, default =  3e-5, help='number of workers for dataset loading')\n",
    "parser.add_argument('--max_epochs', type=int, default = 500, help='number of workers for dataset loading')\n",
    "parser.add_argument('--num_feats', type=int, default = 32, help='number of workers for dataset loading')\n",
    "parser.add_argument('--checkpoint_every', type=int, default = 100, help='number of workers for dataset loading')\n",
    "parser.add_argument('--seed_path', type=str, default =  'data/checkpoints/N-Step-Checkpoint_3_30000.ckpt', help='number of workers for dataset loading')\n",
    "parser.add_argument('--dims', type=list, default = [ 768, 768, 768, 1], help='number of workers for dataset loading')\n",
    "\n",
    "args = parser.parse_args(args=[])###############33\n",
    "\n",
    "\n",
    "#load pretained model of Mole-Bert\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.runseed)\n",
    "\n",
    "num_tasks=1\n",
    "# Load ESM-2 model\n",
    "protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "protein_model.to(device)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "#print(protein_alphabet)\n",
    "#alphabet = esm.Alphabet.from_architecture(model_data[\"args\"].arch)\n",
    "#batch_converter = alphabet.get_batch_converter()\n",
    "protein_model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "#self.molecular.model,self.molecular.node_representation,self.molecular.features = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "molecular_model = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "###################################\n",
    "if not args.input_model_file == \"None\":###############\n",
    "    print('Not from scratch')\n",
    "    molecular_model.from_pretrained('model_gin/{}.pth'.format(args.input_model_file))\n",
    "    print('rese:model_gin')\n",
    "molecular_model.to(device)\n",
    "for i,p in enumerate(molecular_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "#freezing parameters\n",
    "for i,p in enumerate(protein_model.parameters()):\n",
    "    p.requires_grad = False#freezing parameters\n",
    "\n",
    "'''\n",
    "model_param_group = []\n",
    "model_param_group.append({\"params\": molecular_model.gnn.parameters()})\n",
    "if args.graph_pooling == \"attention\":\n",
    "    model_param_group.append({\"params\": molecular_model.pool.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "'''\n",
    "'''\n",
    "\n",
    "CSDNDreamcatcherCC 4.0 BY-SA\n",
    "https://blog.csdn.net/Wind_2028/article/details/120541017   \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "model_param_group.append({\"params\": molecular_model.graph_pred_linear.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "optimizer = optim.Adam(model_param_group, lr=0.01, weight_decay=args.decay)#############\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
    "#from finetune.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "#from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
    "from functools import partial\n",
    "from apex import optimizers\n",
    "import subprocess\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "#from utils import normalize_smiles\n",
    "import sys\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "from tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "#from utils import normalize_smiles\n",
    "# create a function (this my favorite choice)\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(LightningModule, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        #self.hparams = config\n",
    "        #self.mode = config.mode\n",
    "        self.save_hyperparameters(config)\n",
    "        self.tokenizer=tokenizer\n",
    "        '''\n",
    "        self.min_loss = {\n",
    "            self.hparams.measure_name + \"min_valid_loss\": torch.finfo(torch.float32).max,\n",
    "            self.hparams.measure_name + \"min_epoch\": 0,\n",
    "        }\n",
    "        '''\n",
    "        # Word embeddings layer\n",
    "        n_vocab, d_emb = len(tokenizer.vocab), config.n_embd\n",
    "        # input embedding stem\n",
    "        \n",
    "        builder = rotate_builder.from_kwargs(\n",
    "            n_layers=config.n_layer,\n",
    "            n_heads=config.n_head,\n",
    "            query_dimensions=config.n_embd//config.n_head,\n",
    "            value_dimensions=config.n_embd//config.n_head,\n",
    "            feed_forward_dimensions=config.n_embd,\n",
    "            attention_type='linear',\n",
    "            feature_map=partial(GeneralizedRandomFeatures, n_dims=config.num_feats),\n",
    "            activation='gelu',\n",
    "            )\n",
    "        self.pos_emb = None\n",
    "        self.tok_emb = nn.Embedding(n_vocab, config.n_embd)\n",
    "        #print('self.tok_emb:',self.tok_emb)\n",
    "        self.drop = nn.Dropout(config.d_dropout)\n",
    "        \n",
    "        ## transformer\n",
    "        self.blocks = builder.get()\n",
    "        #self.lang_model = self.lm_layer(config.n_embd, n_vocab)\n",
    "        #self.train_config = config\n",
    "        #if we are starting from scratch set seeds\n",
    "        #########################################\n",
    "        # protein_emb_dim, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "        #########################################\n",
    "        '''\n",
    "        self.fcs = []  \n",
    "        self.loss = torch.nn.L1Loss()\n",
    "        self.net = self.Net(\n",
    "            config.n_embd, dims=config.dims, dropout=config.dropout,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        dims = [150, 50, 50, 2]\n",
    "\n",
    "\n",
    "        def __init__(self, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.desc_skip_connection = True \n",
    "            self.fcs = []  # nn.ModuleList()\n",
    "            #print('dropout is {}'.format(dropout))\n",
    "\n",
    "            self.fc1 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.relu1 = nn.GELU()\n",
    "            self.fc2 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.relu2 = nn.GELU()\n",
    "            self.final = nn.Linear(smiles_embed_dim, 1)\n",
    "\n",
    "        def forward(self, smiles_emb):\n",
    "            x_out = self.fc1(smiles_emb)\n",
    "            x_out = self.dropout1(x_out)\n",
    "            x_out = self.relu1(x_out)\n",
    "\n",
    "            if self.desc_skip_connection is True:\n",
    "                x_out = x_out + smiles_emb\n",
    "\n",
    "            z = self.fc2(x_out)\n",
    "            z = self.dropout2(z)\n",
    "            z = self.relu2(z)\n",
    "            if self.desc_skip_connection is True:\n",
    "                z = self.final(z + x_out)\n",
    "            else:\n",
    "                z = self.final(z)\n",
    "\n",
    "            return z\n",
    "\n",
    "    class lm_layer(nn.Module):\n",
    "        def __init__(self, n_embd, n_vocab):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Linear(n_embd, n_embd)\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.head = nn.Linear(n_embd, n_vocab, bias=False)\n",
    "        def forward(self, tensor):\n",
    "            tensor = self.embed(tensor)\n",
    "            tensor = F.gelu(tensor)\n",
    "            tensor = self.ln_f(tensor)\n",
    "            tensor = self.head(tensor)\n",
    "            return tensor\n",
    "\n",
    "    def get_loss(self, smiles_emb, measures):\n",
    "\n",
    "        z_pred = self.net.forward(smiles_emb).squeeze()\n",
    "        measures = measures.float()\n",
    "\n",
    "        return self.loss(z_pred, measures), z_pred, measures\n",
    "    \n",
    "    \n",
    "margs = args\n",
    "tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "seed.seed_everything(margs.seed)\n",
    "if margs.seed_path == '':\n",
    "    #print(\"# training from scratch\")\n",
    "    smile_model = LightningModule(margs, tokenizer)\n",
    "else:\n",
    "    #print(\"# loaded pre-trained model from {args.seed_path}\")\n",
    "    smile_model = LightningModule(margs, tokenizer).load_from_checkpoint(margs.seed_path, strict=False, config=margs, tokenizer=tokenizer, vocab=len(tokenizer.vocab))#########################33\n",
    "\n",
    "#print('model:',smile_model)\n",
    "#freezing parameters\n",
    "for i,p in enumerate(smile_model.parameters()):\n",
    "    p.requires_grad = False\n",
    "    \n",
    "\n",
    "#num_tasks=1\n",
    "model= InteractionModel_4(protein_model=protein_model,molecular_model=molecular_model,smile_model=smile_model,protein_embd_dim=1280,num_tasks=1,device=device,mol_embd_dim=300,smile_embd_dim=768) \n",
    "model.to(device)\n",
    "#print(model)#nice#num_tasks=1\n",
    "\n",
    "\n",
    "###################when doing geometric.data process, if there is some changes, please delete the directory process and let it generate again\n",
    "\n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset, dataset=args.dataset)###########################\n",
    "#print('args.dataset:',args.dataset)\n",
    "#print(gnn_dataset)\n",
    "#for i,gnn_data in enumerate(gnn_dataset):\n",
    "    #print(gnn_data)\n",
    "    \n",
    "seq_dataset=SeqDataset('dataset/affinity/processed/sequence.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#seq_dataset[2]\n",
    "\n",
    "def collate(batch):\n",
    "    #print('collate_batch:',batch)\n",
    "    #print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    #print('tokens[1]_mask:',tokens[1])\n",
    "    #print('colate###########################')\n",
    "    #for i,m in enumerate(tokens):\n",
    "            \n",
    "    #print('collate_tokens_input_ids:',tokens['input_ids'])\n",
    "    #print('colate_tokens_attention_mask:',tokens['attention_mask'])\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "    \n",
    "smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#####test with chartGPT  DataSet extends two parents' classes\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MultiDatasetMixin:\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.dataset3=dataset3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset1), len(self.dataset2),len(self.dataset3))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dataset1[idx], self.dataset2[idx],self.dataset3[idx]\n",
    "\n",
    "class CustomMultiDataset(MultiDatasetMixin, Dataset):###############3extends two classes\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        MultiDatasetMixin.__init__(self, dataset1, dataset2,dataset3)\n",
    "\n",
    "\n",
    "#chartGPT\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
    "\n",
    "class MultiDataLoader:\n",
    "    def __init__(self, dataloader1, dataloader2,dataloader3):\n",
    "        self.dataloader1 = dataloader1\n",
    "        self.dataloader2 = dataloader2\n",
    "        self.dataloader3=dataloader3\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data1, data2,data3 in zip(self.dataloader1, self.dataloader2, self.dataloader3):\n",
    "            yield data1, data2, data3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader1), len(self.dataloader2), len(self.dataloader3))\n",
    "\n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.dataloader1.shuffle = shuffle\n",
    "        self.dataloader2.shuffle = shuffle\n",
    "        self.dataloader3.shuffle=shuffle\n",
    "\n",
    "        \n",
    "###########split train dataset validate dataset test dataset\n",
    "seq_gnn_smile_dataset=MultiDatasetMixin(seq_dataset,gnn_dataset,smiles_dataset)\n",
    "\n",
    "#print('seq_dataset:',seq_dataset)\n",
    "\n",
    "#seq_dataloader=DataLoader(seq_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.num_workers)\n",
    "'''\n",
    "for i ,seq in enumerate(seq_dataloader):\n",
    "    print(seq)\n",
    "'''\n",
    "if args.split == \"scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        \n",
    "        #print('smiles_list:',smiles_list)\n",
    "        mol_train_dataset, mol_valid_dataset, mol_test_dataset ,seq_train_dataset,seq_valid_dataset,seq_test_dataset,smile_train_dataset,smile_valid_dataset,smile_test_dataset= scaffold_split_1(seq_gnn_smile_dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)##########dataset\n",
    "        print(\"scaffold\")\n",
    "elif args.split == \"random\":\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random\")\n",
    "elif args.split == \"random_scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random scaffold\")\n",
    "else:\n",
    "        raise ValueError(\"Invalid split option.\")\n",
    "\n",
    "#print('++++++++++', mol_train_dataset[0])\n",
    "'''\n",
    "for i, mol in enumerate(mol_train_dataset):\n",
    "    print('mol:',mol)\n",
    "'''\n",
    "#seq_mol_train_dataset=MultiDatasetMixini(seq_train_dataset,mol_train_dataset)\n",
    "#seq_mol_valid_dataset=SeqMolDataset(seq_valid_dataset,mol_valid_dataset)\n",
    "#seq_mol_test_dataset=SeqMolDataset(seq_train_dataset,mol_test_dataset)\n",
    "seq_train_dataloader1 = DataLoader(seq_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_train_dataloader2 = GeometricDataLoader(mol_train_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_train_dataloader3=DataLoader(smile_train_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "seq_mol_smile_train_multi_loader = MultiDataLoader(seq_train_dataloader1, mol_train_dataloader2,smile_train_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_train_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "print('seq_train_dataset#########:',seq_train_dataset)\n",
    "\n",
    "for i,seq in enumerate(seq_train_dataloader1):\n",
    "    print(seq)\n",
    "'''\n",
    "seq_valid_dataloader1 = DataLoader(seq_valid_dataset, batch_size=args.batch_size,shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_valid_dataloader2 = GeometricDataLoader(mol_valid_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_valid_dataloader3=DataLoader(smile_valid_dataset, batch_size=args.batch_size,collate_fn=collate,shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "'''\n",
    "for i,m in enumerate(smile_train_dataloader3):\n",
    "    #print('m@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:',m)\n",
    "    break\n",
    "'''\n",
    "seq_mol_smile_valid_multi_loader = MultiDataLoader(seq_valid_dataloader1, mol_valid_dataloader2,smile_valid_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_valid_multi_loader.set_shuffle(True)\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_valid_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "'''\n",
    "for i ,(seq,mol,smile) in enumerate(seq_mol_smile_train_multi_loader):\n",
    "    print(seq)\n",
    "    print(mol)\n",
    "    print(smile)\n",
    "'''\n",
    "'''\n",
    "print('mol_dataloader:')\n",
    "for mol in mol_train_dataloader2:\n",
    "    print(mol)\n",
    "for seq in seq_train_dataloader1:\n",
    "    print(seq)\n",
    " '''   \n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset+'/test/', dataset=args.dataset)\n",
    "seq_dataset=SeqDataset('dataset/affinity/test/processed/sequence.csv')\n",
    "smiles_dataset=SmileDataset('dataset/affinity/test/processed/smiles.csv')\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_test_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "\n",
    "\n",
    "\n",
    "def train(args, epoch, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    save_pt='results/model2/'\n",
    "    #epoch_iter = tqdm(loader, desc=\"Iteration\")\n",
    "    for step, (A,B,C) in enumerate(loader):\n",
    "        \n",
    "        seq_data_list=[]\n",
    "        seq=A\n",
    "        lenth=len(seq)\n",
    "        for m , s in enumerate(seq):\n",
    "            seq_data_list.append((str(m),s))\n",
    "        B=B.to(device)\n",
    "        D,E=C\n",
    "        D=D.to(device)\n",
    "        E=E.to(device)\n",
    "        C=(D,E)\n",
    "        #print('D!!!!!!!!!!!!!!!!:',D)\n",
    "        #print('E#####################:',E)\n",
    "        #pred=model(seq_data_list,B,C)#model is error\n",
    "        pred,u12,u34=model(seq_data_list,B,C)#model is error\n",
    "        y_true = B.y.view(pred.shape).to(torch.float64)\n",
    "        #loss = criterion(pred, y_true)\n",
    "        \n",
    "        loss1=criterion(pred,y_true)\n",
    "        loss2=criterion(u12,u34)\n",
    "        #print('loss1{0},loss2{1}:',loss1,loss2)\n",
    "        loss=loss1+loss2\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #epoch_iter.set_description(f\"Epoch: {epoch} tloss: {loss:.4f}\")\n",
    "        nn=(epoch+1)//10\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'training Epoch [{epoch+1}/50], Loss: {loss.item():.4f}')\n",
    "            torch.save(model, save_pt+f'full_model_{nn}.pt')\n",
    "    return loss.item\n",
    "\n",
    "\n",
    "def eval(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            D,E=C\n",
    "            D=D.to(device)\n",
    "            E=E.to(device)##################\n",
    "            C=(D,E)\n",
    "        \n",
    "        \n",
    "            #pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            pred,out12,out34=model(seq_data_list,B,C)#model is error\n",
    "            y_true=B.y.view(pred.shape).to(torch.float64)\n",
    "            val_loss = criterion(pred, y_true)+criterion(out12,out34)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "\n",
    "    \n",
    "    \n",
    "    return val_loss.item()\n",
    "\n",
    "def test(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            D,E=C\n",
    "            D=D.to(device)\n",
    "            E=E.to(device)##################\n",
    "            C=(D,E)\n",
    "        \n",
    "        \n",
    "            #pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            pred,out12,out34=model(seq_data_list,B,C)#model is error\n",
    "            y_true=B.y.view(pred.shape).to(torch.float64)\n",
    "            test_loss = mse_criterion(pred, y_true)+mse_criterion(out12,out34)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "\n",
    "    \n",
    "    \n",
    "    return test_loss.item()\n",
    "\n",
    "gnn_dataset = MoleculeDatasetBig(root=\"./dataset/\" + args.dataset+'/test/', dataset=args.dataset)\n",
    "seq_dataset=SeqDataset('dataset/affinity/test/processed/sequence.csv')\n",
    "smiles_dataset=SmileDataset('dataset/affinity/test/processed/smiles.csv')\n",
    "\n",
    "seq_test_dataloader1 = DataLoader(seq_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "mol_test_dataloader2 = GeometricDataLoader(mol_test_dataset, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "smile_test_dataloader3=DataLoader(smile_test_dataset,batch_size=args.batch_size,collate_fn=collate, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_test_dataloader2,smile_test_dataloader3)\n",
    "# Set the shuffle parameter simultaneously for both dataloaders\n",
    "seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "\n",
    "\n",
    "\n",
    "results_save_file='results/model2/results_save_single_cross_model2.txt'\n",
    "\n",
    "\n",
    "import torch, gc\n",
    "#criterion = nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "criterion=nn.SmoothL1Loss()\n",
    "mse_criterion=torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "if not args.filename == \"\":\n",
    "    fname = 'runs/seq_mol_finetune_cls_runseed' + str(args.runseed) + '/' + args.filename\n",
    "    #delete the directory if there exists one\n",
    "    if os.path.exists(fname):\n",
    "        shutil.rmtree(fname)\n",
    "        print(\"removed the existing file.\")\n",
    "    writer = SummaryWriter(fname)\n",
    "\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    print(\"====epoch \" + str(epoch))\n",
    "        \n",
    "    train(args, epoch, model, device, seq_mol_smile_train_multi_loader, optimizer)\n",
    "\n",
    "    print(\"====Evaluation\")\n",
    "    if args.eval_train:\n",
    "        train_loss = eval(args, model, device, seq_mol_smile_train_multi_loader)\n",
    "    else:\n",
    "        print(\"omit the training accuracy computation\")\n",
    "        train_loss = 0\n",
    "    val_loss = eval(args, model, device, seq_mol_smile_valid_multi_loader)\n",
    "    ##test_loss = eval(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "    test_loss = test(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "    with open(results_save_file, 'w+') as f:\n",
    "        f.write(str(epoch)+'\\t'+str(train_loss)+'\\t'+str(val_loss)+'\\n')\n",
    "        #f.write(epoch)\n",
    "        #f.write('\\t')\n",
    "        #f.write(train_loss)\n",
    "        #f.write('\\t')\n",
    "        #f.write(val_loss)\n",
    "        #f.write('\\n')\n",
    "    print(\"train: %f val: %f test: %f\" %(train_loss, val_loss, test_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#train: 0.651232 val: 3.536341 test: 0.086236   \n",
    "#test_loss: 0.2863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5006f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/esm-main/SeqMolSmile_model2.py:429: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fused_x1=F.softmax(fused_x1)\n",
      "/home/wuyou/esm-main/SeqMolSmile_model2.py:430: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fused_x2=F.softmax(fused_x2)\n",
      "/home/wuyou/esm-main/SeqMolSmile_model2.py:481: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fused_x3=F.softmax(fused_x3)\n",
      "/home/wuyou/esm-main/SeqMolSmile_model2.py:482: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  fused_x4=F.softmax(fused_x4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.451597\n"
     ]
    }
   ],
   "source": [
    "test_loss = eval(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "print('test_loss: %f' %test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"./dataset/\" + args.dataset\n",
    "y=os.path.join(x,'raw')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6554bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19, 2], edge_index=[2, 36], edge_attr=[36, 2], id=[1], y=[1])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.load('dataset/affinity/processed/data_6.pt')\n",
    " \n",
    "# \n",
    "#print(tensor.shape)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f4416c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/affinity/processed/data_7.pt\n"
     ]
    }
   ],
   "source": [
    "idx=7\n",
    "processed_dir=\"dataset/affinity/processed/\"\n",
    "\n",
    "y=os.path.join(processed_dir, f'data_{idx}.pt')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4bb3f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[39, 2], edge_index=[2, 86], edge_attr=[86, 2], id=[1], y=[1])\n"
     ]
    }
   ],
   "source": [
    "z=torch.load(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c095deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file.txt\", \"w\") as file:\n",
    "    file.write(\"Hello, World!\\n\")\n",
    "    file.write(\"This is a new line.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26408c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2863564212655271\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y=np.sqrt(0.082)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1f406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
