{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.multiprocessing as mp\n",
    "from torch.cuda.amp import GradScaler\n",
    "from loader1 import myMoleculeDataset,MoleculeDataset,MoleculeDatasetBig, SeqDataset,SeqMolDataset,SmileDataset#########################\n",
    "import torch\n",
    "import torch\n",
    "#import args\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "#from loader import MoleculeDataset#################\n",
    "#from torch_geometric.data import DataLoader\n",
    "#from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import GNN, GNN_graphpred,GNN_graphpred_1\n",
    "\n",
    "\n",
    "from splitters import scaffold_split,scaffold_split_1\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "#import esm2_t33_650M_UR50D\n",
    "import esm\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_warn, rank_zero_only, seed\n",
    "#from finetune.tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from fast_transformers.masking import LengthMask as LM\n",
    "#from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "from fast_transformers.feature_maps import GeneralizedRandomFeatures\n",
    "from functools import partial\n",
    "from apex import optimizers\n",
    "import subprocess\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "#from utils import normalize_smiles\n",
    "import sys\n",
    "sys.path.append('finetune/')\n",
    "from utilss import normalize_smiles\n",
    "from tokenizer.tokenizer import MolTranBertTokenizer\n",
    "from rotate_attention.rotate_builder import RotateEncoderBuilder as rotate_builder\n",
    "#from SeqMolModel import InteractionModel,InteractionModel_1,SequenceModel,InteractionModel_4\n",
    "#from SeqMolSmile import InteractionModel_4\n",
    "#from SeqMolModel import InteractionModel_4\n",
    "from SeqMolSmile_model2 import InteractionModel_4\n",
    "print(torch.cuda.is_available())\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda._initialized = True\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int,default=0,\n",
    "                        help='which gpu to use if any (default: 0)')#0000\n",
    "parser.add_argument('--gpu',default='0,1,2')\n",
    "parser.add_argument('--batch_size', type=int, default=16,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=400,\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                        help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "parser.add_argument('--decay', type=float, default=0,\n",
    "                        help='weight decay (default: 0)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                        help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5,\n",
    "                        help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                        help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                        help='how the node features across layers are combined. last, sum, max or concat')\n",
    "parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "parser.add_argument('--dataset', type=str, default = 'davis', help='root directory of dataset. For now, only classification.')\n",
    "#parser.add_argument('--input_model_file', type=str, default = 'None', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--input_model_file', type=str, default = 'Mole-BERT', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--filename', type=str, default = '', help='output filename')\n",
    "parser.add_argument('--seed', type=int, default=42, help = \"Seed for splitting the dataset.\")\n",
    "parser.add_argument('--runseed', type=int, default=0, help = \"Seed for minibatch selection, random initialization.\")\n",
    "parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "parser.add_argument('--eval_train', type=int, default = 1, help='evaluating training or not')\n",
    "parser.add_argument('--num_workers', type=int, default = 8, help='number of workers for dataset loading')\n",
    "#parser.add_argument('--gpu', type=int, default=0, help='')\n",
    "parser.add_argument('--rank',type=int,default=0,help='')\n",
    "parser.add_argument('--world_size', type=float,default=0.1,help='')\n",
    "parser.add_argument('--dist_backend ',type=str, default='nccl',help='')\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--local_rank',type=int,default=0,help='')\n",
    "parser.add_argument('--n_layer', type=int, default = 12, help='number of workers for dataset loading')\n",
    "parser.add_argument('--d_dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--n_embd', type=int, default = 768, help='number of workers for dataset loading')\n",
    "parser.add_argument('--dropout', type=float, default = 0.1, help='number of workers for dataset loading')\n",
    "parser.add_argument('--lr_start', type=float, default =  3e-5, help='number of workers for dataset loading')\n",
    "parser.add_argument('--max_epochs', type=int, default = 500, help='number of workers for dataset loading')\n",
    "parser.add_argument('--num_feats', type=int, default = 32, help='number of workers for dataset loading')\n",
    "parser.add_argument('--checkpoint_every', type=int, default = 100, help='number of workers for dataset loading')\n",
    "parser.add_argument('--seed_path', type=str, default =  'data/checkpoints/N-Step-Checkpoint_3_30000.ckpt', help='number of workers for dataset loading')\n",
    "parser.add_argument('--dims', type=list, default = [ 768, 768, 768, 1], help='number of workers for dataset loading')\n",
    "\n",
    "args = parser.parse_args(args=[])###############33\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "# 初始化分布式训练环境\n",
    "\n",
    "\n",
    "def init_ddp(local_rank):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'  \n",
    "    os.environ['MASTER_PORT'] = '19198'  \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu  \n",
    "    world_size = torch.cuda.device_count()\n",
    "    os.environ['WORLD_SIZE'] = str(world_size)\n",
    "    #print('init_ddp_local_rank:',local_rank)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    os.environ['RANK'] = str(local_rank)\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "    dist.init_process_group(backend='nccl', init_method='env://')\n",
    "    \n",
    "def get_ddp_generator(seed=3407):\n",
    "    local_rank = dist.get_rank()\n",
    "    print('local_ran_get_ggp_generator:',local_rank)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed + local_rank)\n",
    "    return g\n",
    "\n",
    "\n",
    "\n",
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super(LightningModule, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        #self.hparams = config\n",
    "        #self.mode = config.mode\n",
    "        self.save_hyperparameters(config)\n",
    "        self.tokenizer=tokenizer\n",
    "        '''\n",
    "        self.min_loss = {\n",
    "            self.hparams.measure_name + \"min_valid_loss\": torch.finfo(torch.float32).max,\n",
    "            self.hparams.measure_name + \"min_epoch\": 0,\n",
    "        }\n",
    "        '''\n",
    "        # Word embeddings layer\n",
    "        n_vocab, d_emb = len(tokenizer.vocab), config.n_embd\n",
    "        # input embedding stem\n",
    "        \n",
    "        builder = rotate_builder.from_kwargs(\n",
    "            n_layers=config.n_layer,\n",
    "            n_heads=config.n_head,\n",
    "            query_dimensions=config.n_embd//config.n_head,\n",
    "            value_dimensions=config.n_embd//config.n_head,\n",
    "            feed_forward_dimensions=config.n_embd,\n",
    "            attention_type='linear',\n",
    "            feature_map=partial(GeneralizedRandomFeatures, n_dims=config.num_feats),\n",
    "            activation='gelu',\n",
    "            )\n",
    "        self.pos_emb = None\n",
    "        self.tok_emb = nn.Embedding(n_vocab, config.n_embd)\n",
    "        #print('self.tok_emb:',self.tok_emb)\n",
    "        self.drop = nn.Dropout(config.d_dropout)\n",
    "        \n",
    "        ## transformer\n",
    "        self.blocks = builder.get()\n",
    "        #self.lang_model = self.lm_layer(config.n_embd, n_vocab)\n",
    "        #self.train_config = config\n",
    "        #if we are starting from scratch set seeds\n",
    "        #########################################\n",
    "        # protein_emb_dim, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "        #########################################\n",
    "        '''\n",
    "        self.fcs = []  \n",
    "        self.loss = torch.nn.L1Loss()\n",
    "        self.net = self.Net(\n",
    "            config.n_embd, dims=config.dims, dropout=config.dropout,\n",
    "        )\n",
    "        '''\n",
    "\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        dims = [150, 50, 50, 2]\n",
    "\n",
    "\n",
    "        def __init__(self, smiles_embed_dim, dims=dims, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.desc_skip_connection = True \n",
    "            self.fcs = []  # nn.ModuleList()\n",
    "            #print('dropout is {}'.format(dropout))\n",
    "\n",
    "            self.fc1 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.relu1 = nn.GELU()\n",
    "            self.fc2 = nn.Linear(smiles_embed_dim, smiles_embed_dim)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.relu2 = nn.GELU()\n",
    "            self.final = nn.Linear(smiles_embed_dim, 1)\n",
    "\n",
    "        def forward(self, smiles_emb):\n",
    "            x_out = self.fc1(smiles_emb)\n",
    "            x_out = self.dropout1(x_out)\n",
    "            x_out = self.relu1(x_out)\n",
    "\n",
    "            if self.desc_skip_connection is True:\n",
    "                x_out = x_out + smiles_emb\n",
    "\n",
    "            z = self.fc2(x_out)\n",
    "            z = self.dropout2(z)\n",
    "            z = self.relu2(z)\n",
    "            if self.desc_skip_connection is True:\n",
    "                z = self.final(z + x_out)\n",
    "            else:\n",
    "                z = self.final(z)\n",
    "\n",
    "            return z\n",
    "\n",
    "    class lm_layer(nn.Module):\n",
    "        def __init__(self, n_embd, n_vocab):\n",
    "            super().__init__()\n",
    "            self.embed = nn.Linear(n_embd, n_embd)\n",
    "            self.ln_f = nn.LayerNorm(n_embd)\n",
    "            self.head = nn.Linear(n_embd, n_vocab, bias=False)\n",
    "        def forward(self, tensor):\n",
    "            tensor = self.embed(tensor)\n",
    "            tensor = F.gelu(tensor)\n",
    "            tensor = self.ln_f(tensor)\n",
    "            tensor = self.head(tensor)\n",
    "            return tensor\n",
    "\n",
    "    def get_loss(self, smiles_emb, measures):\n",
    "\n",
    "        z_pred = self.net.forward(smiles_emb).squeeze()\n",
    "        measures = measures.float()\n",
    "\n",
    "        return self.loss(z_pred, measures), z_pred, measures\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    #print('collate_batch:',batch)\n",
    "    #print('collate_batch_0:',batch[0])\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "        \n",
    "    tokens = tokenizer.batch_encode_plus([ smile for smile in batch], padding=True, add_special_tokens=True)\n",
    "    #print('tokens[1]_mask:',tokens[1])\n",
    "    #print('colate###########################')\n",
    "    #for i,m in enumerate(tokens):\n",
    "            \n",
    "    #print('collate_tokens_input_ids:',tokens['input_ids'])\n",
    "    #print('colate_tokens_attention_mask:',tokens['attention_mask'])\n",
    "    return (torch.tensor(tokens['input_ids']), torch.tensor(tokens['attention_mask']))\n",
    "    \n",
    "smiles_dataset=SmileDataset('dataset/davis/processed/smiles.csv')\n",
    "#smiles_dataset=SmileDataset('dataset/affinity/processed/smiles.csv')\n",
    "\n",
    "#####test with chartGPT  DataSet extends two parents' classes\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MultiDatasetMixin:\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.dataset3=dataset3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset1), len(self.dataset2),len(self.dataset3))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dataset1[idx], self.dataset2[idx],self.dataset3[idx]\n",
    "\n",
    "class CustomMultiDataset(MultiDatasetMixin, Dataset):###############3extends two classes\n",
    "    def __init__(self, dataset1, dataset2,dataset3):\n",
    "        MultiDatasetMixin.__init__(self, dataset1, dataset2,dataset3)\n",
    "\n",
    "\n",
    "#chartGPT太厉害了，杀了我吧\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader as GeometricDataLoader\n",
    "\n",
    "class MultiDataLoader:\n",
    "    def __init__(self, dataloader1, dataloader2,dataloader3):\n",
    "        self.dataloader1 = dataloader1\n",
    "        self.dataloader2 = dataloader2\n",
    "        self.dataloader3=dataloader3\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data1, data2,data3 in zip(self.dataloader1, self.dataloader2, self.dataloader3):\n",
    "            yield data1, data2, data3\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataloader1), len(self.dataloader2), len(self.dataloader3))\n",
    "\n",
    "    def set_shuffle(self, shuffle):\n",
    "        self.dataloader1.shuffle = shuffle\n",
    "        self.dataloader2.shuffle = shuffle\n",
    "        self.dataloader3.shuffle=shuffle\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def train(args, epoch, model,  loader, optimizer,scaler):\n",
    "    if hasattr(torch.cuda, 'empty_cache'):\n",
    "        torch.cuda.empty_cache()\n",
    "    device=args.device\n",
    "    model.train()\n",
    "    save_pt='results/davis/model1/'\n",
    "    #epoch_iter = tqdm(loader, desc=\"Iteration\")\n",
    "    for step, (A,B,C) in enumerate(loader):\n",
    "        print('eposh,step:',epoch,step)\n",
    "        seq_data_list=[]\n",
    "        seq=A\n",
    "        lenth=len(seq)\n",
    "        \n",
    "        for m , s in enumerate(seq):\n",
    "            seq_data_list.append((str(m),s))\n",
    "        B=B.to(device)\n",
    "        #print(C)\n",
    "        D,E=C\n",
    "        D=D.to(device)\n",
    "        E=E.to(device)\n",
    "        C=(D,E)\n",
    "        \n",
    "        pred=model(seq_data_list,B,C)#model is error\n",
    "        \n",
    "        y_true = B.y.view(pred.shape).to(torch.float32)\n",
    "        \n",
    "        loss1=criterion(pred,y_true)\n",
    "        \n",
    "        loss=loss1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scaler.scale(loss).backward()  ###\n",
    "        scaler.step(optimizer)  ###\n",
    "        scaler.update()\n",
    "\n",
    "def eval(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    #for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "    with torch.no_grad():\n",
    "        for step, (A,B,C) in enumerate(loader):\n",
    "            seq_data_list=[]\n",
    "            seq=A\n",
    "            lenth=len(seq)\n",
    "            for m , s in enumerate(seq):\n",
    "                seq_data_list.append((str(m),s))\n",
    "            B=B.to(device)\n",
    "            D,E=C\n",
    "            D=D.to(device)\n",
    "            E=E.to(device)##################\n",
    "            C=(D,E)\n",
    "        \n",
    "        \n",
    "            #pred=model(seq_data_list,B,C)#model is error\n",
    "            \n",
    "            pred=model(seq_data_list,B,C)#model is error\n",
    "            y_true=B.y.view(pred.shape).to(torch.float32)\n",
    "            val_loss = criterion(pred, y_true)\n",
    "            #print(f'Validation Loss: {val_loss.item():.4f}')    \n",
    "                             \n",
    "\n",
    "   \n",
    "    \n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "import torch, gc\n",
    "\n",
    "def main(local_rank,args):\n",
    "    gnn_dataset = myMoleculeDataset(root=\"./dataset/\" + args.dataset)###########################转换成了分子图的格式\n",
    "    print('args.dataset:',args.dataset)\n",
    "    seq_dataset=SeqDataset('dataset/davis/processed/sequence.csv')\n",
    "    smiles_dataset=SmileDataset('dataset/davis/processed/smiles.csv')\n",
    "    # 初始化分布式训练环境\n",
    "    #init_distributed()\n",
    "    #init_distributed_mode(args)\n",
    "    print('distributed#########')\n",
    "    # 创建模型\n",
    "    init_ddp(local_rank) ############\n",
    "    #print('init_ddp_rank:',local_rank)\n",
    "    \n",
    "    \n",
    "    \n",
    "    protein_model, protein_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    num_tasks=1\n",
    "    if torch.cuda.is_available():\n",
    "        protein_model.to(device)\n",
    "    for i,p in enumerate(protein_model.parameters()):\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    protein_model.eval()  # disables dropout for deterministic results\n",
    "    molecular_model = GNN_graphpred_1(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "    if not args.input_model_file == \"None\":###############\n",
    "        print('Not from scratch')\n",
    "        molecular_model.from_pretrained('model_gin/{}.pth'.format(args.input_model_file))\n",
    "        print('rese:model_gin')\n",
    "        molecular_model.to(device)\n",
    "\n",
    "\n",
    "    print('molecular load############')\n",
    "    for i,p in enumerate(molecular_model.parameters()):\n",
    "        p.requires_grad = False#freezing parameters\n",
    "    #freezing parameters\n",
    "    for i,p in enumerate(protein_model.parameters()):\n",
    "        p.requires_grad = False#freezing parameters\n",
    "    print('parameter frozen###########')\n",
    "\n",
    "    \n",
    "    \n",
    "    margs = args\n",
    "    tokenizer = MolTranBertTokenizer('finetune/bert_vocab.txt')\n",
    "    #seed.seed_everything(margs.seed)\n",
    "\n",
    "    print('smile_model_start$$$$$')\n",
    "    if margs.seed_path == '':\n",
    "        #print(\"# training from scratch\")\n",
    "        smile_model = LightningModule(margs, tokenizer)\n",
    "    else:\n",
    "        #print(\"# loaded pre-trained model from {args.seed_path}\")\n",
    "        smile_model = LightningModule(margs, tokenizer).load_from_checkpoint(margs.seed_path, strict=False, config=margs, tokenizer=tokenizer, vocab=len(tokenizer.vocab))#########################33\n",
    "\n",
    "    #print('model:',smile_model)\n",
    "    #freezing parameters\n",
    "\n",
    "    print('smile_model_load************')\n",
    "    for i,p in enumerate(smile_model.parameters()):\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    print('smile_model_parameters***********')\n",
    "\n",
    "\n",
    "    model= InteractionModel_4(protein_model=protein_model,molecular_model=molecular_model,smile_model=smile_model,protein_embd_dim=1280,num_tasks=1,device=device,mol_embd_dim=300,smile_embd_dim=768) \n",
    "    model.to(device)\n",
    "    model = model.cuda()\n",
    "    print('model%%%%%%%%%%%%%%%%')\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model) \n",
    "    print('nn.SyncBatchNorm.convert_sync_batchnorm(model)################')\n",
    "    # 将模型封装为DistributedDataParallel对象\n",
    "    model = DistributedDataParallel(model,device_ids=[local_rank]) \n",
    "    print('DistributedDataParallled(model)@@@@@@@@@@@@')                                                        \n",
    "    \n",
    "    # 加载数据集\n",
    "    if args.split == \"scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        \n",
    "        #print('smiles_list:',smiles_list)\n",
    "        mol_train_dataset, mol_valid_dataset, mol_test_dataset ,seq_train_dataset,seq_valid_dataset,seq_test_dataset,smile_train_dataset,smile_valid_dataset,smile_test_dataset= scaffold_split_1(seq_gnn_smile_dataset, smiles_list, null_value=0, frac_train=0.6,frac_valid=0.2, frac_test=0.2)##########dataset\n",
    "        print(\"scaffold\")\n",
    "    elif args.split == \"random\":\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random\")\n",
    "    elif args.split == \"random_scaffold\":\n",
    "        smiles_list = pd.read_csv('./dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        #print(\"random scaffold\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split option.\")\n",
    "    \n",
    "    gnn_train_sampler = DistributedSampler(gnn_train_dataset)\n",
    "    seq_train_sampler = DistributedSampler(seq_train_dataset)\n",
    "    smiles_train_sampler = DistributedSampler(smiles_train_dataset)\n",
    "    print('sampler#############')\n",
    "    gnn_test_sampler = DistributedSampler(gnn_test_dataset)\n",
    "    seq_test_sampler = DistributedSampler(seq_test_dataset)\n",
    "    smiles_test_sampler = DistributedSampler(smiles_test_dataset)\n",
    "    g = get_ddp_generator()##########################\n",
    "    seq_train_dataloader1 = torch.utils.data.DataLoader(seq_train_dataset,generator=g,sampler=seq_train_sampler, pin_memory=True,batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "    mol_train_dataloader2 = GeometricDataLoader(gnn_train_dataset, generator=g,sampler=gnn_train_sampler,pin_memory=True,batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "    \n",
    "    \n",
    "    smile_train_dataloader3=torch.utils.data.DataLoader(smiles_train_dataset,                                                                generator=g,sampler=smiles_train_sampler,pin_memory=True,batch_size=args.batch_size,collate_fn=collate_fn, shuffle=False,num_workers=args.num_workers) \n",
    "                                                                    \n",
    "    seq_mol_smile_train_multi_loader = MultiDataLoader(seq_train_dataloader1, mol_train_dataloader2,smile_train_dataloader3)\n",
    "    # Set the shuffle parameter simultaneously for both dataloaders\n",
    "    seq_mol_smile_train_multi_loader.set_shuffle(True)\n",
    "        \n",
    "    print('dataloader####################################')   \n",
    "        \n",
    "    seq_test_dataloader1 = torch.utils.data.DataLoader(seq_test_dataset,generator=g,sampler=seq_train_sampler,pin_memory=True, batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)######False\n",
    "    mol_test_dataloader2 = GeometricDataLoader(gnn_test_dataset,generator=g, sampler=gnn_train_sampler,pin_memory=True,batch_size=args.batch_size, shuffle=False,num_workers=args.num_workers)\n",
    "    smile_test_dataloader3=torch.utils.data.DataLoader(smiles_test_dataset,generator=g,sampler=smiles_train_sampler,pin_memory=True,batch_size=args.batch_size,collate_fn=collate_fn, shuffle=False,num_workers=args.num_workers)\n",
    "\n",
    "    seq_mol_smile_test_multi_loader = MultiDataLoader(seq_test_dataloader1, mol_test_dataloader2,smile_test_dataloader3)\n",
    "    # Set the shuffle parameter simultaneously for both dataloaders\n",
    "    seq_mol_smile_test_multi_loader.set_shuffle(True)\n",
    "    #train_dataset = ...  # 替换为您自己的训练数据集\n",
    "    #train_sampler = DistributedSampler(train_dataset)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    \n",
    "    criterion=nn.SmoothL1Loss()\n",
    "    mse_criterion=nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scaler = GradScaler()  ###\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    if not args.filename == \"\":\n",
    "        fname = 'runs/seq_mol_finetune_cls_runseed' + str(args.runseed) + '/' + args.filename\n",
    "        #delete the directory if there exists one\n",
    "        if os.path.exists(fname):\n",
    "            shutil.rmtree(fname)\n",
    "            print(\"removed the existing file.\")\n",
    "        writer = SummaryWriter(fname)\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        print(\"====epoch \" + str(epoch))\n",
    "        if local_rank == 0:  ### \n",
    "            print('begin training of epoch {}'.format((epoch + 1)/(args.epochs)))\n",
    "        #seq_mol_smile_train_multi_loader.sampler.set_epoch(epoch)  ### \n",
    "        #train(model, seq_mol_smile_train_multi_loader, criterion, optimizer, scaler)\n",
    "        train(args, epoch, model,  seq_mol_smile_train_multi_loader, optimizer,scaler)\n",
    "    if local_rank == 0:\n",
    "        print('begin testing')\n",
    "    test(model, seq_mol_smile_test_multi_loader)\n",
    "    \n",
    "    if local_rank == 0:  ##\n",
    "        print(\"train: %f  test: %f\" %(train_loss, test_loss))\n",
    "        if epoch %50==0:\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'scaler': scaler.state_dict()\n",
    "            }, 'results/davis/model1/ddp_davis_model1_checkpoint.pt')\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "    '''\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        print(\"====epoch \" + str(epoch))\n",
    "        train(args, epoch, model, device, seq_mol_smile_train_multi_loader, optimizer)\n",
    "        print(\"====Evaluation\")\n",
    "        if args.eval_train:\n",
    "            train_loss = eval(args, model, device, seq_mol_smile_train_multi_loader)\n",
    "        else:\n",
    "            print(\"omit the training accuracy computation\")\n",
    "            train_loss = 0\n",
    "            \n",
    "        test_loss = eval(args, model, device, seq_mol_smile_test_multi_loader)\n",
    "        with open(results_save_file, 'w+') as f:\n",
    "            f.write(str(epoch)+'\\t'+str(train_loss)+'\\t'+str(val_loss)+'\\n')\n",
    "        \n",
    "        print(\"train: %f  test: %f\" %(train_loss, test_loss))\n",
    "    \n",
    "        '''\n",
    "if __name__ == '__main__':\n",
    "    #args = prepare()\n",
    "    time_start = time.time()\n",
    "    mp.spawn(main, args=(args, ), nprocs=torch.cuda.device_count())\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print('\\ntime elapsed: {time_elapsed:.2f} seconds')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
